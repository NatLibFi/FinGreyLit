{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Extraction using local LLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/wrk-vakka/users/oisuomin/git/FinGreyLit/experiments/llm-inference/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "#import transformers\n",
    "import requests\n",
    "import fitz\n",
    "import io\n",
    "import re\n",
    "import regex\n",
    "import peft\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [02:44<00:00, 20.62s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Write me a poem about Machine Learning.\n",
      "\n",
      "In a world where data is the new gold,\n",
      "Machine Learning is the alchemist's tale,\n",
      "Transforming raw data into insights bold,\n",
      "With algorithms that never fail.\n",
      "\n",
      "It's a science that's both art and craft,\n",
      "A symphony of math and code,\n",
      "A journey that's both deep and vast,\n",
      "A story that's yet to be told.\n",
      "\n",
      "It's a language that's spoken by the stars,\n",
      "A language that's written in the sky,\n",
      "A language that's whispered in the wind,\n",
      "A language that'\n"
     ]
    }
   ],
   "source": [
    "# load and test the language model\n",
    "\n",
    "MODEL = \"NatLibFi/zephyr-7b-meteor-ludwig\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a PDF URL:  https://www.regjeringen.no/contentassets/7464f476cb4744e59554c2cb4b192df5/no/pdfs/dataspillstrategi.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: https://www.regjeringen.no/contentassets/7464f476cb4744e59554c2cb4b192df5/no/pdfs/dataspillstrategi.pdf\n"
     ]
    }
   ],
   "source": [
    "# ask for PDF URL\n",
    "\n",
    "url = input(\"Please enter a PDF URL: \")\n",
    "print(f\"You entered: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1485 characters\n",
      "\n",
      "title: Tid for spill – regjeringens dataspillstrategi 2024–2026\n",
      "author: Kultur- og likestillingsdepartementet\n",
      "subject: Tid for spill – regjeringens dataspillstrategi 2024–2026\n",
      "creationDate: D:20231212102035+01'00'\n",
      "modDate: D:20231212150502+01'00'\n",
      "Kultur- og likestillingsdepartementet\n",
      "Strategi\n",
      "Tid for spill – regjeringens dataspillstrategi 2024–2026\n",
      "Illustrasjonsfoto: AdobeStock 2\n",
      "Vurdering av dagens virkemidler i\n",
      "Forsideillustrasjon: Sunlight, Krillbite Studio AS 3\n",
      "Illustrasjonsfoto: AdobeStock 4\n",
      "Forord\n",
      "Lubna Jaffery\n",
      "KULTUR- OG LIKESTILLINGSMINISTER\n",
      "Foto: Ilja C. Hendel/KUD\n",
      "Takk til Norsk filminstitutt (NFI), Medietilsynet, Kulturtanken, Innovasjon\n",
      "Illustrasjonsfoto: AdobeStock 6\n",
      "Statsminister Jonas Gahr Støre\n",
      "Dataspill er næring\n",
      "Dataspill er kultur\n",
      "Dataspill er internasjonalisering\n",
      "Dataspill er innovasjon\n",
      "Chart: Are You Not Entertained? | Statista 7\n",
      "Om dataspillstrategien\n",
      "Dataspill er kunnskap\n",
      "Arbeidet med regjeringens dataspillstrategi er ledet av Kultur- og likestillingsdepartementet.\n",
      "Kulturtanken og Innovasjon Norge.\n",
      "Dataspill er fellesskap\n",
      "Foto: Pexels\n",
      "Utgitt av:\n",
      "Kultur- og likestillingsdepartementet\n",
      "Bestilling av publikasjoner:\n",
      "Departementenes sikkerhets- og serviceorganisasjon publikasjoner.dep.no\n",
      "Telefon: 22 24 00 00\n",
      "Publikasjoner er også tilgjengelige på: www.regjeringen.no\n",
      "Publikasjonskode: V-1043 B\n",
      "M\n",
      "E\n",
      "E\n",
      "N\n",
      "R\n",
      "A\n",
      "K\n",
      "V\n",
      "E\n",
      "S\n",
      "T\n",
      "Design: Melkeveien Designkontor AS\n",
      "Trykk: Departementenes sikkerhets- og serviceorganisasjon\n",
      "Trykksak 12/2023 – opplag 150 2041 0446\n"
     ]
    }
   ],
   "source": [
    "# download the PDF and extract the relevant text\n",
    "\n",
    "# settings for text extraction\n",
    "PAGES = [0, 1, 2, 3, 4, 5, 6, 7, -1]  # pages to analyze: first 8 pages + last page\n",
    "THRESHOLD = 100                       # paragraphs shorter than this will always be kept\n",
    "LONG_PARAGRAPH_PAGES = [0, 1]         # on first two pages, some long paragraphs are accepted\n",
    "LONG_PARAGRAPH_MAX = 2                # how many long paragraphs to keep on the first two pages\n",
    "PDF_METADATA_SKIP = {'format', 'creator', 'producer'}  # PDF metadata fields not to include in extracted text\n",
    "\n",
    "def download_and_open_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    pdf_stream = io.BytesIO(response.content)\n",
    "    return fitz.open(stream=pdf_stream, filetype=\"pdf\")\n",
    "\n",
    "def extract_text(pdf):\n",
    "    texts = []\n",
    "\n",
    "    for key in pdf.metadata.keys():\n",
    "        if key not in PDF_METADATA_SKIP and pdf.metadata.get(key):\n",
    "            texts.append(f\"{key}: {pdf.metadata.get(key)}\")\n",
    "\n",
    "    for page in PAGES:\n",
    "        if page > len(pdf) - 2:\n",
    "            continue\n",
    "\n",
    "        text = pdf[page].get_text(sort=True)\n",
    "        # Use regular expression to split text into paragraphs\n",
    "        # Delimiter: newline(s) followed by an upper case character\n",
    "        paragraphs = regex.split(r'\\n+(?=\\p{Lu})', text, flags=re.UNICODE)\n",
    "        long_paragraph_count = 0\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = \" \".join(paragraph.strip().split())\n",
    "\n",
    "            if '.....' in paragraph or '. . . . .' in paragraph: # looks like a ToC entry, skip it\n",
    "                continue\n",
    "            elif len(paragraph) < THRESHOLD:  # short paragraph, keep it\n",
    "                texts.append(paragraph)\n",
    "            elif page in LONG_PARAGRAPH_PAGES and long_paragraph_count < LONG_PARAGRAPH_MAX:\n",
    "                # allow some long paragraphs on the first two pages\n",
    "                long_paragraph_count += 1\n",
    "                texts.append(paragraph)\n",
    "            else:  # must be a long paragraph, skip it\n",
    "                pass\n",
    "    return '\\n'.join(texts)\n",
    "pdf = download_and_open_pdf(url)\n",
    "\n",
    "doc_text = extract_text(pdf)\n",
    "print(f\"text length: {len(doc_text)} characters\")\n",
    "print()\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length: 680 tokens\n",
      "\n",
      "{   'dc.contributor.author': ['Jaffery, Lubna'],\n",
      "    'dc.date.issued': '2023-12-12',\n",
      "    'dc.language.iso': 'nor',\n",
      "    'dc.publisher': ['Kultur- og likestillingsdepartementet'],\n",
      "    'dc.title': 'Tid for spill : regjeringens dataspillstrategi 2024-2026'}\n",
      "\n",
      "CPU times: user 3.45 s, sys: 72.7 ms, total: 3.52 s\n",
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# submit the text to the LLM and display results\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "PROMPT_TEMPLATE = \\\n",
    "\"\"\"### Instruction:\n",
    "Extract metadata from the following document. Return as JSON.\n",
    "\n",
    "### Input:\n",
    "{text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def generate(doc_text):\n",
    "    input_text = PROMPT_TEMPLATE.format(text=doc_text)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(f\"input length: {len(input_ids[0])} tokens\")\n",
    "\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=1024)\n",
    "    return tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "response = generate(doc_text)\n",
    "extracted_data = json.loads(response)\n",
    "print()\n",
    "pp.pprint(extracted_data)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Meteor output\n",
    "\n",
    "Command:\n",
    "\n",
    "    curl -s -d fileUrl=https://www.regjeringen.no/contentassets/7464f476cb4744e59554c2cb4b192df5/no/pdfs/dataspillstrategi.pdf http://127.0.0.1:5000/json|jq .\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"year\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"PDFINFO\",\n",
    "      \"pageNumber\": 52\n",
    "    },\n",
    "    \"value\": 2023\n",
    "  },\n",
    "  \"language\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"LANGUAGE_MODEL\"\n",
    "    },\n",
    "    \"value\": \"no\"\n",
    "  },\n",
    "  \"title\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"PDFINFO\",\n",
    "      \"pageNumber\": 1\n",
    "    },\n",
    "    \"value\": \"Tid for spill – regjeringens dataspillstrategi 2024–2026\"\n",
    "  },\n",
    "  \"publisher\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"PAGE\",\n",
    "      \"pageNumber\": 52\n",
    "    },\n",
    "    \"value\": \"Kultur- og likestillingsdepartementet\"\n",
    "  },\n",
    "  \"publicationType\": null,\n",
    "  \"authors\": [],\n",
    "  \"isbn\": null,\n",
    "  \"issn\": null\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-inference)",
   "language": "python",
   "name": "llm-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
