{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Extraction using local LLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/wrk-vakka/users/oisuomin/git/FinGreyLit/experiments/llm-inference/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "#import transformers\n",
    "import requests\n",
    "import fitz\n",
    "import io\n",
    "import re\n",
    "import regex\n",
    "import peft\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a poem about Machine Learning. \n",
      "\n",
      "Machine Learning: A Journey Through the Digital Universe\n",
      "\n",
      "In the vast digital universe,  \n",
      "A machine learning journey begins  \n",
      "A neural network, a self-learning system  \n",
      "Guiding us through the unknown terrain  \n",
      "\n",
      "The first step, a dataset to feed  \n",
      "A vast sea of data, a treasure trove  \n",
      "Structured and unstructured,  \n",
      "A treasure trove of knowledge to explore  \n",
      "\n",
      "The algorithm, a master of learning  \n",
      "A self-adapting system,  \n",
      "A journey through the unknown  \n",
      "A journey of discovery, a journey of learning  \n",
      "\n",
      "The neural network, a self-learning system  \n",
      "A journey through the digital universe  \n",
      "A journey of discovery\n"
     ]
    }
   ],
   "source": [
    "# load and test the language model\n",
    "\n",
    "#MODEL = \"NatLibFi/Nous-Hermes-2-Mistral-7B-DPO-meteor\"\n",
    "MODEL = \"NatLibFi/stablelm-2-zephyr-1_6b-meteor\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "input_text = \"Write a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=128, pad_token_id=tokenizer.pad_token_id)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a PDF URL:  https://www.doria.fi/bitstream/handle/10024/176353/Kansalliskirjaston%20metatietovisio%202020%20v2.pdf?sequence=5&isAllowed=y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: https://www.doria.fi/bitstream/handle/10024/176353/Kansalliskirjaston%20metatietovisio%202020%20v2.pdf?sequence=5&isAllowed=y\n"
     ]
    }
   ],
   "source": [
    "# ask for PDF URL\n",
    "\n",
    "url = input(\"Please enter a PDF URL: \")\n",
    "print(f\"You entered: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1023 characters\n",
      "\n",
      "creationDate: D:20200309090627+02'00'\n",
      "modDate: D:20200309090635+02'00'\n",
      "* KANSALLISKIRJASTON METATIETOVISIO\n",
      "© Kansalliskirjasto 2020\n",
      "Matias Frosterus, Nina Hyvönen, Katri Kananen, Marja-Liisa Seppälä\n",
      "Kuvitus: Pixabay / Gerd Altmann\n",
      "ISBN 978-951-51-5905-2\n",
      "METATIETOVISIO\n",
      "Laadukkaan ja yhteentoimivan metatiedon tuottaminen vaatii yhteistyötä.\n",
      "MIKÄ IHMEEN METATIETO?\n",
      "LUOTETTAVAA METATIETOA IHMISILLE JA KONEILLE\n",
      "Laadukas metatieto on kirjastojen, arkistojen ja museoiden menestystekijä 2020-luvulla.\n",
      "Kansalliskirjasto sitoutuu laadukkaan ja rikkaan metatiedon tuottamiseen.\n",
      "Asiakas\n",
      "Metatieto\n",
      "Järjestelmä\n",
      "Tieto\n",
      "HAJAUTETUT JA YHTEISKÄYTTÖISET METATIETOVARANNOT\n",
      "Yhteistä perustaa voidaan täydentää kunkin sektorin omien vaatimusten mukai- sella tiedolla.\n",
      "Päällekkäiset tiedot\n",
      "Siiloutuneet järjestelmät\n",
      "Epämodulaarisuus\n",
      "Yhteiset metatietoa ohjaavat varannot\n",
      "Yhteentoimiva metatieto\n",
      "Käyttäjäryhmille räätälöidyt palvelut\n",
      "METATIETO JÄRJESTELMIEN POLTTOAINEENA\n",
      "Kuvailtavan tiedon määrän kasvaessa tarvitaan automaatiota yhä enemmän.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the PDF and extract the relevant text\n",
    "\n",
    "# settings for text extraction\n",
    "PAGES = [0, 1, 2, 3, 4, 5, 6, 7, -1]  # pages to analyze: first 8 pages + last page\n",
    "THRESHOLD = 100                       # paragraphs shorter than this will always be kept\n",
    "LONG_PARAGRAPH_PAGES = [0, 1]         # on first two pages, some long paragraphs are accepted\n",
    "LONG_PARAGRAPH_MAX = 2                # how many long paragraphs to keep on the first two pages\n",
    "PDF_METADATA_SKIP = {'format', 'creator', 'producer'}  # PDF metadata fields not to include in extracted text\n",
    "\n",
    "def download_and_open_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    pdf_stream = io.BytesIO(response.content)\n",
    "    return fitz.open(stream=pdf_stream, filetype=\"pdf\")\n",
    "\n",
    "def extract_text(pdf):\n",
    "    texts = []\n",
    "\n",
    "    for key in pdf.metadata.keys():\n",
    "        if key not in PDF_METADATA_SKIP and pdf.metadata.get(key):\n",
    "            texts.append(f\"{key}: {pdf.metadata.get(key)}\")\n",
    "\n",
    "    for page in PAGES:\n",
    "        if page > len(pdf) - 2:\n",
    "            continue\n",
    "\n",
    "        text = pdf[page].get_text(sort=True)\n",
    "        # Use regular expression to split text into paragraphs\n",
    "        # Delimiter: newline(s) followed by an upper case character\n",
    "        paragraphs = regex.split(r'\\n+(?=\\p{Lu})', text, flags=re.UNICODE)\n",
    "        long_paragraph_count = 0\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = \" \".join(paragraph.strip().split())\n",
    "\n",
    "            if '.....' in paragraph or '. . . . .' in paragraph: # looks like a ToC entry, skip it\n",
    "                continue\n",
    "            elif len(paragraph) < THRESHOLD:  # short paragraph, keep it\n",
    "                texts.append(paragraph)\n",
    "            elif page in LONG_PARAGRAPH_PAGES and long_paragraph_count < LONG_PARAGRAPH_MAX:\n",
    "                # allow some long paragraphs on the first two pages\n",
    "                long_paragraph_count += 1\n",
    "                texts.append(paragraph)\n",
    "            else:  # must be a long paragraph, skip it\n",
    "                pass\n",
    "    return '\\n'.join(texts)\n",
    "pdf = download_and_open_pdf(url)\n",
    "\n",
    "doc_text = extract_text(pdf)\n",
    "print(f\"text length: {len(doc_text)} characters\")\n",
    "print()\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length: 515 tokens\n",
      "{\"dc.title\": \"Metatietovisio : Mik\\u00e4 ihmeen metatieto? Luotettavaa metatietoa ihmisille ja koneille\", \"dc.date.issued\": \"2020\", \"dc.identifier.isbn\": [\"9789515159052\"], \"dc.language.iso\": \"fin\", \"dc.publisher\": [\"Kansalliskirjasto\"]}\t\n",
      "\n",
      "\n",
      "\n",
      "{   'dc.date.issued': '2020',\n",
      "    'dc.identifier.isbn': ['9789515159052'],\n",
      "    'dc.language.iso': 'fin',\n",
      "    'dc.publisher': ['Kansalliskirjasto'],\n",
      "    'dc.title': 'Metatietovisio : Mikä ihmeen metatieto? Luotettavaa '\n",
      "                'metatietoa ihmisille ja koneille'}\n",
      "\n",
      "CPU times: user 3.31 s, sys: 23.8 ms, total: 3.33 s\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# submit the text to the LLM and display results\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def generate(doc_text):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': INSTRUCTION + \"\\n\\n\" + doc_text}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "    print(f\"input length: {len(input_ids[0])} tokens\")\n",
    "\n",
    "    outputs = model.generate(input_ids,\n",
    "                             max_new_tokens=2048,\n",
    "                             pad_token_id=tokenizer.pad_token_id,\n",
    "                             eos_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "response = generate(doc_text)\n",
    "#print(response)\n",
    "extracted_data = json.loads(response)\n",
    "print()\n",
    "pp.pprint(extracted_data)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Meteor output\n",
    "\n",
    "Command:\n",
    "\n",
    "    curl -s -d fileUrl=https://www.regjeringen.no/contentassets/7464f476cb4744e59554c2cb4b192df5/no/pdfs/dataspillstrategi.pdf http://127.0.0.1:5000/json|jq .\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"year\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"PDFINFO\",\n",
    "      \"pageNumber\": 52\n",
    "    },\n",
    "    \"value\": 2023\n",
    "  },\n",
    "  \"language\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"LANGUAGE_MODEL\"\n",
    "    },\n",
    "    \"value\": \"no\"\n",
    "  },\n",
    "  \"title\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"PDFINFO\",\n",
    "      \"pageNumber\": 1\n",
    "    },\n",
    "    \"value\": \"Tid for spill – regjeringens dataspillstrategi 2024–2026\"\n",
    "  },\n",
    "  \"publisher\": {\n",
    "    \"origin\": {\n",
    "      \"type\": \"PAGE\",\n",
    "      \"pageNumber\": 52\n",
    "    },\n",
    "    \"value\": \"Kultur- og likestillingsdepartementet\"\n",
    "  },\n",
    "  \"publicationType\": null,\n",
    "  \"authors\": [],\n",
    "  \"isbn\": null,\n",
    "  \"issn\": null\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-inference)",
   "language": "python",
   "name": "llm-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
