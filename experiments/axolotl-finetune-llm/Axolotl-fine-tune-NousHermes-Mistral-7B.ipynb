{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Nous Hermes 2 Mistral 7B DPO model using Axolotl framework\n",
    "\n",
    "How to install dependencies (in HPC environment):\n",
    "\n",
    "- load Python and cuDNN modules\n",
    "- create a Python venv and activate it\n",
    "- install dependencies from requirements.txt (e.g. torch)\n",
    "- install Axolotl from git clone (pip won't work, see [this issue](https://github.com/OpenAccess-AI-Collective/axolotl/issues/945)):\n",
    "\n",
    "```\n",
    "git clone git@github.com:OpenAccess-AI-Collective/axolotl.git\n",
    "cd axolotl\n",
    "pip install -e '.[flash-attn,deepspeed]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 556 train records\n",
      "Wrote 167 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "KEEP_FIELDS = {\n",
    "    'dc.contributor.author',\n",
    "    'dc.date.issued',\n",
    "    'dc.identifier.isbn',\n",
    "    'dc.language.iso',\n",
    "    'dc.publisher',\n",
    "    'dc.relation.eissn',\n",
    "    'dc.title'    \n",
    "}\n",
    "MAX_TEXT_LENGTH = 3072\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    # subset & JSON encode the ground truth\n",
    "    subset = {key: val\n",
    "              for key, val in sample[\"ground_truth\"].items()\n",
    "              if key in KEEP_FIELDS}\n",
    "    if 'dc.date.issued' in subset:\n",
    "        # keep only the year\n",
    "        subset['dc.date.issued'] = subset['dc.date.issued'][:4]\n",
    "    if 'dc.identifier.isbn' in subset:\n",
    "        # normalize ISBN by stripping dashes\n",
    "        subset['dc.identifier.isbn'] = [isbn.replace('-', '') for isbn in subset['dc.identifier.isbn']]\n",
    "    output = json.dumps(subset)\n",
    "    input_ = sample[\"text\"][:MAX_TEXT_LENGTH]\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'gpt', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: true\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: train.jsonl\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    type: sharegpt\n",
    "    conversation: chatml\n",
    "\n",
    "test_datasets:\n",
    "  - path: eval.jsonl\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    type: sharegpt\n",
    "    conversation: chatml\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "#chat_template: chatml\n",
    "\n",
    "adapter: lora\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 5\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-03-20 12:46:02,741] [INFO] [datasets.<module>:58] [PID:109735] PyTorch version 2.2.0 available.\n",
      "[2024-03-20 12:46:08,946] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-20 12:46:15,360] [INFO] [axolotl.normalize_config:178] [PID:109735] [RANK:0] GPU memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "[2024-03-20 12:46:15,362] [WARNING] [accelerate.utils.other.log:61] [PID:109735] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "                                 dP            dP   dP \n",
      "                                 88            88   88 \n",
      "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
      "                                                       \n",
      "                                                       \n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[2024-03-20 12:46:16,482] [DEBUG] [axolotl.load_tokenizer:255] [PID:109735] [RANK:0] EOS: 32000 / <|im_end|>\u001b[39m\n",
      "[2024-03-20 12:46:16,482] [DEBUG] [axolotl.load_tokenizer:256] [PID:109735] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-03-20 12:46:16,482] [DEBUG] [axolotl.load_tokenizer:257] [PID:109735] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2024-03-20 12:46:16,482] [DEBUG] [axolotl.load_tokenizer:258] [PID:109735] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-03-20 12:46:16,482] [INFO] [axolotl.load_tokenizer:269] [PID:109735] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-03-20 12:46:16,483] [INFO] [axolotl.load_tokenized_prepared_datasets:191] [PID:109735] [RANK:0] Unable to find prepared dataset in last_run_prepared/bc6c1b67d9b5b6cc5730ff1d5a358896\u001b[39m\n",
      "[2024-03-20 12:46:16,483] [INFO] [axolotl.load_tokenized_prepared_datasets:192] [PID:109735] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2024-03-20 12:46:16,483] [WARNING] [axolotl.load_tokenized_prepared_datasets:194] [PID:109735] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2024-03-20 12:46:16,483] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:109735] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
      "Generating train split: 556 examples [00:00, 20344.54 examples/s]\n",
      "Tokenizing Prompts (num_proc=64): 100%|█| 556/556 [00:02<00:00, 223.76 examples/\n",
      "[2024-03-20 12:46:20,261] [INFO] [axolotl.load_tokenized_prepared_datasets:414] [PID:109735] [RANK:0] merging datasets\u001b[39m\n",
      "[2024-03-20 12:46:20,266] [INFO] [axolotl.log:61] [PID:109735] [RANK:0] dropping attention_mask column\u001b[39m\n",
      "Dropping Long Sequences (num_proc=128): 100%|█| 556/556 [00:00<00:00, 590.88 exa\n",
      "Add position_id column (Sample Packing) (num_proc=128): 100%|█| 556/556 [00:01<0\n",
      "[2024-03-20 12:46:25,027] [INFO] [axolotl.load_tokenized_prepared_datasets:427] [PID:109735] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/bc6c1b67d9b5b6cc5730ff1d5a358896\u001b[39m\n",
      "Saving the dataset (1/1 shards): 100%|█| 556/556 [00:00<00:00, 18316.75 examples\n",
      "[2024-03-20 12:46:25,080] [INFO] [axolotl.load_tokenized_prepared_datasets:191] [PID:109735] [RANK:0] Unable to find prepared dataset in last_run_prepared/92dde6b1c6f31eac1e0b6d553e021be9\u001b[39m\n",
      "[2024-03-20 12:46:25,080] [INFO] [axolotl.load_tokenized_prepared_datasets:192] [PID:109735] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2024-03-20 12:46:25,080] [WARNING] [axolotl.load_tokenized_prepared_datasets:194] [PID:109735] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2024-03-20 12:46:25,080] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:109735] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 9431.36 examples/s]\n",
      "num_proc must be <= 32. Reducing num_proc to 32 for dataset of size 32.\n",
      "[2024-03-20 12:46:25,792] [WARNING] [datasets.arrow_dataset.map:3036] [PID:109735] num_proc must be <= 32. Reducing num_proc to 32 for dataset of size 32.\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:01<00:00, 29.33 examples/s]\n",
      "[2024-03-20 12:46:27,113] [INFO] [axolotl.load_tokenized_prepared_datasets:414] [PID:109735] [RANK:0] merging datasets\u001b[39m\n",
      "[2024-03-20 12:46:27,117] [INFO] [axolotl.log:61] [PID:109735] [RANK:0] dropping attention_mask column\u001b[39m\n",
      "num_proc must be <= 32. Reducing num_proc to 32 for dataset of size 32.\n",
      "[2024-03-20 12:46:27,123] [WARNING] [datasets.arrow_dataset.map:3036] [PID:109735] num_proc must be <= 32. Reducing num_proc to 32 for dataset of size 32.\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:00<00:00, 130.20 exampl\n",
      "num_proc must be <= 32. Reducing num_proc to 32 for dataset of size 32.\n",
      "[2024-03-20 12:46:27,589] [WARNING] [datasets.arrow_dataset.map:3036] [PID:109735] num_proc must be <= 32. Reducing num_proc to 32 for dataset of size 32.\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "[2024-03-20 12:46:28,186] [INFO] [axolotl.load_tokenized_prepared_datasets:427] [PID:109735] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/92dde6b1c6f31eac1e0b6d553e021be9\u001b[39m\n",
      "Saving the dataset (1/1 shards): 100%|█| 32/32 [00:00<00:00, 1046.84 examples/s]\n",
      "[2024-03-20 12:46:28,233] [DEBUG] [axolotl.log:61] [PID:109735] [RANK:0] total_num_tokens: 614013\u001b[39m\n",
      "[2024-03-20 12:46:28,241] [DEBUG] [axolotl.log:61] [PID:109735] [RANK:0] `total_supervised_tokens: 72651`\u001b[39m\n",
      "[2024-03-20 12:46:34,679] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:109735] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 614013\u001b[39m\n",
      "[2024-03-20 12:46:34,680] [DEBUG] [axolotl.log:61] [PID:109735] [RANK:0] data_loader_len: 9\u001b[39m\n",
      "[2024-03-20 12:46:34,680] [INFO] [axolotl.log:61] [PID:109735] [RANK:0] sample_packing_eff_est across ranks: [0.9140580340129573]\u001b[39m\n",
      "[2024-03-20 12:46:34,680] [DEBUG] [axolotl.log:61] [PID:109735] [RANK:0] sample_packing_eff_est: 0.92\u001b[39m\n",
      "[2024-03-20 12:46:34,680] [DEBUG] [axolotl.log:61] [PID:109735] [RANK:0] total_num_steps: 45\u001b[39m\n",
      "[2024-03-20 12:46:34,680] [DEBUG] [axolotl.train.log:61] [PID:109735] [RANK:0] loading tokenizer... NousResearch/Nous-Hermes-2-Mistral-7B-DPO\u001b[39m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[2024-03-20 12:46:35,350] [DEBUG] [axolotl.load_tokenizer:255] [PID:109735] [RANK:0] EOS: 32000 / <|im_end|>\u001b[39m\n",
      "[2024-03-20 12:46:35,350] [DEBUG] [axolotl.load_tokenizer:256] [PID:109735] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-03-20 12:46:35,350] [DEBUG] [axolotl.load_tokenizer:257] [PID:109735] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2024-03-20 12:46:35,350] [DEBUG] [axolotl.load_tokenizer:258] [PID:109735] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-03-20 12:46:35,350] [INFO] [axolotl.load_tokenizer:269] [PID:109735] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-03-20 12:46:35,350] [DEBUG] [axolotl.train.log:61] [PID:109735] [RANK:0] loading model and peft_config...\u001b[39m\n",
      "[2024-03-20 12:46:35,488] [INFO] [axolotl.load_model:486] [PID:109735] [RANK:0] patching mistral with flash attention\u001b[39m\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:30<00:00, 10.01s/it]\n",
      "[2024-03-20 12:47:07,176] [INFO] [axolotl.load_model:851] [PID:109735] [RANK:0] GPU memory usage after model load: 7.494GB (+0.145GB cache, +1.301GB misc)\u001b[39m\n",
      "[2024-03-20 12:47:07,186] [INFO] [axolotl.load_model:897] [PID:109735] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
      "[2024-03-20 12:47:07,188] [INFO] [axolotl.load_model:906] [PID:109735] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-03-20 12:47:07,190] [INFO] [axolotl.load_lora:1050] [PID:109735] [RANK:0] found linear modules: ['gate_proj', 'q_proj', 'k_proj', 'v_proj', 'down_proj', 'o_proj', 'up_proj']\u001b[39m\n",
      "trainable params: 41,943,040 || all params: 7,283,691,520 || trainable%: 0.5758486597741032\n",
      "[2024-03-20 12:47:07,633] [INFO] [axolotl.load_model:951] [PID:109735] [RANK:0] GPU memory usage after adapters: 7.650GB (+1.125GB cache, +1.301GB misc)\u001b[39m\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[2024-03-20 12:47:07,638] [WARNING] [accelerate.utils.other.log:61] [PID:109735] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2024-03-20 12:47:07,925] [INFO] [axolotl.train.log:61] [PID:109735] [RANK:0] Pre-saving adapter config to ./out-Nous-Hermes-2-Mistral-7B-DPO\u001b[39m\n",
      "[2024-03-20 12:47:07,971] [INFO] [axolotl.train.log:61] [PID:109735] [RANK:0] Starting trainer...\u001b[39m\n",
      "[2024-03-20 12:47:08,169] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:109735] [RANK:0] packing_efficiency_estimate: 0.92 total_num_tokens per device: 614013\u001b[39m\n",
      "[2024-03-20 12:47:08,169] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:109735] [RANK:0] packing_efficiency_estimate: 0.92 total_num_tokens per device: 614013\u001b[39m\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s][2024-03-20 12:47:09,483] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:109735] [RANK:0] packing_efficiency_estimate: 0.92 total_num_tokens per device: 614013\u001b[39m\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/wrk-vakka/users/oisuomin/git/FinGreyLit/experiments/axolotl-finetune-llm/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/wrk-vakka/users/oisuomin/git/FinGreyLit/experiments/axolotl-finetune-llm/venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 1.0056, 'grad_norm': 14.29641056060791, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "  1%|▍                                           | 1/95 [00:20<31:55, 20.38s/it][2024-03-20 12:47:29,897] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A[2024-03-20 12:47:31,447] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:11,  1.27it/s]\u001b[A[2024-03-20 12:47:33,025] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:14,  1.10s/it]\u001b[A[2024-03-20 12:47:34,557] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 25%|███████████                                 | 4/16 [00:04<00:15,  1.27s/it]\u001b[A[2024-03-20 12:47:36,112] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 31%|█████████████▊                              | 5/16 [00:06<00:14,  1.36s/it]\u001b[A[2024-03-20 12:47:37,652] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 38%|████████████████▌                           | 6/16 [00:07<00:14,  1.42s/it]\u001b[A[2024-03-20 12:47:39,188] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 44%|███████████████████▎                        | 7/16 [00:09<00:13,  1.46s/it]\u001b[A[2024-03-20 12:47:40,742] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 50%|██████████████████████                      | 8/16 [00:10<00:11,  1.49s/it]\u001b[A[2024-03-20 12:47:42,300] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 56%|████████████████████████▊                   | 9/16 [00:12<00:10,  1.51s/it]\u001b[A[2024-03-20 12:47:43,854] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 62%|██████████████████████████▉                | 10/16 [00:13<00:09,  1.52s/it]\u001b[A[2024-03-20 12:47:45,383] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:15<00:07,  1.53s/it]\u001b[A[2024-03-20 12:47:46,939] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:17<00:06,  1.54s/it]\u001b[A[2024-03-20 12:47:48,509] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:18<00:04,  1.54s/it]\u001b[A[2024-03-20 12:47:50,036] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:20<00:03,  1.54s/it]\u001b[A[2024-03-20 12:47:51,599] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:21<00:01,  1.55s/it]\u001b[A[2024-03-20 12:47:53,166] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "[2024-03-20 12:47:54,674] [INFO] [accelerate.accelerator.log:61] [PID:109735] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.1184587478637695, 'eval_runtime': 24.8247, 'eval_samples_per_second': 1.289, 'eval_steps_per_second': 0.645, 'epoch': 0.05}\n",
      "  1%|▍                                           | 1/95 [00:45<31:55, 20.38s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:23<00:00,  1.55s/it]\u001b[A\n",
      "                                                                                \u001b[A[2024-03-20 12:48:14,102] [INFO] [axolotl.callbacks.on_step_end:123] [PID:109735] [RANK:0] GPU memory usage while training: 13.099GB (+14.057GB cache, +1.336GB misc)\u001b[39m\n",
      "{'loss': 1.1489, 'grad_norm': 15.86984920501709, 'learning_rate': 4e-05, 'epoch': 0.1}\n",
      "{'loss': 0.858, 'grad_norm': 13.676074028015137, 'learning_rate': 6e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4653, 'grad_norm': 6.395700931549072, 'learning_rate': 8e-05, 'epoch': 0.2}\n",
      "{'loss': 0.307, 'grad_norm': 4.118259429931641, 'learning_rate': 0.0001, 'epoch': 0.25}\n",
      "{'loss': 0.249, 'grad_norm': 2.598846435546875, 'learning_rate': 0.00012, 'epoch': 0.3}\n",
      "{'loss': 0.1942, 'grad_norm': 1.8049249649047852, 'learning_rate': 0.00014, 'epoch': 0.35}\n",
      "{'loss': 0.1642, 'grad_norm': 1.3978232145309448, 'learning_rate': 0.00016, 'epoch': 0.41}\n",
      "{'loss': 0.1508, 'grad_norm': 1.2853097915649414, 'learning_rate': 0.00018, 'epoch': 0.46}\n",
      "{'loss': 0.1366, 'grad_norm': 1.1367076635360718, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      " 11%|████▌                                      | 10/95 [03:37<27:45, 19.59s/it][2024-03-20 12:50:47,229] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A[2024-03-20 12:50:48,767] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:11,  1.27it/s]\u001b[A[2024-03-20 12:50:50,341] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:14,  1.09s/it]\u001b[A[2024-03-20 12:50:51,862] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 25%|███████████                                 | 4/16 [00:04<00:15,  1.27s/it]\u001b[A[2024-03-20 12:50:53,432] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 31%|█████████████▊                              | 5/16 [00:06<00:14,  1.36s/it]\u001b[A[2024-03-20 12:50:54,967] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 38%|████████████████▌                           | 6/16 [00:07<00:14,  1.42s/it]\u001b[A[2024-03-20 12:50:56,504] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 44%|███████████████████▎                        | 7/16 [00:09<00:13,  1.45s/it]\u001b[A[2024-03-20 12:50:58,025] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 50%|██████████████████████                      | 8/16 [00:10<00:11,  1.49s/it]\u001b[A[2024-03-20 12:50:59,602] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 56%|████████████████████████▊                   | 9/16 [00:12<00:10,  1.52s/it]\u001b[A[2024-03-20 12:51:01,184] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 62%|██████████████████████████▉                | 10/16 [00:13<00:09,  1.52s/it]\u001b[A[2024-03-20 12:51:02,715] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:15<00:07,  1.54s/it]\u001b[A[2024-03-20 12:51:04,307] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:17<00:06,  1.54s/it]\u001b[A[2024-03-20 12:51:05,844] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:18<00:04,  1.54s/it]\u001b[A[2024-03-20 12:51:07,369] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:20<00:03,  1.54s/it]\u001b[A[2024-03-20 12:51:08,916] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:21<00:01,  1.54s/it]\u001b[A[2024-03-20 12:51:10,458] [INFO] [axolotl.monkeypatch.mistral._prepare_decoder_attention_mask:113] [PID:109735] [RANK:0] skipping sliding window mask, not broadcastable with attention mask\u001b[39m\n",
      "[2024-03-20 12:51:11,992] [INFO] [accelerate.accelerator.log:61] [PID:109735] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.13685032725334167, 'eval_runtime': 24.8019, 'eval_samples_per_second': 1.29, 'eval_steps_per_second': 0.645, 'epoch': 0.51}\n",
      " 11%|████▌                                      | 10/95 [04:02<27:45, 19.59s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:23<00:00,  1.55s/it]\u001b[A\n",
      "{'loss': 0.1643, 'grad_norm': 1.0842865705490112, 'learning_rate': 0.0001999317060143023, 'epoch': 0.56}\n",
      "{'loss': 0.1648, 'grad_norm': 1.1569361686706543, 'learning_rate': 0.00019972691733857883, 'epoch': 0.61}\n",
      "{'loss': 0.0706, 'grad_norm': 0.6219239830970764, 'learning_rate': 0.0001993859136895274, 'epoch': 0.66}\n",
      "{'loss': 0.1133, 'grad_norm': 0.6862692832946777, 'learning_rate': 0.0001989091608371146, 'epoch': 0.71}\n",
      "{'loss': 0.134, 'grad_norm': 0.9409552216529846, 'learning_rate': 0.0001982973099683902, 'epoch': 0.76}\n",
      "{'loss': 0.0947, 'grad_norm': 0.6689745783805847, 'learning_rate': 0.00019755119679804367, 'epoch': 0.81}\n",
      "{'loss': 0.0901, 'grad_norm': 0.6291688084602356, 'learning_rate': 0.00019667184042691875, 'epoch': 0.86}\n",
      "{'loss': 0.0934, 'grad_norm': 0.8027587532997131, 'learning_rate': 0.0001956604419500441, 'epoch': 0.91}\n",
      " 19%|████████▏                                  | 18/95 [06:35<25:13, 19.66s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "qlora_model = f\"./out-{MODEL_SHORT_NAME}\"\n",
    "base_model = MODEL_NAME\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"auto\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").eval()\n",
    "model = PeftModel.from_pretrained(model, qlora_model)\n",
    "model = model.merge_and_unload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages):\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "    output_ids = model.generate(\n",
    "        torch.as_tensor(input_ids).cuda(),\n",
    "        #input_ids,\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    output_ids = output_ids[0][len(input_ids[0]):]\n",
    "    return tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f'test-records-{MODEL_SHORT_NAME}.jsonl', 'w') as outfile:\n",
    "    for rec in test_recs:\n",
    "        messages = [\n",
    "            {\"role\": msg[\"from\"], \"content\": msg[\"value\"]}\n",
    "            for msg in rec[\"conversations\"]\n",
    "            if msg[\"from\"] != \"gpt\"\n",
    "        ]\n",
    "        response = generate(messages)\n",
    "\n",
    "        ground_truth = rec['conversations'][-1][\"value\"]\n",
    "\n",
    "        print(100 * \"-\")\n",
    "        print(\"Ground Truth:\")\n",
    "        print(ground_truth)\n",
    "        print(\"Prediction:\")\n",
    "        print(response)\n",
    "\n",
    "        ground_truth = json.loads(ground_truth)\n",
    "\n",
    "        try:\n",
    "            prediction = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            prediction = {}\n",
    "        \n",
    "        # rowid is set to unknown as we've lost it somewhere along the way...\n",
    "        record = {\"ground_truth\": ground_truth, \"prediction\": prediction, \"rowid\": \"unknown\"}\n",
    "        json.dump(record, outfile)\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the statistics of the extracted metadata and save to file\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from eval import MetadataEvaluator\n",
    "\n",
    "evaluator = MetadataEvaluator(f'test-records-{MODEL_SHORT_NAME}.jsonl')\n",
    "results = evaluator.evaluate_records() #prediction_records[:9])\n",
    "# Use only the fields that Meteor extracts\n",
    "fields = [\n",
    "        \"dc.contributor.author\",\n",
    "        \"dc.date.issued\",\n",
    "        \"dc.identifier.isbn\",\n",
    "        \"dc.language.iso\",\n",
    "        \"dc.publisher\",\n",
    "        \"dc.relation.eissn\",\n",
    "        \"dc.title\",\n",
    "    ]\n",
    "statistics_filename = '../results-axolotl-fine-tune-' + MODEL_SHORT_NAME + '.md'\n",
    "evaluator.save_md(results, statistics_filename, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Axolotl)",
   "language": "python",
   "name": "fingreylit-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
