{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c40ed4-71a7-409b-a8d5-b3a00084799a",
   "metadata": {},
   "source": [
    "# Metadata extraction using DSPy and a local LLM using GEPA optimization\n",
    "\n",
    "To run this, you first need to start two local vLLM servers in the backround. These commands are tested on a single A100 80GB in non-exclusive mode (e.g. Turso oversub GPU). The GPT-OSS 120B model has to be partially offloaded to CPU to preserve VRAM.\n",
    "\n",
    "For the main extractor model:\n",
    "\n",
    "    vllm serve google/gemma-3-4b-it --port 7987 --max-model-len 16384 --gpu-memory-utilization 0.25\n",
    "\n",
    "For the reflection model:\n",
    "    \n",
    "    llama-server -hf ggml-org/gpt-oss-120b-GGUF --host 0.0.0.0 --port 7988 --ctx-size 0 --jinja -ub 2048 -b 2048 --n-cpu-moe 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5209ed6-b8db-4c6b-9030-b995851f8c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm Gemma, a large language model created by the Gemma team at Google DeepMind. I’m an open-weights model, which means I’m widely available for public use! \\n\\nI can take text and images as inputs and generate text-based responses. \\n\\nYou can learn more about me on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-4b-it\"  # should match the model vLLM is running (does it matter??)\n",
    "PORT = 7987  # should match the port where vLLM is running\n",
    "MAX_TOKENS = 1024  # limit on how many new tokens to generate (default: 4000)\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "lm = dspy.LM(\"openai/\" + MODEL_ID,\n",
    "             api_base=f\"http://localhost:{PORT}/v1\",  # ensure this points to your port\n",
    "             api_key=\"local\", model_type=\"chat\", max_tokens=MAX_TOKENS, temperature=TEMPERATURE)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# test the connection to the LLM\n",
    "lm(\"Who are you?\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678b2403-0239-4b0c-9bc4-e58caefe2be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’m ChatGPT\\u202f—\\u202fa large language model created by OpenAI. I’ve been trained on a wide variety of text up through June\\u202f2024, which lets me help with things like answering questions, brainstorming ideas, explaining concepts, drafting or editing writing, solving problems, and much more. I don’t have personal experiences or consciousness, and I can’t browse the web in real time, but I can draw on the information I was trained on to generate useful, context‑aware responses.  \\n\\nIf there’s anything specific you’d like to know or discuss, just let me know!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "REFLECTION_MODEL_ID = \"ggml-org/gpt-oss-120b-GGUF\"\n",
    "REFLECTION_PORT = PORT + 1\n",
    "REFLECTION_MAX_TOKENS = 7988\n",
    "\n",
    "reflection_lm = dspy.LM(\"openai/\" + REFLECTION_MODEL_ID,\n",
    "             api_base=f\"http://localhost:{REFLECTION_PORT}/v1\",  # ensure this points to your port\n",
    "             api_key=\"local\", model_type=\"chat\", max_tokens=REFLECTION_MAX_TOKENS, temperature=TEMPERATURE)\n",
    "\n",
    "# test the connection to the LLM\n",
    "reflection_lm(\"Who are you?\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875a6803-72a9-43a2-8cf9-f27be5322401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 64, 182)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of validation set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "VAL_SIZE = 64  # how many documents to validate on during optimization\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    # fix some bad field names\n",
    "    ground_truth = { fld.replace('-', '_'): val for fld, val in sample[\"ground_truth\"].items() }\n",
    "    output = json.dumps(ground_truth)\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    return dspy.Example({\"content\": input_, \"metadata\": output}).with_inputs(\"content\")\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "\n",
    "train_val_set = dataset_to_records(train_files)\n",
    "random.shuffle(train_val_set)\n",
    "\n",
    "train_set = train_val_set[VAL_SIZE:]\n",
    "val_set = train_val_set[:VAL_SIZE]\n",
    "\n",
    "test_set = dataset_to_records(test_files)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e18e61-353c-47cd-b0f7-2891a9269194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Message:\n",
      "{\"pdfinfo\": {\"creationDate\": \"D:20201214215341+01'00'\", \"modDate\": \"D:20201214215418+01'00'\"}, \"pages\": [{\"page\": 1, \"text\": \"# ANTAA TAITEEN OPETTAA\\n\\n\\n\"}, {\"page\": 3, \"text\": \"ANTA A TAITEEN OPETTA A GERT BIESTA\\n\\n\\n\"}, {\"page\": 4, \"text\": \"00:00:08.18\\n\\n\\n\"}, {\"page\": 5, \"text\": \"00:00:36.03 00:00:52.19 00:00:54.19\\n\\n\\n\"}, {\"page\": 6, \"text\": \"00:00:58.16 00:01:00.17 00:01:0\\n\\n\\n\"}, {\"page\": 65, \"text\": \"\\u2018Opastan sinua kaikessa, n\\u00e4yt\\u00e4n sinulle kaiken ja nime\\u00e4n kaiken.\\u2019\\n\\u2014 COMENIUS\\nT\\u00e4ss\\u00e4 kirjassa Gert Biesta esitt\\u00e4\\u00e4 uuden n\\u00e4kemyksen nykyaikaisesta taidekasvatuksesta\\n\\nosoittamalla, ett\\u00e4 taide tarjoaa ainutlaatuisia v\\u00e4lineit\\u00e4 olla dialogissa maailman kanssa. N\\u00e4kemys\\n\\nperustuu ajatukseen, ett\\u00e4 opettaminen on n\\u00e4ytt\\u00e4mist\\u00e4. Opettaja n\\u00e4ytt\\u00e4\\u00e4 oppilaalle millaisiin\\n\\nhyviin, t\\u00e4rkeisiin tai merkitt\\u00e4viin asioihin maailmassa voisi kiinnitt\\u00e4\\u00e4 huomiota. Biesta havainnol\\nlistaa asiaa ottamalla l\\u00e4ht\\u00f6kohdaksi Joseph Beuysin vuonna 1965 esitt\\u00e4m\\u00e4n performanssin\\n_Kuinka selitt\\u00e4\\u00e4 kuvia kuolleelle j\\u00e4nikselle_ . Kirjassa on useita kuvia t\\u00e4st\\u00e4 tapahtumasta.\\nGERT BIESTA (www.gertbiesta.com) on kasvatustieteen professori Lontoon Brunel\\n\\nyliopistossa sek\\u00e4 kasvatustieteen NIVOZ professori Humanististen tieteiden yliopistossa\\nAlankomaissa. H\\u00e4n on my\\u00f6s vieraileva professori NLA yliopistossa Bergeniss\\u00e4, Norjassa.\\nVuoteen 2016 asti h\\u00e4n oli vieraileva professori Taideyliopisto ArtEZissa Alankomaissa. Vuosina\\n\\n1999\\u20132014 h\\u00e4n oli _Studies in Philosophy and Education_ lehden p\\u00e4\\u00e4toimittaja ja vuodesta\\n\\n2016 l\\u00e4htien _Educational Theory_ lehden toimituskunnan j\\u00e4sen. H\\u00e4nell\\u00e4 on runsaasti kasvatuksen\\n\\nteorian ja filosofian sek\\u00e4 kasvatus- ja sosiaalitieteiden alaan kuuluvia julkaisuja. H\\u00e4nen\\n\\nmonografioitaan ovat _Beyond Learning; Democratic Education for a Human Future_ (Paradigm\\nPublishers 2006, kirja voitti vuonna 2008 Amerikan kasvatustieteellisen yhdistyksen\\n\\n(AESA) Kriitikon valinta palkinnon), _Good Education in an Age of Measurement: Ethics, politics,_\\n\\n_democracy_ (Paradigm Publishers 2010) sek\\u00e4 _The Beautiful Risk of Education_ (Paradigm\\nPublishers 2014, kirja sai Amerikan kasvatuksen tutkimuksen yhdistyksen (AERA, Division B)\\n\\npalkinnon samana vuonna). H\\u00e4nen tuotantoaan on t\\u00e4h\\u00e4n menness\\u00e4 k\\u00e4\\u00e4nnetty 16 kielelle\\n\\n(englanti, hollanti, saksa, ruotsi, suomi, islanti, italia, espanja, katalaani, portugali, puola, romania,\\n\\nven\\u00e4j\\u00e4, kiina ja japani). Vuosina 2011\\u20132012 h\\u00e4n toimi Yhdysvaltojen kasvatusfilofisen\\n\\nyhdistyksen puheenjohtajana.\\nARTEZ UNIVERSITY OF THE ARTS on iso taideakatemia Alankomaissa.\\nArtEZ kouluttaa ammatteihin, joissa taide, tieto ja luovuus ovat keskeisi\\u00e4. ArtEZ Press,\\nArtEZin oma kustantamo. Filosofiamme on, ett\\u00e4 jokaisen ArtEZ Press -lehden tulisi olla arvokas\\n\\ntaiteelle ja yhteiskunnalle. Julkaisumme ohjaavat muutosta ja innovaatioita, tarjoavat uusia\\n\\nn\\u00e4k\\u00f6kulmia ja osoittavat taiteen voiman. Ne ovat inspiroiva tietol\\u00e4hde opiskelijoille, taiteilijoille,\\n\\ntutkijoille ja kaikille taiteesta, kulttuurista ja koulutuksesta kiinnostuneille. Julkaisumme\\n\\nstimuloivat monimuotoisuutta ja rikkautta teoriassa ja k\\u00e4yt\\u00e4nn\\u00f6ss\\u00e4.\\nVuonna 2005 perustettu kustantamomme julkaisee sis\\u00e4ll\\u00f6lt\\u00e4\\u00e4n ja muotoilultaan\\n\\nkorkealaatuisia kirjoja. Ainutlaatuinen muotoilu tukee sis\\u00e4lt\\u00f6\\u00e4 ja p\\u00e4invastoin. ArtEZ Pressin\\n\\nvuosien varrella rakentama huolellisesti kuratoitu kokoelma edustaa monia aloja, kuten\\n\\narkkitehtuuri, kuvataide, tanssi, sisustus, taidekasvatus, muoti, musiikki, musiikkiterapia,\\n\\ntuotesuunnittelu, teatteri, muotoilu ja taideteoria.\\n\\n\\n\"}, {\"page\": 66, \"text\": \"# TAIDEKASVATUS JOSEPH BEUYSIN \\u2018J\\u00c4LKEEN\\u2019\\n\\n\\n\"}]}\n",
      "\n",
      "\n",
      "Gold Answer:\n",
      "language: fi\n",
      "title: Antaa taiteen opettaa : taidekasvatus Joseph Beuysin 'jälkeen'\n",
      "creator: ['Biesta, Gert']\n",
      "year: 2020\n",
      "publisher: ['ArtEZ Press', 'Taideyliopisto']\n",
      "e_isbn: ['9789523291928']\n",
      "type_coar: book\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Message:\")\n",
    "print(train_set[-1]['content'])\n",
    "\n",
    "print(\"\\n\\nGold Answer:\")\n",
    "for k, v in json.loads(train_set[-1]['metadata']).items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b029db-7ee1-4e8c-97f9-353ce5db1d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The text describes an announcement by Apple Inc. regarding the iPhone 14. The CEO, Tim Cook, is mentioned, indicating a press release. The information provided is sufficient to identify the main entities and the type of resource.',\n",
      "    language='en',\n",
      "    title='Apple Inc. Announces iPhone 14',\n",
      "    alt_title=[],\n",
      "    creator=['Apple Inc.', 'Tim Cook'],\n",
      "    year=None,\n",
      "    publisher=['Apple Inc.'],\n",
      "    doi=None,\n",
      "    e_isbn=[],\n",
      "    p_isbn=[],\n",
      "    e_issn=None,\n",
      "    p_issn=None,\n",
      "    type_coar='News Article'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class ExtractInfo(dspy.Signature):\n",
    "    \"\"\"Extract structured metadata from text extracted from a PDF.\"\"\"\n",
    "\n",
    "    content: str = dspy.InputField()\n",
    "    language: str = dspy.OutputField(desc=\"The language of the resource expressed as a BCP47 language tag.\")\n",
    "    title: str = dspy.OutputField(desc=\"The main title of the publication.\")\n",
    "    alt_title: list[str] = dspy.OutputField(desc=\"Alternative or parallel titles of the publication, suffixed with a BCP47 language tag in curly brackets.\")\n",
    "    creator: list[str] = dspy.OutputField(desc=\"The primary author(s) of the resource (order: Last Name, First Names).\")\n",
    "    year: Optional[str] = dspy.OutputField(desc=\"The year on which the resource was issued or made available.\")\n",
    "    publisher: list[str] = dspy.OutputField(desc=\"The entity/entities responsible for making the resource available.\")\n",
    "    doi: Optional[str] = dspy.OutputField(desc=\"The Digital Object Identifier (DOI) associated with the resource.\")\n",
    "    e_isbn: list[str] = dspy.OutputField(desc=\"The ISBN associated with the electronic resource.\")\n",
    "    p_isbn: list[str] = dspy.OutputField(desc=\"The ISBN of the printed version of this document.\")\n",
    "    e_issn: Optional[str] = dspy.OutputField(desc=\"The ISSN associated with the electronic resource.\")\n",
    "    p_issn: Optional[str] = dspy.OutputField(desc=\"The ISSN of the printed version of this document.\")\n",
    "    type_coar: str = dspy.OutputField(desc=\"The type of the resource according to the COAR Resource Types classification.\")\n",
    "\n",
    "module = dspy.ChainOfThought(ExtractInfo)\n",
    "\n",
    "text = \"Apple Inc. announced its latest iPhone 14 today.\" \\\n",
    "    \"The CEO, Tim Cook, highlighted its new features in a press release.\"\n",
    "response = module(content=text)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de37b1d5-1180-434e-bf12-1b38f9297006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "ALMOST_THRESHOLD = 0.95  # Adjust as needed\n",
    "\n",
    "def feedback_simple_string(field, true_val, pred_val):\n",
    "    score = 1.0 if true_val == pred_val else 0.0\n",
    "    if score == 1.0:\n",
    "        feedback = f\"✅ `{field}` is correct: `{true_val}`.\"\n",
    "    else:\n",
    "        feedback = f\"❌ `{field}` is incorrect. You predicted `{pred_val}`, but the correct value is `{true_val}`.\"\n",
    "    return score, feedback\n",
    "\n",
    "def feedback_fuzzy_string(field, true_val, pred_val):\n",
    "    base_score = 1.0 if true_val == pred_val else 0.0\n",
    "    if base_score == 1.0 or (true_val and pred_val and Levenshtein.ratio(true_val.lower(), pred_val.lower()) >= ALMOST_THRESHOLD):\n",
    "        score = 1.0\n",
    "        feedback = f\"✅ `{field}` is approximately correct: `{pred_val}` matches `{true_val}` closely.\"\n",
    "    else:\n",
    "        score = 0.0\n",
    "        feedback = f\"❌ `{field}` is incorrect. You predicted `{pred_val}`, but the correct value is `{true_val}`.\"\n",
    "    return score, feedback\n",
    "\n",
    "def feedback_set(field, true_val, pred_val):\n",
    "    true_set = set(true_val or [])\n",
    "    pred_set = set(pred_val or [])\n",
    "\n",
    "    if not true_set and not pred_set:\n",
    "        return 1.0, f\"✅ `{field}` is empty as expected.\"\n",
    "    elif not true_set or not pred_set:\n",
    "        return 0.0, f\"❌ `{field}` is incorrect. Expected `{true_set}`, but got `{pred_set}`.\"\n",
    "\n",
    "    tp = len(true_set & pred_set)\n",
    "    fp = len(pred_set - true_set)\n",
    "    fn = len(true_set - pred_set)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    feedback = f\"🔍 `{field}` partial match.\"\n",
    "    feedback += f\"- Correctly included: `{list(true_set & pred_set)}`\\n\"\n",
    "    if fp:\n",
    "        feedback += f\"- Incorrectly included: `{list(pred_set - true_set)}`\\n\"\n",
    "    if fn:\n",
    "        feedback += f\"- Missed: `{list(true_set - pred_set)}`\"\n",
    "\n",
    "    return f1, feedback.strip()\n",
    "\n",
    "def feedback_e_issn(field, true_val, pred_val, p_issn_val):\n",
    "    if true_val == pred_val:\n",
    "        return 1.0, f\"✅ `{field}` is correct: `{true_val}`.\"\n",
    "    elif p_issn_val and pred_val == p_issn_val and true_val is None:\n",
    "        return 1.0, f\"✅ `{field}` is correctly inferred from `p_issn`: `{pred_val}`.\"\n",
    "    else:\n",
    "        return 0.0, f\"❌ `{field}` is incorrect. You predicted `{pred_val}`, but the correct value is `{true_val}`.\"\n",
    "\n",
    "def metadata_metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    fields = [\n",
    "        'language', 'title', 'creator', 'year', 'publisher',\n",
    "        'doi', 'e_isbn', 'p_isbn', 'e_issn', 'p_issn', 'type_coar'\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    feedback_parts = []\n",
    "\n",
    "    metadata = json.loads(example.get(\"metadata\", \"{}\"))\n",
    "    ground_truth = example.get(\"ground_truth\", {})\n",
    "\n",
    "    for field in fields:\n",
    "        true_val = metadata.get(field)\n",
    "        pred_val = pred.get(field) or None\n",
    "\n",
    "        if field in ['language', 'year', 'doi', 'p_issn', 'type_coar']:\n",
    "            score, feedback = feedback_simple_string(field, true_val, pred_val)\n",
    "        elif field == 'title':\n",
    "            score, feedback = feedback_fuzzy_string(field, true_val, pred_val)\n",
    "        elif field in ['creator', 'publisher', 'e_isbn', 'p_isbn']:\n",
    "            score, feedback = feedback_set(field, true_val, pred_val)\n",
    "        elif field == 'e_issn':\n",
    "            p_issn_val = ground_truth.get(\"p_issn\")\n",
    "            score, feedback = feedback_e_issn(field, true_val, pred_val, p_issn_val)\n",
    "        else:\n",
    "            score, feedback = feedback_simple_string(field, true_val, pred_val)\n",
    "\n",
    "        scores.append(score)\n",
    "        feedback_parts.append(feedback)\n",
    "\n",
    "    overall_score = sum(scores) / len(scores) if scores else 0\n",
    "    full_feedback = \"\\n\".join(feedback_parts)\n",
    "\n",
    "    return dspy.Prediction(score=overall_score, feedback=full_feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da690986-d2a8-445b-9ec7-25c1387acf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=metadata_metric_with_feedback,\n",
    "    auto=\"heavy\",\n",
    "    num_threads=64,\n",
    "    track_stats=False,\n",
    "    use_merge=True,\n",
    "    reflection_lm=reflection_lm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a88bd083-01f4-40dc-aa87-dce2f0e7936a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:18:15 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1483 metric calls of the program. This amounts to 2.32 full evals on the train+val set.\n",
      "2025/09/30 09:18:15 INFO dspy.teleprompt.gepa.gepa: Using 64 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
      "GEPA Optimization:   0%|          | 0/1483 [00:00<?, ?rollouts/s]2025/09/30 09:18:15 INFO dspy.evaluate.evaluate: Average Metric: 38.1608225108225 / 64 (59.6%)\n",
      "2025/09/30 09:18:15 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.5962628517316018\n",
      "GEPA Optimization:   4%|▍         | 64/1483 [00:00<00:15, 91.89rollouts/s]2025/09/30 09:18:15 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.5962628517316018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.91 / 3 (63.6%): 100%|██████████| 3/3 [00:00<00:00, 111.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:18:15 INFO dspy.evaluate.evaluate: Average Metric: 1.9090909090909092 / 3 (63.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:19:35 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for predict: **Task**: Extract structured bibliographic metadata from the JSON representation of a PDF document.  \n",
      "The input is a JSON object with two top‑level keys:\n",
      "\n",
      "* `pdfinfo` – metadata extracted from the PDF file (may contain `title`, `author`, `creationDate`, `modDate`, etc.).\n",
      "* `pages` – a list of page objects, each with `page` (number) and `text` (the OCR‑extracted plain text of that page).\n",
      "\n",
      "Your job is to produce **one JSON object** containing the fields listed below.  \n",
      "If a field cannot be determined, use the exact empty value specified (e.g. `null` for a missing string, `[]` for an empty list).\n",
      "\n",
      "---\n",
      "\n",
      "### Output JSON Schema\n",
      "\n",
      "| Field | Type | Description | Required format |\n",
      "|-------|------|-------------|-----------------|\n",
      "| `language` | string | ISO‑639‑1 language code of the document. Detect from the text: if the text contains Finnish‑specific characters (ä, ö, Ä, Ö, å, Å) or obvious Finnish words, use `fi`; otherwise default to `en`. | `\"fi\"` or `\"en\"` |\n",
      "| `title` | string | Main title of the work. Prefer `pdfinfo.title` if present; otherwise use the first prominent heading (e.g. the line that looks like a title on page 1). Remove surrounding whitespace, normalise quotation marks to plain `\"` and `’`, and collapse multiple spaces. | plain string |\n",
      "| `alt_title` | list of strings | Any alternative titles found (e.g. subtitles separated by a colon, titles in other languages, or titles inside quotation marks). Return each alternative title as a separate element, already cleaned as described for `title`. If none, return an empty list. | `[]` or `[\"Alt 1\", \"Alt 2\"]` |\n",
      "| `creator` | list of strings | Author(s) in **“Surname, Given‑Name”** order. Use `pdfinfo.author` if present; otherwise locate the author line in the text (e.g. “Author: …”, “By …”, or a line that contains a name near the top of the document). If a name appears as “First Last”, convert it to “Last, First”. For multiple authors, return one entry per author. If no author can be identified, return an empty list. | `[]` or `[\"Kauppinen, Petri\"]` |\n",
      "| `year` | integer | Publication year. Prefer the four‑digit year from `pdfinfo.creationDate` or `pdfinfo.modDate`. If those are missing, search the full text for the first occurrence of a four‑digit year that is plausibly a publication year (e.g. 1900‑2099). | e.g. `2020` |\n",
      "| `publisher` | list of strings | The institution or publisher responsible for the work. **For theses/dissertations** use the awarding university or faculty (e.g. “Åbo Akademi University”). **For journal articles** leave the list empty (`[]`). If a clear commercial publisher is mentioned (e.g. “Springer”), include it. Return each distinct publisher as a separate element. | `[]` or `[\"Åbo Akademi University\"]` |\n",
      "| `doi` | string or null | DOI if present. Detect patterns like `10.<digits>/<suffix>` (case‑insensitive). Return the DOI **without** a leading URL (e.g. `10.1000/xyz123`). If none, return `null`. |\n",
      "| `e_isbn` | list of strings | Digital (electronic) ISBN(s). Locate strings that match the ISBN‑13 pattern (13 digits, optionally with hyphens). If the surrounding text contains the word **digital**, **electronic**, **e‑ISBN**, or “(digital)”, treat that ISBN as electronic. Return the ISBN stripped of all hyphens (e.g. `9789521241352`). If none, return an empty list. |\n",
      "| `p_isbn` | list of strings | Print ISBN(s). Same detection as `e_isbn` but the surrounding text must contain **print**, **paper**, **hardcover**, or “(print)”. Return hyphen‑free numbers. |\n",
      "| `e_issn` | string or null | Electronic ISSN, detected similarly to ISBN (8 digits, optionally hyphenated). Use the same “digital/e‑ISSN” cue. Return hyphen‑free string or `null`. |\n",
      "| `p_issn` | string or null | Print ISSN, using the “print/pp‑ISSN” cue. Return hyphen‑free string or `null`. |\n",
      "| `type_coar` | string | COAR‑compatible type of the resource. Determine from clues in the text or metadata: <br>• Contains “master’s thesis”, “master thesis”, “maisteri”, “maisterintutkielma” → **`master thesis`** <br>• Contains “doctoral thesis”, “dissertation”, “PhD”, “doctoral” → **`doctoral thesis`** <br>• Contains journal‑style citation elements (journal name, volume, issue, pages) → **`journal article`** <br>• Contains “conference”, “proceedings”, “paper presented at” → **`conference proceeding`** <br>• Otherwise, if it is a research report or article not clearly a journal article, use **`research`**. Return the exact string (lower‑case, spaces as shown). |\n",
      "\n",
      "---\n",
      "\n",
      "### General Extraction Strategy (to be followed for every request)\n",
      "\n",
      "1. **Parse the JSON input** safely; ignore any fields that are not required.\n",
      "2. **Normalize dates**: `creationDate`/`modDate` are strings like `D:20201216144002+02'00'`. Extract the first four digits as the year.\n",
      "3. **Detect language** early: scan the first 200 characters of the combined text for Finnish‑specific characters. If found, set `language` to `fi`; otherwise `en`.\n",
      "4. **Title extraction**:  \n",
      "   * If `pdfinfo.title` exists, clean it (strip, collapse spaces, normalise quotes).  \n",
      "   * If not, take the first non‑empty line on page 1 that is longer than 5 characters and does not start with “##” or “###”. Clean it the same way.  \n",
      "   * If the title contains a colon, treat the part after the colon as a possible `alt_title` (but also keep the whole string as `title`).\n",
      "5. **Creator extraction**:  \n",
      "   * Use `pdfinfo.author` if present; split on commas or semicolons.  \n",
      "   * If the name looks like “First Last” (two words, first capitalised, second capitalised), reorder to “Last, First”.  \n",
      "   * For multiple authors, repeat the reordering for each.\n",
      "6. **Publisher detection**:  \n",
      "   * Search for university or faculty names (keywords: “University”, “Universität”, “Akademi”, “Institute”, “College”, “School”, “Faculty”).  \n",
      "   * If the document is a thesis/dissertation (see step 9), the first such institution is the publisher.  \n",
      "   * For journal articles, leave empty.\n",
      "7. **ISBN / ISSN extraction**:  \n",
      "   * Use regular expressions: ISBN‑13 → `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b` (13 digits total). <br>* ISSN → `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`.  \n",
      "   * After a match, look at up to 20 characters before/after for the cues “digital”, “electronic”, “e‑ISBN”, “(digital)”, “print”, “paper”, “hardcover”, “(print)”. Assign to the appropriate list.  \n",
      "   * Remove all hyphens and spaces before storing.\n",
      "8. **DOI extraction**: regex `10\\.\\d{4,9}/\\S+` (stop at whitespace or punctuation). Strip trailing punctuation.\n",
      "9. **Resource type (type_coar)**:  \n",
      "   * Scan the whole text for the keywords listed in the table above. Use the first matching rule in the order given (doctoral > master > journal article > conference proceeding > research).  \n",
      "   * Normalize to lower‑case with a single space (e.g., `master thesis`).\n",
      "10. **Assemble the output** exactly following the schema, preserving the order of keys as shown in the table (though JSON order is not strict, keeping the order helps readability).\n",
      "\n",
      "---\n",
      "\n",
      "### Example of a Correct Output (formatted)\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Kasvua kohti moniulotteista kehollisuutta\",\n",
      "  \"alt_title\": [],\n",
      "  \"creator\": [\"Kauppinen, Petri\"],\n",
      "  \"year\": 2020,\n",
      "  \"publisher\": [],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": null,\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"journal article\"\n",
      "}\n",
      "2025/09/30 09:19:52 INFO dspy.evaluate.evaluate: Average Metric: 2.1818181818181817 / 3 (72.7%)\n",
      "2025/09/30 09:20:26 INFO dspy.evaluate.evaluate: Average Metric: 36.75757575757574 / 64 (57.4%)\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset score for new program: 0.5743371212121212\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full train_val score for new program: 0.5743371212121212\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Individual valset scores for new program: [0.6363636363636364, 0.45454545454545453, 0.45454545454545453, 0.36363636363636365, 0.6363636363636364, 0.36363636363636365, 0.7272727272727273, 0.6363636363636364, 0.4848484848484848, 0.6363636363636364, 0.8181818181818182, 0.2727272727272727, 0.5454545454545454, 0.45454545454545453, 0.36363636363636365, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.36363636363636365, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.36363636363636365, 0.8181818181818182, 0.36363636363636365, 1.0, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.45454545454545453, 0.7272727272727273, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.36363636363636365, 0.36363636363636365, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.6363636363636364, 0.2727272727272727, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364]\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New valset pareto front scores: [0.696969696969697, 0.6363636363636364, 0.7878787878787878, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364, 0.4805194805194805, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.36363636363636365, 0.8181818181818182, 0.5151515151515151, 1.0, 0.8181818181818182, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.45454545454545453, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.45454545454545453, 0.45454545454545453, 0.6363636363636364, 0.6590909090909091, 0.5454545454545454, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364]\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset pareto front score: 0.6399655032467533\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Updated valset pareto front programs: [{0}, {0}, {0}, {0, 1}, {0}, {0}, {0, 1}, {0}, {0}, {1}, {1}, {0}, {0}, {0}, {0}, {1}, {1}, {1}, {0}, {1}, {1}, {0, 1}, {1}, {1}, {1}, {1}, {0, 1}, {1}, {1}, {0}, {1}, {0}, {0, 1}, {1}, {1}, {0, 1}, {1}, {0}, {1}, {0}, {0, 1}, {1}, {1}, {1}, {1}, {0}, {0}, {0, 1}, {0}, {0}, {1}, {0}, {0}, {0}, {0}, {1}, {1}, {0}, {0}, {0}, {1}, {1}, {1}, {0, 1}]\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best valset aggregate score so far: 0.5962628517316018\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on train_val: 0\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on valset: 0\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on valset: 0.5962628517316018\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on train_val: 0.5962628517316018\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Linear pareto front program index: 0\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program candidate index: 1\n",
      "GEPA Optimization:   9%|▉         | 134/1483 [02:11<25:37,  1.14s/rollouts]2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 2: No merge candidates found\n",
      "2025/09/30 09:20:26 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 0 score: 0.5962628517316018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.73 / 3 (57.6%): 100%|██████████| 3/3 [00:07<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:20:33 INFO dspy.evaluate.evaluate: Average Metric: 1.727272727272727 / 3 (57.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:21:35 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for predict: markdown\n",
      "# Instruction: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will receive a JSON object that contains two parts:\n",
      "\n",
      "* **pdfinfo** – a dictionary with the PDF’s internal metadata (title, author, creationDate, …).  \n",
      "* **pages** – a list of pages, each with a `page` number and the plain‑text that was extracted from that page.\n",
      "\n",
      "Your task is to **produce a single JSON object** that contains the following fields **exactly** (order does not matter).  \n",
      "All values must be of the type shown in the examples (strings, numbers, lists, or `null`).  \n",
      "If a field cannot be determined, use `null` for a scalar field or an empty list `[]` for a list field.\n",
      "\n",
      "| Field | Expected type | How to obtain / rules |\n",
      "|-------|---------------|-----------------------|\n",
      "| `language` | string (ISO‑639‑1 code) | Detect the primary language of the document’s main body text (the bulk of the pages). Use `fi` for Finnish, `en` for English, `sv` for Swedish, etc. Do **not** default to Finnish. |\n",
      "| `title` | string | The **main title** of the work. Prefer the title shown on the first page (usually a heading) or the `pdfinfo.title` if it matches the page title. Remove any leading series identifiers (e.g., “ePooki 34/2020”, “Sibelius‑Akatemian …”) and trim surrounding whitespace. |\n",
      "| `alt_title` | list of strings | Any alternative title that appears in the document, typically a translation in another language or a subtitle separated by a colon. Include each distinct alternative title as a separate list element. If none, use `[]`. |\n",
      "| `creator` | list of strings | All author names **in the order they appear** in the document. Each name must be formatted as `\"Lastname, Firstname\"` (surname first, a comma, a space, then given name(s)). If a name already appears as “Firstname Lastname”, split it correctly. Do not include duplicate entries. |\n",
      "| `year` | integer | The year of publication. Extract from `pdfinfo.creationDate` (format `D:YYYY…`) or from a visible date on the first pages (e.g., “2020”). |\n",
      "| `publisher` | list of strings | Institution or organisation that published the work. Typical sources: university name, faculty, research institute, or journal name. Return each distinct publisher as a separate list element. |\n",
      "| `doi` | string or null | DOI if present (look for patterns `10.\\d{4,9}/[-._;()/:A-Z0-9]+`). If none, return `null`. |\n",
      "| `e_isbn` | list of strings | All **electronic** ISBNs found (pattern `ISBN\\s*[:=]?\\s*[\\d-]+`). Return each as a string without the “ISBN” label. |\n",
      "| `p_isbn` | list of strings | All **print** ISBNs found (same pattern). If the document does not distinguish, put the numbers in `e_isbn`. |\n",
      "| `e_issn` | string or null | Electronic ISSN if present (pattern `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[\\dxX]`). Return the pure ISSN (e.g., `\"1798-2022\"`). |\n",
      "| `p_issn` | string or null | Print ISSN if present (same pattern). |\n",
      "| `type_coar` | string | COAR‑compatible type of the work. Determine from contextual clues: <br>• If the document is a **master’s thesis** (keywords like “Master’s thesis”, “Master’s degree programme”, “opinnäytetyö”), use `\"master thesis\"`.<br>• If it is a **bachelor’s thesis** (keywords like “Kandidatavhandling”, “bachelor thesis”, “avhandling” without master‑level wording), use `\"bachelor thesis\"`.<br>• If it is a **journal article** (appears in a journal, has volume/issue numbers, or contains a DOI), use `\"journal article\"`.<br>• If it is a **research report** (no thesis wording, often labelled “Report”, “Opinnäytetyö” in a non‑degree context), use `\"research report\"`.<br>• If it is a **book** or **monograph**, use `\"book\"`.<br>If none of the above apply, default to `\"other\"`.\n",
      "\n",
      "## Extraction Strategy (for reference)\n",
      "\n",
      "1. **Parse `pdfinfo.creationDate`** to obtain the year (first four digits after `D:`).  \n",
      "2. **Detect language** by counting common stop‑words for Finnish, English, and Swedish across the body pages; pick the language with the highest count.  \n",
      "3. **Title extraction**  \n",
      "   * Look at page 1 text. The first line that is all‑caps or heading‑style (often preceded/followed by blank lines) is the title.  \n",
      "   * If the line starts with a series identifier (e.g., “ePooki 34/2020”), strip that part.  \n",
      "   * If a colon is present, keep the whole line as the title; the part after the colon may also be stored in `alt_title` if it appears as a translation elsewhere.  \n",
      "4. **Alternative title** – search the document for a line that is a direct translation of the main title (often in a different language or in brackets). Add each distinct occurrence to `alt_title`.  \n",
      "5. **Authors** – locate the author line(s) on page 1 (often a list of names with hyperlinks). Split each name into surname and given name(s). Preserve the order.  \n",
      "6. **Publisher** – common sources: a line containing “University of …”, “School of …”, “Institute of …”, or a line labelled “Metatiedot”. Return each distinct entity.  \n",
      "7. **DOI / ISBN / ISSN** – use regular expressions to find these identifiers anywhere in the pages. Distinguish electronic vs. print ISBN/ISSN only if the surrounding text explicitly says “e‑ISBN”, “print ISBN”, “e‑ISSN”, etc.; otherwise place the number in the electronic field.  \n",
      "8. **COAR type** – apply the keyword rules above.  \n",
      "\n",
      "## Output Format\n",
      "\n",
      "Return **only** the JSON object, no additional text or explanation. Example:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Ikätasoinen seksuaalikasvatus on lapsen turva\",\n",
      "  \"alt_title\": [\"Age‑appropriate sexual education is a child's safety\"],\n",
      "  \"creator\": [\"Sikala, Irmeli\", \"Myllykangas, Kirsi\", \"Tölli, Sirpa\", \"Tuura, Jaana\"],\n",
      "  \"year\": 2020,\n",
      "  \"publisher\": [\"Oulun ammattikorkeakoulu\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": \"1798-2022\",\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"journal article\"\n",
      "}\n",
      "2025/09/30 09:21:41 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/09/30 09:22:13 INFO dspy.evaluate.evaluate: Average Metric: 38.538816738816735 / 64 (60.2%)\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New program is on the linear pareto front\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full valset score for new program: 0.6021690115440116\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full train_val score for new program: 0.6021690115440116\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Individual valset scores for new program: [0.8181818181818182, 0.45454545454545453, 0.6060606060606061, 0.2727272727272727, 0.7272727272727273, 0.2727272727272727, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.45454545454545453, 0.6363636363636364, 0.6060606060606061, 0.2987012987012987, 0.6565656565656566, 0.36363636363636365, 0.6363636363636364, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.696969696969697, 0.5151515151515151, 0.9545454545454546, 0.5454545454545454, 0.8181818181818182, 0.45454545454545453, 0.6272727272727273, 0.696969696969697, 0.45454545454545453, 0.9090909090909091, 0.7272727272727273, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.5454545454545454, 0.8181818181818182, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.3090909090909091, 0.36363636363636365, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454, 0.5454545454545454, 0.4848484848484848, 0.5714285714285714, 0.8181818181818182, 0.45454545454545453, 0.45454545454545453, 0.6060606060606061]\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New valset pareto front scores: [0.8181818181818182, 0.6363636363636364, 0.7878787878787878, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364, 0.4805194805194805, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.5454545454545454, 0.6363636363636364, 0.5454545454545454, 0.696969696969697, 0.6363636363636364, 0.9545454545454546, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.6272727272727273, 0.8181818181818182, 0.5151515151515151, 1.0, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.6590909090909091, 0.5714285714285714, 0.8181818181818182, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364]\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full valset pareto front score: 0.666507711038961\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Updated valset pareto front programs: [{2}, {0}, {0}, {0, 1}, {0, 2}, {0}, {0, 1, 2}, {0}, {0}, {2}, {1}, {0, 2}, {0}, {0}, {0}, {1}, {1}, {2}, {2}, {1, 2}, {1}, {2}, {1}, {2}, {1}, {1, 2}, {0, 1, 2}, {2}, {1}, {0}, {1}, {0}, {2}, {1, 2}, {1, 2}, {0, 1, 2}, {2}, {0, 2}, {1, 2}, {2}, {0, 1}, {1, 2}, {1}, {1}, {1, 2}, {0, 2}, {0}, {0, 1, 2}, {0, 2}, {0}, {1}, {0}, {0}, {0, 2}, {2}, {1, 2}, {2}, {0}, {0}, {2}, {2}, {1, 2}, {1}, {0, 1}]\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best valset aggregate score so far: 0.6021690115440116\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best program as per aggregate score on train_val: 2\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best program as per aggregate score on valset: 2\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best score on valset: 0.6021690115440116\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best score on train_val: 0.6021690115440116\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Linear pareto front program index: 2\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New program candidate index: 2\n",
      "GEPA Optimization:  14%|█▍        | 204/1483 [03:58<28:11,  1.32s/rollouts]2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 3: No merge candidates found\n",
      "2025/09/30 09:22:13 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 2 score: 0.6021690115440116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.16 / 3 (72.0%): 100%|██████████| 3/3 [00:07<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:22:20 INFO dspy.evaluate.evaluate: Average Metric: 2.159090909090909 / 3 (72.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:23:52 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for predict: markdown\n",
      "# Task: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will receive **one JSON object** with the following two top‑level keys:\n",
      "\n",
      "* **`pdfinfo`** – a dictionary containing the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* **`pages`** – a list of page objects, each having:\n",
      "  * `page` – the page number (integer)  \n",
      "  * `text` – the plain‑text extracted from that page (string, may contain markdown‑style headings, bold markup, URLs, etc.)\n",
      "\n",
      "Your job is to **produce a single JSON object** that contains **exactly** the fields listed in the table below (order does not matter).  \n",
      "All fields must be present; if a value cannot be determined, use `null` for scalar fields or an empty list `[]` for list fields.\n",
      "\n",
      "| Field | Type | How to obtain (rules) |\n",
      "|-------|------|-----------------------|\n",
      "| `language` | string (ISO‑639‑1) | Detect the primary language of the **body text** (the bulk of the pages). Count occurrences of language‑specific stop‑words (see the stop‑word list below). Choose the language with the highest count. Do **not** default to Finnish. |\n",
      "| `title` | string | The **main title** of the work. <br>1. Look at page 1. The first non‑empty line that is either (a) a Markdown heading (`# …`, `## …`, …) **or** (b) all‑caps / title‑case and is separated from surrounding text by blank lines is considered the title line. <br>2. If the line starts with a *series identifier* (e.g. `ePooki 34/2020`, `Report 12‑2021`, `BOFIT Policy Brief 2/2014`), **strip** that prefix, keeping only the part after the identifier. <br>3. Trim surrounding whitespace. |\n",
      "| `alt_title` | list of strings | Any alternative title(s) that appear elsewhere in the document. Typical sources: <br>• Subtitle after a colon (`Main Title: Subtitle`). <br>• A translation in another language, often on the next line or in brackets. <br>• A line that repeats the main title but with a different language. <br>Collect each distinct alternative title as a separate list element; keep the exact wording (do **not** truncate). |\n",
      "| `creator` | list of strings | All author names **in the order they appear** in the document (usually on page 1). <br>• Names may appear as `Firstname Lastname`, `Lastname, Firstname`, or as a Markdown link `**[Firstname Lastname](url)**`. <br>• Convert every name to the canonical form **`Lastname, Firstname`** (preserve all given‑name parts). <br>• Preserve diacritics and special characters. <br>• Remove duplicate entries while preserving the first occurrence. |\n",
      "| `year` | integer | Publication year. <br>1. Prefer the four‑digit year found in `pdfinfo.creationDate` (`D:YYYY…`). <br>2. If that is missing or ambiguous, look for a year on page 1 (e.g. `5.3.2021`, `2021`, `2014`). <br>Take the first year that looks like a plausible publication year (1900‑2099). |\n",
      "| `publisher` | list of strings | Institution or organisation that published the work. Typical cues: <br>• Lines containing words like `University`, `Institute`, `College`, `Korkeakoulu`, `Akatemia`, `Research Institute`, `BOFIT`, `Suomen Pankki`, etc. <br>• A line labelled “Metatiedot”, “Publisher”, or similar. <br>Collect each distinct publisher as a separate list element. |\n",
      "| `doi` | string or null | DOI if present. Search the whole text for the pattern `10.\\d{4,9}/[-._;()/:A-Z0-9]+` (case‑insensitive). Return the **first** match exactly as it appears (preserve case). If none, return `null`. |\n",
      "| `e_isbn` | list of strings | All **electronic** ISBNs. Look for patterns `ISBN\\s*[:=]?\\s*[\\d-]+` (case‑insensitive). If the surrounding text contains the word **e‑ISBN**, **online**, **electronic**, or the ISBN is listed together with “e‑ISBN”, place the number in `e_isbn`. Remove the leading “ISBN” label and any surrounding punctuation; keep hyphens as they appear. |\n",
      "| `p_isbn` | list of strings | All **print** ISBNs. Use the same pattern as for `e_isbn`. If the surrounding text contains **print**, **hardcover**, **paperback**, or explicitly says “Print ISBN”, place the number in `p_isbn`. If the document never distinguishes, put all found numbers in `e_isbn` and leave `p_isbn` empty. |\n",
      "| `e_issn` | string or null | Electronic ISSN. Search for `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[0-9Xx]`. If the surrounding text contains **e‑ISSN**, **online**, or **electronic**, treat it as electronic; otherwise treat it as print. Return the pure ISSN (e.g., `\"1798-2022\"`). If none, return `null`. |\n",
      "| `p_issn` | string or null | Print ISSN. Same detection as `e_issn` but with surrounding clues **print**, **hardcopy**, **paper**, etc. If not found, return `null`. |\n",
      "| `type_coar` | string | COAR‑compatible type. Determine by scanning the whole document for the following keywords (case‑insensitive). Use the **first matching category** in the order given: <br>1. **master thesis** – presence of any of `master’s thesis`, `master’s degree programme`, `opinnäytetyö` *and* a word like `master` or `maisteri`. <br>2. **bachelor thesis** – presence of `bachelor thesis`, `bachelor’s`, `kandidatavhandling`, `avhandling` *without* master‑level wording. <br>3. **journal article** – presence of a DOI, a journal name, volume/issue numbers, or the word `journal`. <br>4. **research report** – presence of the word `Report`, `research report`, `policy brief`, or similar *and* not matching the thesis categories. <br>5. **book** – presence of `book`, `monograph`, `ISBN` (and not a journal article). <br>6. If none of the above apply, set to `\"other\"`. |\n",
      "\n",
      "---\n",
      "\n",
      "## Supporting Details & Strategies\n",
      "\n",
      "### 1. Language detection\n",
      "Use the following stop‑word sets (feel free to extend). Count matches case‑insensitively across **all pages except the very first heading line** (the title itself may contain foreign words).\n",
      "\n",
      "*Finnish* (`fi`): `ja, on, että, tämä, se, ei, mutta, kuin, myös, jos, kun, niin, tästä, siitä, mitä, miten, jolloin, mukaan, mukaan, jälkeen, jälkeen, vuoksi, jälkeen, jälkeen, jälkeen, jälkeen`  \n",
      "*English* (`en`): `the, and, of, to, in, a, is, that, for, it, as, with, on, be, by, this, which, are, from, or, at, was, were, has, have, not, can, will, would`  \n",
      "*Swedish* (`sv`): `och, att, i, är, det, som, på, för, med, av, till, den, har, men, om, när, blir, så, från, kan, inte, är, har`  \n",
      "\n",
      "Pick the language with the highest count; if a tie occurs, prefer the language whose stop‑word set yields the highest **percentage** of matched tokens relative to total tokens.\n",
      "\n",
      "### 2. Title extraction details\n",
      "* Remove markdown markup (`#`, `##`, `**`, `*`) before processing.  \n",
      "* If the first heading line contains a colon, keep the whole line as the title; the part **after** the colon may also be added to `alt_title` if it appears elsewhere as a separate line.  \n",
      "* Series identifiers are usually of the form `Word Number/Year` (e.g., `ePooki 34/2020`). Strip everything up to and including the first space after the identifier.\n",
      "\n",
      "### 3. Author name handling\n",
      "1. Find the author line(s) on page 1 (often after the title, possibly prefixed with “by”, “Authors:”, or presented as Markdown links).  \n",
      "2. Split each name:\n",
      "   * If a comma is present → assume `Lastname, Firstname`.  \n",
      "   * Otherwise assume `Firstname Lastname`. Split on the last space to get the surname (handles middle names).  \n",
      "3. Re‑assemble as `Lastname, Firstname`. Preserve hyphens, apostrophes, and diacritics.  \n",
      "4. Example conversions:  \n",
      "   * `Pentikäinen Tytti` → `Pentikäinen, Tytti`  \n",
      "   * `Tytti Pentikäinen` → `Pentikäinen, Tytti`  \n",
      "   * `Männistö, Merja` → `Männistö, Merja`  \n",
      "\n",
      "### 4. Publisher detection\n",
      "Typical keywords (case‑insensitive): `university`, `universitet`, `korkeakoulu`, `akademi`, `institute`, `research institute`, `boﬁt`, `suomen pankki`, `college`, `faculty`, `department`, `school of`, `yliopisto`, `ammattikorkeakoulu`.  \n",
      "If a line contains more than one of these keywords, treat the whole line (trimmed) as the publisher entry.\n",
      "\n",
      "### 5. Regular expressions\n",
      "* **DOI**: `(?i)10\\.\\d{4,9}/[-._;()/:A-Z0-9]+`  \n",
      "* **ISBN**: `(?i)ISBN\\s*[:=]?\\s*([\\d-]+)` – capture group 1 is the number.  \n",
      "* **ISSN**: `(?i)ISSN\\s*[:=]?\\s*(\\d{4}-\\d{3}[0-9Xx])` – capture group 1 is the pure ISSN.\n",
      "\n",
      "When extracting ISBN/ISSN, strip surrounding punctuation and whitespace from the captured group.\n",
      "\n",
      "### 6. COAR type hierarchy\n",
      "Apply the categories **in order**; stop at the first match.  \n",
      "Examples of keyword groups:\n",
      "\n",
      "| Category | Keywords (case‑insensitive) |\n",
      "|----------|----------------------------|\n",
      "| master thesis | `master’s thesis`, `master’s degree programme`, `opinnäytetyö`, `maisteri` |\n",
      "| bachelor thesis | `bachelor thesis`, `bachelor’s`, `kandidatavhandling`, `avhandling` (but **not** `master`/`maisteri`) |\n",
      "| journal article | `doi:`, `journal`, `vol\\.`, `no\\.`, `issue` |\n",
      "| research report | `report`, `policy brief`, `research report`, `brief` |\n",
      "| book | `book`, `monograph`, presence of ISBN without journal clues |\n",
      "\n",
      "### 7. Missing data handling\n",
      "* If a regex finds no match → return `null` (for scalar) or `[]` (for list).  \n",
      "* If a numeric year cannot be parsed → `null`.  \n",
      "* If language detection yields no clear winner → `null`.\n",
      "\n",
      "### 8. Output format\n",
      "Return **only** the JSON object, no surrounding text, no markdown fences, no explanations. Example:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Syömishäiriötaustaisen äidin kohtaaminen lastenneuvolassa\",\n",
      "  \"alt_title\": [],\n",
      "  \"creator\": [\"Pentikäinen, Tytti\", \"Pihkakoski, Tanja\", \"Vänskä, Suvi\", \"Männistö, Merja\"],\n",
      "  \"year\": 2021,\n",
      "  \"publisher\": [\"Oulun ammattikorkeakoulu\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": \"1798-2022\",\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"journal article\"\n",
      "}\n",
      "2025/09/30 09:24:02 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/09/30 09:24:02 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score is not better, skipping\n",
      "GEPA Optimization:  14%|█▍        | 210/1483 [05:47<47:23,  2.23s/rollouts]2025/09/30 09:24:02 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 2 score: 0.6021690115440116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.82 / 3 (60.6%): 100%|██████████| 3/3 [00:10<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:24:13 INFO dspy.evaluate.evaluate: Average Metric: 1.8181818181818181 / 3 (60.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:25:21 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Proposed new text for predict: markdown\n",
      "# Instruction: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will be given a **single JSON object** with two top‑level keys:\n",
      "\n",
      "* **pdfinfo** – a dictionary containing the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* **pages** – an ordered list of pages. Each page is a dictionary with:\n",
      "  * `page` – the page number (integer, 1‑based)  \n",
      "  * `text` – the plain‑text extracted from that page (string, may contain line‑breaks, markdown‑style headings, etc.)\n",
      "\n",
      "Your job is to **produce ONE JSON object** containing **exactly** the fields listed in the table below (order does not matter).  \n",
      "All values must follow the indicated type. If a value cannot be determined, use `null` for scalar fields or an empty list `[]` for list fields.\n",
      "\n",
      "| Field | Type | Extraction rules (detailed) |\n",
      "|-------|------|----------------------------|\n",
      "| **language** | string (ISO‑639‑1) | Detect the primary language of the *body* of the document (i.e. the majority of pages). Count occurrences of a small stop‑word list for Finnish (`ja`, `on`, `että`, …), English (`the`, `and`, `of`, …), Swedish (`och`, `att`, `är`, …). Choose the language with the highest count. **Never default to Finnish.** |\n",
      "| **title** | string | 1. Look at **page 1**. The first non‑empty line that looks like a heading (all‑caps, title‑case, or preceded/followed by blank lines) is the candidate title. 2. If the line begins with a *series identifier* (e.g. “ePooki 34/2020”, “Sibelius‑Akatemian …”, “ACTA … 1075”), strip everything up to and including the first space after the identifier. 3. Remove surrounding whitespace. 4. Keep the whole line **including any colon**. 5. If the title appears again on later pages (exact match ignoring case and surrounding whitespace), keep the first occurrence. |\n",
      "| **alt_title** | list of strings | Search the whole document for lines that are **distinct** from the main title but appear to be a subtitle, translation or alternative wording. Typical cues: <br>• Text after a colon in the title line (e.g. “Main Title: Subtitle”). <br>• The same title in another language, often on the second page or in brackets. <br>Collect each unique alternative title (trim whitespace, keep original capitalization) as a separate list element. If none, return `[]`. |\n",
      "| **creator** | list of strings | Locate the author/author‑list on page 1 (often directly under the title). Split each name into *surname* and *given name(s)* and format as `\"Lastname, Firstname\"` (preserve any middle names). If a name is already in “Firstname Lastname” order, reverse it. Preserve the order of appearance, remove duplicates, and return the list. |\n",
      "| **year** | integer | 1. Prefer the four‑digit year extracted from `pdfinfo.creationDate` (pattern `D:YYYY…`). 2. If that is missing or ambiguous, look for a four‑digit year (1900‑2099) on the first three pages (often in the heading “2020”, “2023”, etc.). Return the year as an integer. |\n",
      "| **publisher** | list of strings | Scan the document (especially the title page and the “Metatiedot” / “Acknowledgements” sections) for institutional names such as universities, faculties, research institutes, or journal titles. Return each distinct name **exactly as it appears** (trim surrounding whitespace). Preserve order of first appearance. |\n",
      "| **doi** | string or null | Search the entire text for a DOI using the regex `10\\.\\d{4,9}/[-._;()/:A-Z0-9]+` (case‑insensitive). Return the first match **without surrounding whitespace**. If none, return `null`. |\n",
      "| **e_isbn** | list of strings | Find all ISBN strings with the pattern `ISBN\\s*[:=]?\\s*[\\d\\-]+`. For each match: <br>1. Determine if the surrounding text contains the word **“PDF”**, **“Online”**, **“e‑ISBN”**, or similar → treat as *electronic*. <br>2. If no such cue, still place the number in `e_isbn` (the electronic field is the default). <br>3. Strip the leading “ISBN” label, remove all hyphens and spaces, keep only the digits (e.g. `978-952-335-936-9` → `9789523359369`). <br>Return a list of unique cleaned numbers, preserving order of first appearance. |\n",
      "| **p_isbn** | list of strings | Same detection as `e_isbn` but only keep numbers whose surrounding text contains **“Print”**, **“hardcover”**, **“p‑ISBN”**, etc. Clean them in the same way (remove hyphens/spaces). If the document never distinguishes print vs. electronic, leave `p_isbn` empty (`[]`). |\n",
      "| **e_issn** | string or null | Search for ISSN with pattern `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[\\dxX]`. If the surrounding text mentions **“Online”**, **“Electronic”**, **“e‑ISSN”**, treat as electronic; otherwise default to electronic. Return the ISSN **without hyphen** (e.g. `1798-2022` → `17982022`). If none, return `null`. |\n",
      "| **p_issn** | string or null | Same as `e_issn` but only keep the number when the surrounding text contains **“Print”**, **“Print ISSN”**, etc. Return without hyphen. If none, return `null`. |\n",
      "| **type_coar** | string | Determine the COAR‑compatible type using the following hierarchy (first match wins): <br>• **doctoral thesis** – if the document contains phrases like “Doctor of Science (Technology)”, “Dissertation”, “Doctoral study”, “Doctor of Philosophy”, “PhD thesis”, etc. <br>• **master thesis** – if it contains “Master’s thesis”, “Master’s degree programme”, “opinnäytetyö” (and not doctoral wording). <br>• **bachelor thesis** – if it contains “bachelor thesis”, “Kandidatavhandling”, “avhandling” without master/doctoral qualifiers. <br>• **journal article** – if a DOI is present **or** the document mentions volume/issue numbers, journal name, or looks like a journal article. <br>• **research report** – if labelled “Report”, “Research report”, or similar, and none of the thesis or journal cues apply. <br>• **book** – if it is a monograph/edited volume (often has ISBN/ISSN but no thesis/journal cues). <br>• **other** – if none of the above apply. Return the exact lower‑case string (e.g. `\"doctoral thesis\"`). |\n",
      "| **alt_title** (already defined) | list of strings | – |\n",
      "| **creator** (already defined) | list of strings | – |\n",
      "\n",
      "## General Extraction Strategy (for reference)\n",
      "\n",
      "1. **Parse `pdfinfo.creationDate`** → extract year.  \n",
      "2. **Detect language** → count stop‑words across all pages (ignore very short pages).  \n",
      "3. **Title** → first heading‑style line on page 1, strip series identifiers, keep colon.  \n",
      "4. **Alternative titles** → look for subtitles after colon, translations on other pages, bracketed titles.  \n",
      "5. **Authors** → names on page 1, split/reorder to “Lastname, Firstname”.  \n",
      "6. **Publisher** → institutional names on title page or in metadata sections.  \n",
      "7. **Identifiers** → regex search for DOI, ISBN, ISSN; classify electronic vs print by surrounding keywords; clean numbers (remove hyphens/spaces).  \n",
      "8. **COAR type** → apply hierarchy of keyword checks (doctoral > master > bachelor > journal > report > book > other).  \n",
      "\n",
      "## Output Format\n",
      "\n",
      "Return **only** the JSON object, no extra text, no comments. Example:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Liikearvon alaskirjaukset tuloksenjärjestelykeinona : tarkastelussa Helsingin pörssiin listatut yritykset\",\n",
      "  \"alt_title\": [\"Age‑appropriate sexual education is a child's safety\"],\n",
      "  \"creator\": [\"Virtanen, Samuli\"],\n",
      "  \"year\": 2020,\n",
      "  \"publisher\": [\"Vaasan yliopisto\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": null,\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"master thesis\"\n",
      "}\n",
      "2025/09/30 09:25:26 INFO dspy.evaluate.evaluate: Average Metric: 1.909090909090909 / 3 (63.6%)\n",
      "2025/09/30 09:26:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:26:06 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/09/30 09:26:10 ERROR dspy.utils.parallelizer: Error for Example({'content': '{\"pdfinfo\": {\"title\": \"Annales E 80 K\\\\u00f6hler.pdf\", \"creationDate\": \"D:20211021113120Z\", \"modDate\": \"D:20211021151149+03\\'00\\'\"}, \"pages\": [{\"page\": 1, \"text\": \"## SIS\\\\u00c4ISESTI VAHVA\\\\nSis\\\\u00e4isten suhteiden dynamiikka\\\\nmarkkinaorientoituvassa yrityksess\\\\u00e4 \\\\u2013\\\\n\\\\nmarkkinoinnin, myynnin ja viestinn\\\\u00e4n\\\\n\\\\nvuorovaikutus\\\\nOuti K\\\\u00f6hler\\\\nTURUN YLIOPISTON JULKAISUJA \\\\u2013 ANNALES UNIVERSITATIS TURKUENSIS\\\\nSARJA \\\\u2013 SER. E OSA \\\\u2013 TOM.80 | OECONOMICA | TURKU 2021\\\\n\\\\n\\\\n\"}, {\"page\": 3, \"text\": \"# SIS\\\\u00c4ISESTI VAHVA\\\\n\\\\n##### Sis\\\\u00e4isten suhteiden dynamiikka markkinaorientoituvassa yrityksess\\\\u00e4 \\\\u2013 markkinoinnin, myynnin ja viestinn\\\\u00e4n vuorovaikutus Outi K\\\\u00f6hler\\\\n**TURUN YLIOPISTON JULKAISUJA \\\\u2013 ANNALES UNIVERSITATIS TURKUENSIS**\\\\n**SARJA \\\\u2013 SER. E OSA \\\\u2013 TOM. 80 | OECONOMICA | TURKU 2021**\\\\n\\\\n\\\\n\"}, {\"page\": 4, \"text\": \"Turun yliopisto\\\\nTurun kauppakorkeakoulu\\\\nMarkkinoinnin ja kansainv\\\\u00e4lisen liiketoiminnan laitos\\\\nMarkkinointi\\\\nTy\\\\u00f6n ohjaajat\\\\nProfessori Rami Olkkonen\\\\nTurun kauppakorkeakoulu\\\\nTurun yliopisto\\\\nTarkastajat\\\\nProfessori Arto Rajala\\\\nVaasan yliopisto\\\\nVastav\\\\u00e4itt\\\\u00e4j\\\\u00e4t\\\\nProfessori Arto Rajala\\\\nVaasan yliopisto\\\\nProfessori Elina Jaakkola\\\\nTurun kauppakorkeakoulu\\\\nTurun yliopisto\\\\nTy\\\\u00f6el\\\\u00e4m\\\\u00e4professori Pekka Mattila\\\\nAalto-yliopisto\\\\nTy\\\\u00f6el\\\\u00e4m\\\\u00e4professori Pekka Mattila\\\\nAalto-yliopisto\\\\nTurun yliopiston laatuj\\\\u00e4rjestelm\\\\u00e4n mukaisesti t\\\\u00e4m\\\\u00e4n julkaisun alkuper\\\\u00e4isyys on\\\\ntarkastettu Turnitin OriginalityCheck-j\\\\u00e4rjestelm\\\\u00e4ll\\\\u00e4.\\\\nSis\\\\u00e4kansien kuvitus: Lotta K\\\\u00f6hler\\\\nISBN 978-951-29-8615-6 (PRINT)\\\\nISBN 978-951-29-8616-3 (PDF)\\\\nISSN 2343-3159 (Painettu/Print)\\\\nISSN 2343-3167 (Verkkojulkaisu/Online)\\\\nPainosalama, Turku 2021\\\\n\\\\n\\\\n\"}, {\"page\": 5, \"text\": \"_Sapere aude!_\\\\n\\\\n\\\\n3\\\\n\\\\n\\\\n\"}, {\"page\": 6, \"text\": \"TURUN YLIOPISTO\\\\nTurun kauppakorkeakoulu\\\\nMarkkinoinnin ja kansainv\\\\u00e4lisen liiketoiminnan laitos\\\\nMarkkinointi\\\\nOUTI K\\\\u00d6HLER: Sis\\\\u00e4isesti vahva. Sis\\\\u00e4isten suhteiden dynamiikka markkinaorientoituvassa yrityksess\\\\u00e4: markkinoinnin, myynnin ja viestinn\\\\u00e4n vuorovaikutus\\\\nV\\\\u00e4it\\\\u00f6skirja, 292 s.\\\\nTurun kauppakorkeakoulun tohtoriohjelma\\\\nLokakuu 2021\\\\nTIIVISTELM\\\\u00c4\\\\nMarkkinaorientaation kehitt\\\\u00e4minen on yrityksen menestystekij\\\\u00e4 ja toimiva yhteisty\\\\u00f6\\\\nsen edellytys. Miksi siin\\\\u00e4 onnistutaan joskus paremmin, joskus huonommin? T\\\\u00e4m\\\\u00e4\\\\nlaadullinen tutkimus tuo lis\\\\u00e4ymm\\\\u00e4rryst\\\\u00e4 siit\\\\u00e4, miten vertaissuhteet rakentuvat, toimivat ja vaikuttavat yrityksen sis\\\\u00e4isess\\\\u00e4 yhteistoiminnassa markkinaorientaatiota\\\\nkehitett\\\\u00e4ess\\\\u00e4. Markkinaorientoitumisen mahdollistumisen osa-alueiksi on tarkastelussa rajattu markkinatiedon sis\\\\u00e4ll\\\\u00f6n jalostaminen, hy\\\\u00f6dynt\\\\u00e4minen ja jakaminen.\\\\nTutkimusasetelmassa on kolme merkitt\\\\u00e4v\\\\u00e4\\\\u00e4 l\\\\u00e4ht\\\\u00f6kohtaa. Ensinn\\\\u00e4kin, toimivan\\\\nyhteisty\\\\u00f6n vaateesta huolimatta sosiaalinen n\\\\u00e4k\\\\u00f6kulma on markkinaorientaation pitk\\\\u00e4ss\\\\u00e4 tutkimusperinteess\\\\u00e4 perin harvinainen. Toiseksi, markkinoinnin, myynnin ja\\\\nviestinn\\\\u00e4n keskin\\\\u00e4isen yhteisty\\\\u00f6n ja vertaissuhteiden tarkastelu triadina vuorovaikutustutkimuksessa on poikkeuksellista. Kolmanneksi, markkinaorientaatiotutkimus\\\\non painottunut johdon n\\\\u00e4k\\\\u00f6kulmaan ja tulkintoihin. T\\\\u00e4ss\\\\u00e4 tutkimuksessa keski\\\\u00f6ss\\\\u00e4\\\\non tiimitason omat havainnot, m\\\\u00e4\\\\u00e4ritykset ja kokemukset. Tutkimusta viitoittavat\\\\naihealueet ovat: Ketk\\\\u00e4 toimivat yhdess\\\\u00e4? Miten he toimivat yhdess\\\\u00e4? Miksi he toimivat juuri niin?\\\\nSosiaalisen n\\\\u00e4k\\\\u00f6kulman teoreettinen kehys kiteytettiin sosiaalisen p\\\\u00e4\\\\u00e4oman teoriasta ja sosiaalisesta verkostoteoriasta sovelletuilla k\\\\u00e4sitteill\\\\u00e4. Sen kolmeksi osaalueeksi t\\\\u00e4smentyiv\\\\u00e4t yhteisty\\\\u00f6kumppanit, yhteistoiminta sek\\\\u00e4 yhteisen toiminnan\\\\nlopputulema. T\\\\u00e4ss\\\\u00e4 yksitt\\\\u00e4istapaustutkimuksessa noudatettiin tulkitsevan tutkimuksen l\\\\u00e4hestymistapaa. Todellisuus ja tieto n\\\\u00e4htiin sosiaalisesti rakentuneena. Tietoa\\\\nsiit\\\\u00e4 saatiin mikrotasolla, toimijoiden omina kokemuksina ja havaintoina.\\\\nKattava ja monipuolinen aineisto ker\\\\u00e4ttiin kohdeyrityksess\\\\u00e4 markkinoinnin,\\\\nviestinn\\\\u00e4n ja myynnin toimijoilta sek\\\\u00e4 otoksena tuotekehitykselt\\\\u00e4 ja johdolta. Tiimitason aineisto kuvasi kokemuksia markkinaorientoitumisen kehittymisest\\\\u00e4 poikittaistarkasteluna. Niiden analyysiss\\\\u00e4 sosiaalista verkostoanalyysi\\\\u00e4 ja affektio-kognitio-toiminta-suhtautumismallia k\\\\u00e4ytettiin toisiaan t\\\\u00e4ydent\\\\u00e4vin\\\\u00e4 metodeina. Johdon\\\\nhaastattelut ja sekund\\\\u00e4\\\\u00e4riaineisto koottiin erilliseksi tapauskuvaukseksi markkinaorientaation kehittymisest\\\\u00e4 pitkitt\\\\u00e4istarkasteluna.\\\\nTutkimuksen tulokset osoittavat sosiaalisen n\\\\u00e4k\\\\u00f6kulman t\\\\u00e4rkeyden ja merkityksen markkinaorientaatioteoriassa ja -tutkimuksessa: 1) Yksil\\\\u00f6n kokemukset sosiaalisesta siiloutumisesta, jopa yksin\\\\u00e4isyydest\\\\u00e4 ty\\\\u00f6yhteis\\\\u00f6ss\\\\u00e4, vaikuttavat k\\\\u00e4ytt\\\\u00e4ytymiseen sosiaalisissa vaihdantatilanteissa. 2) Organisaatio- ja prosessiuudistuksista\\\\nmahdollisesti seuraavat muutokset yhteyksien m\\\\u00e4\\\\u00e4r\\\\u00e4ss\\\\u00e4 ja laadussa voivat muuntaa\\\\ntietopolkuja ei-toivotusti tai ne voivat jopa tyrehty\\\\u00e4 yksil\\\\u00f6iden oman tietoon liittyv\\\\u00e4n\\\\n\\\\n\\\\n4\\\\n\\\\n\\\\n\"}, {\"page\": 296, \"text\": \"ISBN 978-951-29-8615-6 (PRINT)\\\\nISBN 978-951-29-8616-3 (PDF)\\\\nISSN 2343-3159 (Painettu/Print)\\\\nISSN 2343-3167 (Verkkojulkaisu/Online)\\\\n\\\\n\\\\n\"}]}', 'metadata': '{\"language\": \"fi\", \"title\": \"Sis\\\\u00e4isesti vahva : sis\\\\u00e4isten suhteiden dynamiikka markkinaorientoituvassa yrityksess\\\\u00e4 - markkinoinnin, myynnin ja viestinn\\\\u00e4n vuorovaikutus\", \"creator\": [\"K\\\\u00f6hler, Outi\"], \"year\": \"2021\", \"publisher\": [\"Turun yliopisto\"], \"e_isbn\": [\"9789512986163\"], \"p_isbn\": [\"9789512986156\"], \"e_issn\": \"2343-3167\", \"p_issn\": \"2343-3159\", \"type_coar\": \"doctoral thesis\"}'}) (input_keys={'content'}): 1 validation error for nullable[str]\n",
      "  Input should be a valid string [type=string_type, input_value=2021, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "Traceback (most recent call last):\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/parallelizer.py\", line 55, in safe_func\n",
      "    return user_function(item)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/evaluate/evaluate.py\", line 158, in process_item\n",
      "    prediction = program(**example.inputs())\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/primitives/module.py\", line 78, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/predict/chain_of_thought.py\", line 37, in forward\n",
      "    return self.predict(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/predict/predict.py\", line 103, in __call__\n",
      "    return super().__call__(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/primitives/module.py\", line 78, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/predict/predict.py\", line 192, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 47, in __call__\n",
      "    return JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/json_adapter.py\", line 82, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 46, in __call__\n",
      "    raise e\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 38, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 128, in __call__\n",
      "    return self._call_postprocess(processed_signature, signature, outputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 89, in _call_postprocess\n",
      "    value = self.parse(processed_signature, text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/json_adapter.py\", line 169, in parse\n",
      "    fields[k] = parse_value(v, signature.output_fields[k].annotation)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/utils.py\", line 163, in parse_value\n",
      "    return TypeAdapter(annotation).validate_python(value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 421, in validate_python\n",
      "    return self.validator.validate_python(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for nullable[str]\n",
      "  Input should be a valid string [type=string_type, input_value=2021, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "\n",
      "2025/09/30 09:26:10 INFO dspy.evaluate.evaluate: Average Metric: 36.96969696969697 / 64 (57.8%)\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset score for new program: 0.5776515151515151\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full train_val score for new program: 0.5776515151515151\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Individual valset scores for new program: [0.6363636363636364, 0.45454545454545453, 0.6060606060606061, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 0.36363636363636365, 0.8181818181818182, 0.45454545454545453, 0.5909090909090909, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 0.8181818181818182, 0.5, 1.0, 0.45454545454545453, 0.5454545454545454, 0.9090909090909091, 0.36363636363636365, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.2727272727272727, 1.0, 0.5454545454545454, 0.7272727272727273, 0.45454545454545453, 0.8181818181818182, 0.36363636363636365, 0.36363636363636365, 0.8181818181818182, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.2727272727272727, 0.36363636363636365, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.45454545454545453, 0.5454545454545454, 0.5454545454545454, 0.2727272727272727, 0.6363636363636364, 0.45454545454545453, 0.0, 0.8181818181818182]\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New valset pareto front scores: [0.8181818181818182, 0.6363636363636364, 0.7878787878787878, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364, 0.4805194805194805, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.696969696969697, 0.6363636363636364, 0.9545454545454546, 0.7272727272727273, 1.0, 0.45454545454545453, 0.6272727272727273, 0.9090909090909091, 0.5151515151515151, 1.0, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 1.0, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.6590909090909091, 0.5714285714285714, 0.8181818181818182, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182]\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset pareto front score: 0.682132711038961\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Updated valset pareto front programs: [{2}, {0}, {0}, {0, 1, 3}, {0, 2, 3}, {0, 3}, {0, 1, 2}, {0}, {3}, {2}, {1}, {0, 2, 3}, {0, 3}, {0}, {0}, {1, 3}, {1}, {2}, {3}, {1, 2}, {1}, {2}, {1}, {2}, {1}, {3}, {0, 1, 2, 3}, {2}, {3}, {0}, {1}, {0, 3}, {2}, {1, 2, 3}, {1, 2, 3}, {0, 1, 2}, {2}, {3}, {1, 2}, {2, 3}, {0, 1}, {1, 2, 3}, {1}, {1}, {3}, {0, 2}, {0}, {0, 1, 2, 3}, {0, 2}, {0}, {1}, {0}, {0}, {0, 2, 3}, {2}, {1, 2, 3}, {2}, {0}, {0}, {2}, {2}, {1, 2, 3}, {1}, {3}]\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best valset aggregate score so far: 0.6021690115440116\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on train_val: 2\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on valset: 2\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on valset: 0.6021690115440116\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on train_val: 0.6021690115440116\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Linear pareto front program index: 2\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program candidate index: 3\n",
      "GEPA Optimization:  19%|█▉        | 280/1483 [07:54<40:50,  2.04s/rollouts]2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 5: No merge candidates found\n",
      "2025/09/30 09:26:10 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 2 score: 0.6021690115440116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.73 / 3 (57.6%): 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:26:15 INFO dspy.evaluate.evaluate: Average Metric: 1.727272727272727 / 3 (57.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:27:34 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for predict: markdown\n",
      "# Instruction: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will receive **one** JSON object that contains two top‑level keys:\n",
      "\n",
      "* **`pdfinfo`** – a dictionary with the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* **`pages`** – a list of pages, each page is an object with a `page` number and a `text` field that holds the plain‑text extracted from that page.\n",
      "\n",
      "Your job is to **produce a single JSON object** that contains **exactly** the fields listed below (order does not matter).  \n",
      "All values must be of the type shown in the examples (strings, numbers, lists, or `null`).  \n",
      "\n",
      "If a field cannot be determined, use `null` for a scalar field or an empty list `[]` for a list field.\n",
      "\n",
      "| Field | Type | Extraction rules (detailed) |\n",
      "|-------|------|-----------------------------|\n",
      "| `language` | string (ISO‑639‑1) | Detect the primary language of the **body text** (all pages except very short front‑matter). Count occurrences of language‑specific stop‑words for Finnish (`fi`), English (`en`), Swedish (`sv`) (you may add others, but the three are required). Choose the language with the highest count. **Never default to Finnish** – if no clear winner, return `null`. |\n",
      "| `title` | string | The **main title** of the work. <br>1. Look at page 1 text. The first non‑empty line that looks like a heading (all‑caps, title‑case, or surrounded by blank lines) is the candidate. <br>2. If the candidate starts with a series identifier such as `ePooki 34/2020`, `Sibelius‑Akatemian …`, strip that part. <br>3. Trim surrounding whitespace. <br>4. If `pdfinfo.title` exists **and** it matches the candidate (ignoring case and series identifiers), you may use it. <br>5. If the title contains a colon (`:`) keep the whole line as the title; the part after the colon may also become an element of `alt_title` (see below). |\n",
      "| `alt_title` | list of strings | Any **alternative title** that appears elsewhere in the document, typically a translation, subtitle, or a title in another language. <br>• Search all pages for lines that are exact or near‑exact translations of the main title (e.g. same words in another language, or the part after a colon). <br>• Include each distinct alternative title as a separate list element, preserving the order of first appearance. <br>• Do not duplicate the main title. |\n",
      "| `creator` | list of strings | All author names **in the order they appear** in the document. <br>• Names are usually on the first page, often after the words “Author”, “Authors”, “Författare”, “Författare:”, “Författare”, “Författare”, “Författare”, “Författare”, “Författare”. <br>• Each name must be formatted as `\"Lastname, Firstname\"` (surname first, a comma, a space, then given name(s)). <br>• If a name is already in “Firstname Lastname” order, split it correctly. <br>• Preserve middle names/initials as they appear. <br>• Remove duplicate entries while preserving the original order. |\n",
      "| `year` | integer | Publication year. <br>1. Prefer the four‑digit year after `D:` in `pdfinfo.creationDate` (e.g. `D:20210514141925…` → `2021`). <br>2. If not present, look for a four‑digit year on the first few pages (often near “Date of approval”, “Datum för godkännande”, “Approved on”, etc.). |\n",
      "| `publisher` | list of strings | Institution or organisation that published the work. <br>• Typical sources: lines containing “University of …”, “Institute of …”, “School of …”, “Högskolan …”, “Åland University of Applied Sciences”, “Korkeakoulu”, etc. <br>• Return each distinct publisher as a separate list element, preserving the order of first appearance. <br>• Do **not** invent a publisher; only use text that explicitly appears in the document. |\n",
      "| `doi` | string or null | DOI if present. Search the whole text for the pattern `10.\\d{4,9}/[-._;()/:A-Z0-9]+` (case‑insensitive). Return the first match exactly as it appears. If none, `null`. |\n",
      "| `e_isbn` | list of strings | All **electronic** ISBNs. Look for patterns `ISBN\\s*[:=]?\\s*[\\d\\- ]+`. <br>• If the surrounding text contains the word “e‑ISBN”, “online ISBN”, “electronic ISBN”, or “ISBN (online)”, treat the number as electronic. <br>• If the document does **not** distinguish, place the number in `e_isbn`. <br>• Return the ISBN **without** the “ISBN” label, stripped of all spaces and hyphens (e.g. `9789523950900`). |\n",
      "| `p_isbn` | list of strings | All **print** ISBNs. Same detection as `e_isbn` but require the surrounding text to contain “print ISBN”, “p‑ISBN”, “ISBN (print)”, etc. If the document never distinguishes, leave `p_isbn` empty (`[]`). |\n",
      "| `e_issn` | string or null | Electronic ISSN. Search for `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[\\dxX]`. If the surrounding text contains “e‑ISSN”, “online ISSN”, treat as electronic. Return the pure ISSN (e.g. `\"1798-2022\"`). If none, `null`. |\n",
      "| `p_issn` | string or null | Print ISSN. Same pattern, but require “print ISSN”, “p‑ISSN”, etc. If not distinguished, leave `null`. |\n",
      "| `type_coar` | string | COAR‑compatible type of the work. Determine from contextual clues (case‑insensitive). Use the **first** matching rule in the list below: <br>1. **Doctoral thesis** – contains “Doctoral thesis”, “PhD thesis”, “Dissertation”, “Licentiate”, “opinnäytetyö” together with “Doctoral” or “PhD”. <br>2. **Master thesis** – contains “Master’s thesis”, “Master’s degree programme”, “opinnäytetyö” (without doctoral wording), “master thesis”, “examenarbete” (Swedish) with “master”. <br>3. **Bachelor thesis** – contains “Bachelor’s thesis”, “bachelor thesis”, “Kandidatavhandling”, “examensarbete” (Swedish) without master/doctoral wording. <br>4. **Journal article** – appears in a journal (has volume/issue numbers, page ranges, a DOI, or a journal name/ISSN). <br>5. **Research report** – labelled “Report”, “Technical Report”, “Research Report”, or similar, and **not** a thesis. <br>6. **Book** – labelled “Book”, “Monograph”, has a publisher and ISBN(s) but no thesis/report wording. <br>7. **Newspaper article** – contains newspaper‑specific terms like “Tidning”, “Newspaper”, “Press”, often a date and a short length, and no DOI/ISSN. <br>8. **Other** – none of the above apply. |\n",
      "| `type_coar` values must be exactly one of: `\"doctoral thesis\"`, `\"master thesis\"`, `\"bachelor thesis\"`, `\"journal article\"`, `\"research report\"`, `\"book\"`, `\"newspaper article\"`, `\"other\"` |\n",
      "\n",
      "## General Extraction Strategy (for reference)\n",
      "\n",
      "1. **Parse `pdfinfo.creationDate`** – extract the first four digits after `D:` for the year.  \n",
      "2. **Language detection** – tokenise all body pages, count stop‑words for `fi`, `en`, `sv`. Choose the highest.  \n",
      "3. **Title** – scan page 1 line‑by‑line; the first line that looks like a heading (all caps, title case, or surrounded by blank lines) is the candidate. Strip any leading series identifiers. Trim. If a colon is present, keep whole line as title; the part after the colon may also become an `alt_title`.  \n",
      "4. **Alternative titles** – search all pages for lines that are exact translations or subtitles of the main title; collect distinct ones.  \n",
      "5. **Authors** – locate the author line(s) on page 1 (often after “Author”, “Authors”, “Författare”, etc.). Split each name into surname/given‑name, format as `\"Lastname, Firstname\"`, preserve order, remove duplicates.  \n",
      "6. **Publisher** – look for lines containing university, institute, school, or journal names. Return each distinct entity.  \n",
      "7. **Identifiers (DOI, ISBN, ISSN)** – run the regular expressions given above over the whole text. For ISBN/ISSN, examine a few words before/after the match to decide electronic vs. print. Normalise ISBNs by removing spaces and hyphens.  \n",
      "8. **COAR type** – apply the ordered list of keyword rules. Stop at the first rule that matches.  \n",
      "\n",
      "## Output format\n",
      "\n",
      "Return **only** the JSON object, nothing else. Example:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"en\",\n",
      "  \"title\": \"Accurate Battery Modelling for Control Design and Economic Analysis of Lithium‑ion Battery Energy Storage Systems in Smart Grid\",\n",
      "  \"alt_title\": [\"Age‑appropriate sexual education is a child's safety\"],\n",
      "  \"creator\": [\"Sikala, Irmeli\", \"Myllykangas, Kirsi\", \"Tölli, Sirpa\", \"Tuura, Jaana\"],\n",
      "  \"year\": 2020,\n",
      "  \"publisher\": [\"Oulun ammattikorkeakoulu\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": \"1798-2022\",\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"journal article\"\n",
      "}\n",
      "2025/09/30 09:27:44 INFO dspy.evaluate.evaluate: Average Metric: 1.7636363636363637 / 3 (58.8%)\n",
      "2025/09/30 09:28:23 INFO dspy.evaluate.evaluate: Average Metric: 34.904306220095684 / 64 (54.5%)\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset score for new program: 0.5453797846889952\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full train_val score for new program: 0.5453797846889952\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Individual valset scores for new program: [0.5454545454545454, 0.36363636363636365, 0.7272727272727273, 0.36363636363636365, 0.6363636363636364, 0.2727272727272727, 0.6363636363636364, 0.5818181818181819, 0.6181818181818182, 0.45454545454545453, 0.6363636363636364, 0.2727272727272727, 0.6363636363636364, 0.5454545454545454, 0.36363636363636365, 0.6363636363636364, 0.45454545454545453, 0.45454545454545453, 0.7636363636363637, 0.5454545454545454, 0.36363636363636365, 0.7272727272727273, 0.45454545454545453, 0.9090909090909091, 0.45454545454545453, 0.5454545454545454, 0.36363636363636365, 0.5406698564593302, 0.5454545454545454, 0.36363636363636365, 0.9090909090909091, 0.7272727272727273, 0.5454545454545454, 0.5454545454545454, 0.36363636363636365, 0.7272727272727273, 0.36363636363636365, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.2727272727272727, 0.3333333333333333, 0.45454545454545453, 0.7878787878787878, 0.45454545454545453, 0.45454545454545453, 0.3333333333333333, 0.45454545454545453, 0.5454545454545454, 0.45454545454545453, 0.8545454545454546, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454]\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New valset pareto front scores: [0.8181818181818182, 0.6363636363636364, 0.7878787878787878, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364, 0.4805194805194805, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.7636363636363637, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.9545454545454546, 0.7272727272727273, 1.0, 0.45454545454545453, 0.6272727272727273, 0.9090909090909091, 0.5151515151515151, 1.0, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.7878787878787878, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.6590909090909091, 0.5714285714285714, 0.8545454545454546, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182]\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset pareto front score: 0.6917918019480519\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Updated valset pareto front programs: [{2}, {0}, {0}, {0, 1, 3, 4}, {0, 2, 3}, {0, 3}, {0, 1, 2}, {0}, {3}, {2}, {1}, {0, 2, 3}, {0, 3}, {0}, {0}, {1, 3}, {1}, {2}, {4}, {1, 2}, {1}, {4}, {1}, {2}, {1}, {3}, {0, 1, 2, 3}, {2}, {3}, {0}, {1}, {0, 3}, {2}, {1, 2, 3, 4}, {1, 2, 3}, {4}, {2}, {3}, {1, 2}, {2, 3}, {4}, {1, 2, 3}, {1}, {1}, {3}, {0, 2}, {0}, {0, 1, 2, 3}, {0, 2}, {0}, {1}, {0}, {0}, {4}, {2}, {1, 2, 3, 4}, {2}, {0}, {0}, {2}, {4}, {4}, {1}, {3}]\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best valset aggregate score so far: 0.6021690115440116\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on train_val: 2\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on valset: 2\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on valset: 0.6021690115440116\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on train_val: 0.6021690115440116\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Linear pareto front program index: 2\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program candidate index: 4\n",
      "GEPA Optimization:  24%|██▎       | 350/1483 [10:08<37:29,  1.99s/rollouts]2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 6: No merge candidates found\n",
      "2025/09/30 09:28:23 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 4 score: 0.5453797846889952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.41 / 3 (47.0%): 100%|██████████| 3/3 [00:07<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:28:31 INFO dspy.evaluate.evaluate: Average Metric: 1.409090909090909 / 3 (47.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:30:14 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for predict: markdown\n",
      "# Task: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will be given **one** JSON object that contains two top‑level keys:\n",
      "\n",
      "* **`pdfinfo`** – a dictionary with the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* **`pages`** – a list of page objects, each with:\n",
      "  * `page` – the page number (integer)  \n",
      "  * `text` – the plain‑text extracted from that page (string, may contain line‑breaks)\n",
      "\n",
      "Your job is to **produce a single JSON object** that contains **exactly** the fields listed in the table below (order does not matter).  \n",
      "All values must be of the type shown in the examples (strings, numbers, lists, or `null`).  \n",
      "\n",
      "If a field cannot be determined, use `null` for a scalar field or an empty list `[]` for a list field.\n",
      "\n",
      "---\n",
      "\n",
      "## Output Fields & Extraction Rules\n",
      "\n",
      "| Field | Type | Extraction details (must be followed exactly) |\n",
      "|-------|------|----------------------------------------------|\n",
      "| **`language`** | string (ISO‑639‑1) | Detect the primary language of the **body text** (all pages **except** very short front‑matter such as cover pages, title pages, or pages with ≤ 30 characters of non‑whitespace). <br>1. Count occurrences of language‑specific stop‑words for Finnish (`fi`), English (`en`) and Swedish (`sv`). You may add other languages, but the three are required. <br>2. Choose the language with the highest count **only if the count is at least twice the second‑highest count**. <br>3. If there is no clear winner, or the text is too short, return `null`. **Never default to Finnish**. |\n",
      "| **`title`** | string | 1. Scan **page 1** line‑by‑line (preserve line order). <br>2. The first non‑empty line that **looks like a heading** is the candidate. A heading is a line that: <br>   • Is in ALL‑CAPS **or** Title‑Case (first letter of most words capitalised) **or** is surrounded by blank lines (i.e. a line with text, a blank line before, and a blank line after). <br>3. If the candidate starts with a **series identifier** (e.g. `ePooki 34/2020`, `Sibelius‑Akatemian …`, `Acta …`, `Report 12/2021`, etc.) strip that leading part up to the first space or punctuation. <br>4. Trim surrounding whitespace. <br>5. If `pdfinfo.title` exists **and** it matches the candidate after case‑insensitive comparison **and** after removing any series identifier, you may use `pdfinfo.title` instead (but only if it is identical after the cleaning step). <br>6. Keep the **entire line** as the title, even if it contains a colon (`:`). The part after the colon may also be used for `alt_title` (see below). |\n",
      "| **`alt_title`** | list of strings | Collect any **alternative titles** that appear elsewhere in the document. <br>• If the main title contains a colon, the text after the colon is a candidate subtitle – add it as the first element (unless it is identical to the main title). <br>• Search **all pages** for lines that are **exact** or **near‑exact translations** of the main title (e.g. the same words in another language, or the same subtitle in another language). <br>• Also include any line that appears to be a **translation** of the whole title (you may rely on obvious lexical similarity; do not invent titles). <br>• Preserve the order of first appearance and ensure no duplicate of the main title is added. |\n",
      "| **`creator`** | list of strings | Identify **all author names** in the order they appear. <br>1. Look primarily on **page 1** (and page 2 if needed) for lines that follow keywords such as `Author`, `Authors`, `Författare`, `Tekijä`, `Tekijät`, `Kirjoittaja`, `Kirjoittajat`, etc. <br>2. Names may be listed separated by commas, semicolons, line‑breaks, or the word “and”. <br>3. For each name, convert to the format `\"Lastname, Firstname\"` (surname first, a comma, a space, then given name(s)). <br>   • If the name is already in “Firstname Lastname” order, split on the last space to obtain the surname. <br>   • Preserve middle names/initials as they appear. <br>4. Remove duplicate entries **while preserving the original order**. <br>5. **Do not** include editor names, acknowledgements, supervisors, reviewers, or any other non‑author persons. |\n",
      "| **`year`** | integer | 1. Prefer the four‑digit year after `D:` in `pdfinfo.creationDate` (e.g. `D:20210514141925…` → `2021`). <br>2. If `pdfinfo.creationDate` is missing or does not contain a year, search the first **five** pages for a four‑digit year that appears near phrases like “Date of approval”, “Approved on”, “Approved”, “Date”, “Julkaistu”, “Julkaisuvuosi”, “Publication year”, etc. <br>3. If no year can be found, return `null`. |\n",
      "| **`publisher`** | list of strings | Find **institutional or organisational** names that clearly indicate the publisher. <br>• Look for lines containing any of the following keywords (case‑insensitive): `University`, `Universität`, `Universidad`, `Institute`, `Institute of`, `School of`, `College`, `Högskolan`, `Korkeakoulu`, `Ammattikorkeakoulu`, `Polytechnic`, `Yliopisto`, `Universitetet`, `Universitet`, `Universitetet`, `Universities`, `Institute of Technology`, etc. <br>• Return each **distinct** publisher as a separate list element, preserving the order of first appearance. <br>• Do **not** invent a publisher; only use text that explicitly appears. Exclude acknowledgements, funding statements, or any unrelated organization names. |\n",
      "| **`doi`** | string or null | Search the **entire concatenated text** for the pattern: <br>`10\\.\\d{4,9}/[-._;()/:A-Z0-9]+` (case‑insensitive). Return the **first** match exactly as it appears (preserve case). If none, return `null`. |\n",
      "| **`e_isbn`** | list of strings | Detect all ISBNs that are **electronic**. <br>1. Look for patterns `ISBN\\s*[:=]?\\s*[\\d\\-\\s]+` (case‑insensitive). <br>2. For each match, examine the **5 words before and after** the match for any of the following cues (case‑insensitive): `e‑ISBN`, `electronic ISBN`, `online ISBN`, `ISBN (online)`, `ISBN (e‑book)`, `ISBN (e‑)`. <br>3. If such a cue is present, treat the number as electronic; otherwise, if the document never distinguishes between print and electronic, **place the number in `e_isbn`**. <br>4. Normalise the ISBN: remove all spaces and hyphens, keep only digits (and possible trailing `X`). Example: `ISBN 978-952-395-030-6` → `\"9789523950306\"`. <br>5. Return a list of **unique** normalized ISBNs in order of first appearance. |\n",
      "| **`p_isbn`** | list of strings | Detect all ISBNs that are **print**. <br>Same detection as `e_isbn` but require the surrounding cue to contain any of: `print ISBN`, `p‑ISBN`, `ISBN (print)`, `hardcover ISBN`, etc. <br>If the document never distinguishes, leave `p_isbn` empty (`[]`). Normalise as for `e_isbn`. |\n",
      "| **`e_issn`** | string or null | Detect an electronic ISSN. <br>1. Pattern: `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[\\dxX]` (case‑insensitive). <br>2. Examine the 5 words before/after the match for cues: `e‑ISSN`, `online ISSN`, `ISSN (online)`. <br>3. If such a cue exists, return the pure ISSN (e.g. `\"1798-2022\"`). If the document does not distinguish, **do not** place the ISSN here. <br>4. If no electronic ISSN is found, return `null`. |\n",
      "| **`p_issn`** | string or null | Detect a print ISSN. Same pattern, but require cues: `print ISSN`, `p‑ISSN`, `ISSN (print)`. If the document never distinguishes, return `null`. |\n",
      "| **`type_coar`** | string (one of eight exact values) | Determine the COAR‑compatible type by applying the following **ordered** rules; stop at the **first** rule that matches (case‑insensitive). <br>1. **Doctoral thesis** – the document contains any of the words `Doctoral thesis`, `PhD thesis`, `Dissertation`, `Licentiate`, `opinnäytetyö` **and** also contains `Doctoral` or `PhD` (i.e., not just “opinnäytetyö” alone). <br>2. **Master thesis** – contains any of `Master’s thesis`, `Master’s degree programme`, `opinnäytetyö` (without the doctoral wording), `master thesis`, `examenarbete`, `master` etc., **and does not satisfy rule 1**. <br>3. **Bachelor thesis** – contains any of `Bachelor’s thesis`, `bachelor thesis`, `Kandidatavhandling`, `examensarbete` (Swedish) **and does not satisfy rules 1‑2**. <br>4. **Journal article** – the text shows evidence of being a journal article: presence of a DOI, a journal name, volume/issue numbers, page ranges (e.g., “12(3): 45‑58”), or an ISSN. <br>5. **Research report** – contains words like `Report`, `Technical Report`, `Research Report`, `Study Report`, `Raportti`, `Tutkimusraportti` **and** does **not** satisfy any thesis rule. <br>6. **Book** – contains the word `Book`, `Monograph`, has a publisher and at least one ISBN, **and** does not satisfy any previous rule. <br>7. **Newspaper article** – contains terms such as `Tidning`, `Newspaper`, `Press`, often a date and a short length, **and** lacks DOI/ISSN. <br>8. **Other** – none of the above apply. <br>Return the matching value **exactly** as one of: `\"doctoral thesis\"`, `\"master thesis\"`, `\"bachelor thesis\"`, `\"journal article\"`, `\"research report\"`, `\"book\"`, `\"newspaper article\"`, `\"other\"` |\n",
      "| **`type_coar`** values must be exactly one of the eight strings listed above. |\n",
      "\n",
      "---\n",
      "\n",
      "## General Extraction Strategy (for reference)\n",
      "\n",
      "1. **Parse `pdfinfo.creationDate`** – extract the first four digits after `D:` for the year.  \n",
      "2. **Language detection** – tokenise the body pages (exclude front‑matter ≤ 30 non‑whitespace characters). Count stop‑words for `fi`, `en`, `sv`. Choose the language only if its count is at least twice the second‑highest; otherwise `null`.  \n",
      "3. **Title** – scan page 1 line‑by‑line; the first line that looks like a heading (all caps, title case, or surrounded by blank lines) is the candidate. Strip any leading series identifiers. Trim. Keep the whole line (including any colon). If a colon exists, the part after it may become an `alt_title`.  \n",
      "4. **Alternative titles** – search all pages for lines that are exact translations or subtitles of the main title; collect distinct ones, preserving first‑appearance order.  \n",
      "5. **Authors** – locate author line(s) on page 1 (often after “Author”, “Authors”, “Författare”, etc.). Split each name into surname/given‑name, format as `\"Lastname, Firstname\"`, preserve order, remove duplicates.  \n",
      "6. **Publisher** – look for lines containing university, institute, school, or other publishing entity keywords. Return each distinct entity, preserving order.  \n",
      "7. **Identifiers (DOI, ISBN, ISSN)** – run the regular expressions given above over the whole concatenated text. For ISBN/ISSN, examine the surrounding 5 words to decide electronic vs. print. Normalise ISBNs by removing spaces and hyphens.  \n",
      "8. **COAR type** – apply the ordered list of keyword rules. Stop at the first rule that matches.  \n",
      "\n",
      "---\n",
      "\n",
      "## Output Format\n",
      "\n",
      "Return **only** the JSON object, nothing else. Example:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"en\",\n",
      "  \"title\": \"Accurate Battery Modelling for Control Design and Economic Analysis of Lithium‑ion Battery Energy Storage Systems in Smart Grid\",\n",
      "  \"alt_title\": [\"Age‑appropriate sexual education is a child's safety\"],\n",
      "  \"creator\": [\"Sikala, Irmeli\", \"Myllykangas, Kirsi\", \"Tölli, Sirpa\", \"Tuura, Jaana\"],\n",
      "  \"year\": 2020,\n",
      "  \"publisher\": [\"Oulun ammattikorkeakoulu\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": \"1798-2022\",\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"journal article\"\n",
      "}\n",
      "2025/09/30 09:30:21 INFO dspy.evaluate.evaluate: Average Metric: 1.1818181818181817 / 3 (39.4%)\n",
      "2025/09/30 09:30:21 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New subsample score is not better, skipping\n",
      "GEPA Optimization:  24%|██▍       | 356/1483 [12:06<52:53,  2.82s/rollouts]2025/09/30 09:30:21 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 2 score: 0.6021690115440116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.62 / 3 (54.1%): 100%|██████████| 3/3 [00:05<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:30:27 INFO dspy.evaluate.evaluate: Average Metric: 1.6225895316804408 / 3 (54.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:31:48 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for predict: markdown\n",
      "# Task: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will receive a **single JSON object** with two top‑level keys:\n",
      "\n",
      "* `pdfinfo` – a dictionary containing the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* `pages` – a list of page objects, each with:\n",
      "  * `page` – the page number (integer)  \n",
      "  * `text` – the plain‑text extracted from that page (string, may contain line‑breaks)\n",
      "\n",
      "Your job is to **produce ONE JSON object** that contains **exactly** the fields listed below (order does not matter).  \n",
      "All values must conform to the indicated type; if a value cannot be determined, use `null` for a scalar field or an empty list `[]` for a list field.\n",
      "\n",
      "---\n",
      "\n",
      "## Output Schema\n",
      "\n",
      "| Field | Type | Extraction Rules |\n",
      "|-------|------|------------------|\n",
      "| `language` | **string** (ISO‑639‑1) | Detect the primary language of the *main body* (the bulk of the pages). Count occurrences of language‑specific stop‑words for **Finnish (`fi`)**, **English (`en`)**, **Swedish (`sv`)**, and any other language you may encounter. Return the code with the highest count. Do **not** default to Finnish. |\n",
      "| `title` | **string** | The **main title** of the work. Prefer the first prominent heading on page 1 (all‑caps, title‑case, or a line surrounded by blank lines). If `pdfinfo.title` matches that heading, use it; otherwise use the heading. **Strip any leading series identifiers** (e.g. “ePooki 34/2020”, “Sibelius‑Akatemian …”). Trim whitespace. |\n",
      "| `alt_title` | **list of strings** | Any alternative title found in the document: <br>• Sub‑titles after a colon (`:`) that appear on a separate line. <br>• Direct translations of the main title in another language (often appears on a later page or in brackets). <br>• Distinct subtitle lines that are not part of the main title. Include each distinct alternative title once. |\n",
      "| `creator` | **list of strings** | All author names **in the order they appear** in the document (usually on page 1). Each name must be formatted as `\"Lastname, Firstname\"` (surname first, a comma, a space, then given name(s)). If a name is given as “Firstname Lastname”, treat the **last word** as the surname and everything before it as given name(s). Preserve hyphens, diacritics and internal spaces in both parts. Remove duplicate entries while keeping the first occurrence. |\n",
      "| `year` | **integer** | Extract the publication year: <br>1. Parse `pdfinfo.creationDate` (format `D:YYYY…`) – the first four digits after `D:`. <br>2. If that fails, look for a four‑digit year on the first few pages (usually near the publisher line). |\n",
      "| `publisher` | **list of strings** | Institution(s) that published the work. Look for lines containing keywords such as **“University”, “Institute”, “College”, “School”, “University of …”, “Yrkeshögskolan”, “Tiltilaskeskus”, “Arcada”, “Painosalama”, “Publishing”, “Press”**, etc. Return each distinct publisher once, in the order found. Do **not** include author affiliation lines unless they are explicitly marked as the publisher. |\n",
      "| `doi` | **string** or **null** | DOI if present anywhere in the text. Use the regex `10\\.\\d{4,9}/[-._;()/:A-Z0-9]+` (case‑insensitive). Return the matched string **exactly** as it appears. |\n",
      "| `e_isbn` | **list of strings** | All **electronic** ISBNs. Search for patterns `ISBN\\s*[:=]?\\s*[\\d\\-\\s]+`. If the surrounding text contains the word **“PDF”, “online”, “e‑ISBN”, “electronic”**, treat the number as electronic. Strip the leading “ISBN” label, remove all spaces and hyphens, leaving only digits (e.g. `9789522445421`). If the document never distinguishes print vs. electronic, place the numbers in `e_isbn`. |\n",
      "| `p_isbn` | **list of strings** | All **print** ISBNs. Same pattern as above, but only keep numbers that are explicitly marked with **“PRINT”, “paper”, “hardcover”, “p‑ISBN”**, etc. If no such marker exists, leave this list empty. |\n",
      "| `e_issn` | **string** or **null** | Electronic ISSN if present. Pattern `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[\\dxX]`. If the surrounding text contains **“online”, “electronic”, “e‑ISSN”**, treat it as electronic. Return the ISSN with the hyphen (e.g. `\"2343-3167\"`). |\n",
      "| `p_issn` | **string** or **null** | Print ISSN if present. Same pattern, but only keep numbers that are explicitly marked **“PRINT”, “paper”, “p‑ISSN”**, etc. |\n",
      "| `type_coar` | **string** | COAR‑compatible type of the work. Determine from contextual clues (see table below). If none match, return `\"other\"`. |\n",
      "\n",
      "### COAR Type Determination Table\n",
      "\n",
      "| Keyword(s) in document | Resulting `type_coar` |\n",
      "|------------------------|-----------------------|\n",
      "| “Doctoral dissertation”, “Doctoral thesis”, “PhD”, “Dissertation”, “Doctoral dissertation” | `doctoral thesis` |\n",
      "| “Master’s thesis”, “Master’s degree programme”, “Master thesis”, “Master thesis” | `master thesis` |\n",
      "| “Bachelor thesis”, “Kandidatavhandling”, “avhandling” **without** master‑level wording | `bachelor thesis` |\n",
      "| “Journal article”, presence of DOI **and** a journal name/volume/issue, or a line like “Vol. X, No. Y” | `journal article` |\n",
      "| “Report”, “Research report”, “Opinnäytetyö” used **outside** a degree context | `research report` |\n",
      "| “Book”, “Monograph”, “Volume”, “Series”, “ISBN” without thesis/report wording | `book` |\n",
      "| (none of the above) | `other` |\n",
      "\n",
      "---\n",
      "\n",
      "## General Extraction Strategy (for reference)\n",
      "\n",
      "1. **Year** – first try `pdfinfo.creationDate`; fallback to scanning the first three pages for a 4‑digit year.\n",
      "2. **Language** – count stop‑words from a small list for each supported language across all pages; pick the highest.\n",
      "3. **Title** – examine page 1: the first line that looks like a heading (all‑caps, title‑case, or surrounded by blank lines). Remove any leading series identifier. Trim.\n",
      "4. **Alternative titles** – search for lines that are:\n",
      "   * After a colon on the same line as the main title (subtitle).  \n",
      "   * Appear on later pages and are a literal translation (same number of words, similar punctuation).  \n",
      "   * Enclosed in brackets or parentheses.\n",
      "5. **Creators** – locate the author line(s) on page 1 (often after the title). Split each name, re‑order to “Lastname, Firstname”. Preserve order, drop duplicates.\n",
      "6. **Publisher** – look for a line containing typical publisher keywords (see table). Prefer the line that also contains a location and year.\n",
      "7. **Identifiers (DOI/ISBN/ISSN)** – run the regexes globally on all page texts. For ISBN/ISSN, examine a few words before/after the match to decide electronic vs. print.\n",
      "8. **COAR type** – scan the whole document for the keywords in the table, respecting the priority order (doctoral > master > bachelor > journal > report > book).\n",
      "\n",
      "---\n",
      "\n",
      "## Output Formatting Rules\n",
      "\n",
      "* **JSON must be valid** and contain **only** the keys listed in the schema.\n",
      "* Strings must be UTF‑8, without surrounding whitespace.\n",
      "* Lists must contain **unique** items; order should reflect the order of first appearance in the source text.\n",
      "* Use `null` (not the string `\"null\"`) for missing scalar values.\n",
      "* Use an empty list `[]` for missing list values.\n",
      "* Do **not** add any additional commentary, explanations, or formatting outside the JSON object.\n",
      "\n",
      "---\n",
      "\n",
      "## Example (illustrative only)\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Ajassa kiinni ja irrallaan – Yhteisölliset rytmit 2000‑luvun Suomessa\",\n",
      "  \"alt_title\": [\"Trapped in Time and Loose – Community Rhythms of the 2000s in Finland\"],\n",
      "  \"creator\": [\"Anttila, Anu‑Hanna\", \"Anttila, Timo\", \"Liikkanen, Mirja\", \"Pääkkönen, Hannu\"],\n",
      "  \"year\": 2015,\n",
      "  \"publisher\": [\"Statistics Finland\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [\"9789522445421\"],\n",
      "  \"p_isbn\": [\"9789522445414\"],\n",
      "  \"e_issn\": null,\n",
      "  \"p_issn\": \"1798-2022\",\n",
      "  \"type_coar\": \"book\"\n",
      "}\n",
      "2025/09/30 09:31:57 INFO dspy.evaluate.evaluate: Average Metric: 1.809090909090909 / 3 (60.3%)\n",
      "2025/09/30 09:32:35 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:32:39 INFO dspy.evaluate.evaluate: Average Metric: 23.773881673881675 / 64 (37.1%)\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset score for new program: 0.3714669011544011\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full train_val score for new program: 0.3714669011544011\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Individual valset scores for new program: [0.2727272727272727, 0.2727272727272727, 0.18181818181818182, 0.36363636363636365, 0.45454545454545453, 0.2727272727272727, 0.2727272727272727, 0.5454545454545454, 0.5353535353535354, 0.36363636363636365, 0.5454545454545454, 0.36363636363636365, 0.30303030303030304, 0.2727272727272727, 0.4805194805194805, 0.36363636363636365, 0.2727272727272727, 0.42424242424242425, 0.3090909090909091, 0.36363636363636365, 0.36363636363636365, 0.18181818181818182, 0.4, 0.18181818181818182, 0.3181818181818182, 0.36363636363636365, 0.45454545454545453, 0.2424242424242424, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.2727272727272727, 0.36363636363636365, 0.2424242424242424, 0.45454545454545453, 0.2727272727272727, 0.36363636363636365, 0.2727272727272727, 0.5454545454545454, 0.2727272727272727, 0.09090909090909091, 0.45454545454545453, 0.2727272727272727, 0.5454545454545454, 0.36363636363636365, 0.6363636363636364, 0.21818181818181817, 0.3090909090909091, 0.2727272727272727, 0.18181818181818182, 0.45454545454545453, 0.2727272727272727, 0.36363636363636365, 0.45454545454545453, 0.5714285714285714, 0.2727272727272727, 0.3896103896103896, 0.36363636363636365, 0.45454545454545453, 0.5151515151515151, 0.42424242424242425]\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New valset pareto front scores: [0.8181818181818182, 0.6363636363636364, 0.7878787878787878, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364, 0.4805194805194805, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.7636363636363637, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.9545454545454546, 0.7272727272727273, 1.0, 0.45454545454545453, 0.6272727272727273, 0.9090909090909091, 0.5151515151515151, 1.0, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.7878787878787878, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.6590909090909091, 0.5714285714285714, 0.8545454545454546, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182]\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset pareto front score: 0.6917918019480519\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Updated valset pareto front programs: [{2}, {0}, {0}, {0, 1, 3, 4, 5}, {0, 2, 3}, {0, 3}, {0, 1, 2}, {0}, {3}, {2}, {1}, {0, 2, 3}, {0, 3}, {0}, {0, 5}, {1, 3}, {1}, {2}, {4}, {1, 2}, {1}, {4}, {1}, {2}, {1}, {3}, {0, 1, 2, 3, 5}, {2}, {3}, {0}, {1}, {0, 3}, {2}, {1, 2, 3, 4, 5}, {1, 2, 3}, {4}, {2}, {3}, {1, 2}, {2, 3}, {4}, {1, 2, 3}, {1}, {1}, {3}, {0, 2}, {0}, {0, 1, 2, 3}, {0, 2}, {0}, {1}, {0}, {0}, {4}, {2}, {1, 2, 3, 4}, {2}, {0}, {0}, {2}, {4}, {4}, {1}, {3}]\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best valset aggregate score so far: 0.6021690115440116\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on train_val: 2\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on valset: 2\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on valset: 0.6021690115440116\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on train_val: 0.6021690115440116\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Linear pareto front program index: 2\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program candidate index: 5\n",
      "GEPA Optimization:  29%|██▊       | 426/1483 [14:24<43:02,  2.44s/rollouts]2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 8: No merge candidates found\n",
      "2025/09/30 09:32:39 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Selected program 1 score: 0.5743371212121212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.36 / 3 (45.5%): 100%|██████████| 3/3 [00:06<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:32:45 INFO dspy.evaluate.evaluate: Average Metric: 1.3636363636363638 / 3 (45.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:34:38 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Proposed new text for predict: markdown\n",
      "# 📋 Task – Structured Bibliographic Metadata Extraction  \n",
      "\n",
      "You will receive **one JSON object** that represents a PDF document.  \n",
      "The object has two top‑level keys:\n",
      "\n",
      "| Key      | Description |\n",
      "|----------|-------------|\n",
      "| `pdfinfo`| Metadata that was extracted directly from the PDF file (e.g. `title`, `author`, `creationDate`, `modDate`). |\n",
      "| `pages`  | A list of page objects. Each page object contains `page` (the page number) and `text` (the OCR‑extracted plain‑text of that page). |\n",
      "\n",
      "Your job is to **produce ONE JSON object** that follows the schema below.  \n",
      "If a field cannot be determined, use the exact empty value indicated (e.g. `null`, `[]`).  \n",
      "All string values must be plain ASCII – normalise quotes to `\"` or `’`, collapse multiple spaces to a single space, and trim leading/trailing whitespace.\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema\n",
      "\n",
      "| Field | Type | Required format / rules |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | ISO‑639‑1 code. Scan the first 200 characters of the whole document. If any of **ä ö Ä Ö å Å** appear, set to `\"fi\"`; otherwise `\"en\"`. |\n",
      "| `title` | string | **Main title** of the work. <br>1. Prefer `pdfinfo.title` if it exists (clean it as described). <br>2. If not, construct the title from the first page: take the first non‑empty line ≥ 6 characters that is **not** a heading marker (`##`, `###`, etc.) and that does **not** look like an author line (see *Creator extraction*). <br>3. If the title spans several consecutive lines (e.g. a line ends with a colon or the next line is still capitalised and not an author), concatenate them with a single space. <br>4. Keep the subtitle **as part of the title** (do **not** split on the colon). |\n",
      "| `alt_title` | list of strings | Any **alternative** titles (e.g. a translation, a subtitle given in another language, a title inside quotation marks). Return each as a separate cleaned string. Do **not** duplicate the main title. |\n",
      "| `creator` | list of strings | Authors in **“Surname, Given‑Name”** order. <br>1. If `pdfinfo.author` exists, split on commas, semicolons or the word “and”. <br>2. If a name appears as “First Last” (two words, each starting with a capital letter), reorder to “Last, First”. <br>3. If `pdfinfo.author` is missing, look on page 1 for lines that contain personal names (patterns: `First Last`, `Last, First`, or a line that starts with a capitalised name list). Include **all** names found. |\n",
      "| `year` | integer or null | Publication year. <br>1. Prefer the four‑digit year from `pdfinfo.creationDate` or `pdfinfo.modDate` (the first four digits). <br>2. If both are missing, search the whole text for the **first** four‑digit number between 1900‑2099 that appears near publication information (e.g. “2021”, “May 2021”). If none, return `null`. |\n",
      "| `publisher` | list of strings | Institution responsible for the work. <br>• **Theses / dissertations** → the awarding university or faculty (e.g. “Åbo Akademi University”). <br>• **Research reports** → the organisation that produced the report (e.g. “Asumispalvelusäätiö ASPA”). <br>• **Journal articles** → `[]`. <br>Detect university/faculty names by keywords: “University”, “Yliopisto”, “Universität”, “Akademi”, “Institute”, “College”, “School”, “Faculty”. Return each distinct institution once. |\n",
      "| `doi` | string or null | DOI if present. Detect patterns like `10.\\d{4,9}/\\S+` (case‑insensitive). Strip any leading URL (`http://`, `https://`, `doi.org/`). Remove trailing punctuation. Return the bare DOI (e.g. `10.1000/xyz123`). |\n",
      "| `e_isbn` | list of strings | **Electronic** ISBN‑13 numbers. <br>Detect ISBN‑13 (13 digits, hyphens optional). After a match, look at up to 30 characters before/after for any of the cues **electronic**: `digital`, `electronic`, `e‑ISBN`, `(digital)`, `pdf`, `PDF`, `sid.` (if used for electronic), `online`. If a cue is found, add the ISBN (hyphens and spaces removed) to `e_isbn`. |\n",
      "| `p_isbn` | list of strings | **Print** ISBN‑13 numbers. Use the same detection as `e_isbn` but require one of the **print** cues: `print`, `paper`, `hardcover`, `(print)`, `Painettu`, `Print`, `sid.` (if used for print), `PDF` **not** present, `PDF` present → electronic, etc. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Use the same cue logic as for ISBN. Return hyphen‑free string. |\n",
      "| `p_issn` | string or null | Print ISSN (8 digits, optional hyphen). Use the same cue logic as for ISBN. Return hyphen‑free string. |\n",
      "| `type_coar` | string | COAR‑compatible resource type. Scan the whole document (case‑insensitive) and apply the **first** matching rule in this order: <br>1. **doctoral thesis** – contains any of: “doctoral thesis”, “dissertation”, “PhD”, “doctoral”, “väitöskirja”, “väitöskirjan”, “väitöskirja”. <br>2. **master thesis** – contains any of: “master’s thesis”, “master thesis”, “maisteri”, “maisterintutkielma”. <br>3. **journal article** – contains typical journal citation elements (journal name, volume, issue, pages) or the word “article”. <br>4. **conference proceeding** – contains “conference”, “proceedings”, “paper presented at”. <br>5. **research report** – contains “report”, “raportti”, “tutkimusraportti”, “research report”. <br>6. **research** – fallback for any other research‑type document. Return the exact lower‑case string (e.g. `doctoral thesis`). |\n",
      "\n",
      "---\n",
      "\n",
      "## Extraction Procedure (Step‑by‑Step)\n",
      "\n",
      "1. **Parse the input JSON** safely. Ignore any keys that are not listed above.  \n",
      "2. **Normalise dates**: `creationDate` / `modDate` are strings like `D:20201216144002+02'00'`. Extract the first four digits as the year.  \n",
      "3. **Detect language** (rule in the schema).  \n",
      "4. **Title extraction**  \n",
      "   * If `pdfinfo.title` exists → clean and use it.  \n",
      "   * Otherwise, read `pages[0].text` (page 1). Split into lines.  \n",
      "   * Scan lines from the top: skip empty lines and lines that start with markdown heading markers (`#`, `##`, `###`, etc.).  \n",
      "   * The first line that is ≥ 6 characters and does **not** look like an author line becomes the start of the title.  \n",
      "   * If this line ends with a colon **or** the next line is still capitalised and not an author, concatenate the next line(s) (single space between) until a line that is clearly an author, a copyright line (`©`), or a blank line is reached.  \n",
      "   * Clean the resulting string (trim, collapse spaces, normalise quotes). This is the `title`.  \n",
      "5. **Alternative titles**  \n",
      "   * Look for titles in quotation marks (`“ ”`, `\" \"`), or a subtitle given in another language (e.g., after a colon on a separate line). Add each cleaned string to `alt_title`. Do not add the main title again.  \n",
      "6. **Creator extraction**  \n",
      "   * If `pdfinfo.author` exists → split on commas, semicolons, the word “and”, or line breaks. Clean each name.  \n",
      "   * For each name, if it matches `First Last` (two words, each capitalised) → reorder to `Last, First`. If it already matches `Last, First` keep as‑is.  \n",
      "   * If `pdfinfo.author` is missing, scan the first 5 non‑empty lines of page 1 for name patterns (see above) and collect all matches.  \n",
      "7. **Year** – apply the rule in the schema. If a year is found in the text but is clearly part of a citation (e.g., “(2020)”) and not a publication year, ignore it; prefer the first plausible year after the title block.  \n",
      "8. **Publisher detection**  \n",
      "   * If the document is a thesis/dissertation (see `type_coar`), locate the first university/faculty name in the text (keywords list) and return it as a single‑element list.  \n",
      "   * If the document is a research report, locate the organisation name that appears near the title or in the header/footer (e.g., “Asumispalvelusäätiö ASPA”).  \n",
      "   * For journal articles → `[]`.  \n",
      "9. **ISBN / ISSN extraction**  \n",
      "   * Use the regular expressions: <br>`ISBN‑13`: `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b` <br>`ISSN`: `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`  \n",
      "   * For each match, extract up to 30 characters before and after the match. If any **electronic cue** (see schema) appears → add to the electronic list; if any **print cue** appears → add to the print list. If both cues appear, give priority to the **electronic** list.  \n",
      "   * Strip all hyphens and spaces before storing. Keep each ISBN/ISSN only once in the appropriate list.  \n",
      "10. **DOI extraction** – apply the regex `10\\.\\d{4,9}/\\S+`. Remove any surrounding URL parts and trailing punctuation (`.,;`) before storing. If none, set to `null`.  \n",
      "11. **Resource type (`type_coar`)** – apply the ordered rule list in the schema. The first matching category determines the value.  \n",
      "12. **Assemble the output** JSON. Preserve the key order shown in the schema for readability (order is not technically required, but it helps testing). Use `null` for missing string values and `[]` for empty lists.\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning & Normalisation Details\n",
      "\n",
      "* **Whitespace** – collapse any sequence of whitespace characters (spaces, tabs, newlines) to a single space. Trim leading/trailing spaces.  \n",
      "* **Quotes** – replace any fancy quotation marks (`“ ” ‘ ’ „ “ …`) with plain ASCII `\"` or `’`.  \n",
      "* **Hyphens in identifiers** – remove all hyphens (`-`) and spaces from ISBN/ISSN before storing.  \n",
      "* **Case** – identifiers (ISBN, ISSN, DOI) are stored in lower‑case only for the DOI; ISBN/ISSN are numeric only, so case does not matter.  \n",
      "\n",
      "---\n",
      "\n",
      "## Example (illustrative)\n",
      "\n",
      "Given the sample JSON from the prompt, the correct output would be:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Elämää henkimaailman, uskonnon ja yhteiskunnan rajoilla: Etnografinen tutkimus kanavoinnista, tietämisestä ja uushenkisestä yrittäjyydestä Suomessa\",\n",
      "  \"alt_title\": [],\n",
      "  \"creator\": [\"Hulkkonen, Katriina\"],\n",
      "  \"year\": 2021,\n",
      "  \"publisher\": [\"Turun yliopisto\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [\"9789512983735\"],\n",
      "  \"p_isbn\": [\"9789512983728\"],\n",
      "  \"e_issn\": \"23433191\",\n",
      "  \"p_issn\": \"00826987\",\n",
      "  \"type_coar\": \"doctoral thesis\"\n",
      "}\n",
      "```\n",
      "\n",
      "(Values are shown for illustration; your implementation must follow the exact rules above.)\n",
      "\n",
      "---\n",
      "\n",
      "**Remember:**  \n",
      "- Follow the **order of precedence** for each field (e.g., `pdfinfo.title` overrides any title you might infer).  \n",
      "- Use the **cues** surrounding ISBN/ISSN to decide electronic vs. print.  \n",
      "- Detect the **resource type** in the exact order listed; the first match wins.  \n",
      "- Return `null` for missing scalar values and `[]` for missing list values.\n",
      "\n",
      "Good luck! 🎯\n",
      "2025/09/30 09:34:45 INFO dspy.evaluate.evaluate: Average Metric: 1.4545454545454546 / 3 (48.5%)\n",
      "2025/09/30 09:35:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:35:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:35:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:35:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:35:21 INFO dspy.evaluate.evaluate: Average Metric: 28.58559622195987 / 64 (44.7%)\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Full valset score for new program: 0.44664994096812277\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Full train_val score for new program: 0.44664994096812277\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Individual valset scores for new program: [0.18181818181818182, 0.36363636363636365, 0.36363636363636365, 0.36363636363636365, 0.6363636363636364, 0.36363636363636365, 0.2727272727272727, 0.45454545454545453, 0.6363636363636364, 0.6103896103896104, 0.6363636363636364, 0.5454545454545454, 0.2727272727272727, 0.18181818181818182, 0.6363636363636364, 0.45454545454545453, 0.09090909090909091, 0.6363636363636364, 0.18181818181818182, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.6363636363636364, 0.5909090909090909, 0.3181818181818182, 0.45454545454545453, 0.45454545454545453, 0.36363636363636365, 0.7272727272727273, 0.09090909090909091, 0.6363636363636364, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.36363636363636365, 0.7272727272727273, 0.2727272727272727, 0.5454545454545454, 0.18181818181818182, 0.7272727272727273, 0.6363636363636364, 0.2727272727272727, 0.5454545454545454, 0.2727272727272727, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.5454545454545454, 0.5151515151515151, 0.18181818181818182, 0.2727272727272727, 0.36363636363636365, 0.18181818181818182, 0.2727272727272727, 0.5454545454545454, 0.45454545454545453, 0.3691460055096419, 0.7272727272727273, 0.18181818181818182, 0.7272727272727273, 0.7272727272727273, 0.36363636363636365]\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New valset pareto front scores: [0.8181818181818182, 0.6363636363636364, 0.7878787878787878, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.7636363636363637, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.9545454545454546, 0.7272727272727273, 1.0, 0.45454545454545453, 0.6272727272727273, 0.9090909090909091, 0.5151515151515151, 1.0, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.7878787878787878, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.6590909090909091, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Full valset pareto front score: 0.6995028409090909\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Updated valset pareto front programs: [{2}, {0}, {0}, {0, 1, 3, 4, 5, 6}, {0, 2, 3}, {0, 3}, {0, 1, 2}, {0}, {3}, {2}, {1}, {6}, {0, 3}, {0}, {6}, {1, 3}, {1}, {2, 6}, {4}, {1, 2}, {1}, {4}, {1, 6}, {2}, {1}, {3}, {0, 1, 2, 3, 5, 6}, {2}, {3}, {0}, {1}, {0, 3}, {2}, {1, 2, 3, 4, 5, 6}, {1, 2, 3}, {4}, {2}, {3}, {1, 2}, {2, 3}, {4}, {1, 2, 3}, {1}, {1}, {3}, {0, 2}, {0}, {0, 1, 2, 3}, {0, 2}, {0}, {1}, {0}, {0}, {4}, {2}, {1, 2, 3, 4}, {2, 6}, {0}, {0}, {6}, {4}, {4, 6}, {6}, {3}]\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best valset aggregate score so far: 0.6021690115440116\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best program as per aggregate score on train_val: 2\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best program as per aggregate score on valset: 2\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best score on valset: 0.6021690115440116\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best score on train_val: 0.6021690115440116\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Linear pareto front program index: 2\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New program candidate index: 6\n",
      "GEPA Optimization:  33%|███▎      | 496/1483 [17:05<39:21,  2.39s/rollouts]2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 9: No merge candidates found\n",
      "2025/09/30 09:35:21 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Selected program 1 score: 0.5743371212121212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.82 / 3 (60.6%): 100%|██████████| 3/3 [00:05<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:35:26 INFO dspy.evaluate.evaluate: Average Metric: 1.8181818181818181 / 3 (60.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:37:23 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Proposed new text for predict: markdown\n",
      "# Bibliographic‑Metadata Extraction – Detailed Assistant Instructions\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF file.  \n",
      "Its top‑level keys are:\n",
      "\n",
      "* `pdfinfo` – metadata extracted from the PDF (may contain `title`, `author`,\n",
      "  `creationDate`, `modDate`, …).  \n",
      "* `pages` – an ordered list of page objects, each with:\n",
      "  * `page` – page number (integer)  \n",
      "  * `text` – the OCR‑extracted plain‑text of that page (UTF‑8 string)\n",
      "\n",
      "Your job is to produce **exactly one JSON object** that follows the schema below.\n",
      "If a value cannot be determined, use the exact empty value specified\n",
      "(`null` for a missing string, `[]` for an empty list).\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema (order matters for readability)\n",
      "\n",
      "| Field | Type | Required format / notes |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | `\"fi\"` if the document is Finnish, otherwise `\"en\"`. Detect by scanning **only the first 200 characters of the concatenated page texts** (preserve the order of pages). If any of the characters `ä ö Ä Ö å Å` appear **or** a Finnish‑specific word such as “opinnäytetyö”, “ammattikorkeakoulu”, “tutkielma”, set to `fi`; otherwise `en`. |\n",
      "| `title` | string | Main title of the work, **cleaned** (see *Cleaning Rules*). Preference order: <br>1. `pdfinfo.title` if it exists and is non‑empty. <br>2. The first “prominent heading” on **page 1** (see *Title‑Finding Rules*). |\n",
      "| `alt_title` | list of strings | Any alternative titles. If the selected `title` contains a colon (`:`), treat the part **after** the colon as an alternative title (still cleaned). Also treat any text that appears inside quotation marks (`\"` or `’`) as an alternative title. Return each alternative as a separate element; if none, return `[]`. |\n",
      "| `creator` | list of strings | Author(s) in **“Surname, Given‑Name”** order. Use `pdfinfo.author` if present; otherwise locate an author line in the text (see *Author‑Finding Rules*). Split multiple authors on commas, semicolons, the word “and”, ampersand `&`, or line breaks. For each name that appears as “First Last” (or “First Middle Last”), reorder to “Last, First Middle”. Preserve diacritics. Return an empty list if no author can be identified. |\n",
      "| `year` | integer | Publication year. Extract the first four‑digit year (1900‑2099) from `pdfinfo.creationDate` or `pdfinfo.modDate`. If both are missing, search the whole text for the first such year. |\n",
      "| `publisher` | list of strings | Institution or commercial publisher. **Only** include a publisher when the resource type is a thesis/dissertation (see *type_coar*). In that case, return the awarding university/faculty (e.g. “Åbo Akademi University”, “Laurea University of Applied Sciences”, “Oulun ammattikorkeakoulu”). For journal articles, conference papers, or research reports, return `[]`. |\n",
      "| `doi` | string or null | DOI string **without** any URL prefix. Detect with case‑insensitive regex `10\\.\\d{4,9}/\\S+` and strip trailing punctuation (`.,;`). Return `null` if none found. |\n",
      "| `e_isbn` | list of strings | Electronic ISBN‑13 numbers. Find all 13‑digit ISBNs (with optional hyphens/spaces). An ISBN belongs to this list **only if** up to 20 characters before or after the match contain one of the cues: `digital`, `electronic`, `e‑ISBN`, `(digital)`. Store the number **without** hyphens or spaces. |\n",
      "| `p_isbn` | list of strings | Print ISBN‑13 numbers. Same detection as `e_isbn` but the surrounding cue must be one of: `print`, `paper`, `hardcover`, `(print)`. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Cue words as for `e_isbn`. Return hyphen‑free string or `null`. |\n",
      "| `p_issn` | string or null | Print ISSN. Cue words as for `p_isbn`. Return hyphen‑free string or `null`. |\n",
      "| `type_coar` | string | COAR‑compatible resource type, **lower‑case**. Detect in this order (first match wins): <br>1. **doctoral thesis** – contains any of `doctoral thesis`, `dissertation`, `PhD`, `doctoral`, `väitöskirja` <br>2. **master thesis** – contains any of `master’s thesis`, `master thesis`, `maisteri`, `maisterintutkielma` <br>3. **bachelor thesis** – contains any of `bachelor thesis`, `bachelor’s thesis`, `bachelor`, `opinnäytetyö` (Finnish) <br>4. **journal article** – contains a journal‑style citation (journal name, volume, issue, pages) or the word “article” together with typical citation fields. <br>5. **conference proceeding** – contains `conference`, `proceedings`, `paper presented at` <br>6. **research** – fallback for any other research report or article. |\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning Rules (apply to every string you output)\n",
      "\n",
      "1. **Trim** leading and trailing whitespace.  \n",
      "2. **Collapse** any sequence of whitespace characters (space, tab, newline) to a single space.  \n",
      "3. **Normalize quotation marks**: replace curly double quotes (`“”`) and single quotes (`‘’`) with plain `\"` and `'` respectively; then replace any remaining single quotes that are used as apostrophes with `’`.  \n",
      "4. **Remove Markdown formatting**: strip leading `#`, `##`, `###`, any surrounding asterisks `*` or underscores `_`, and surrounding double‑asterisks `**`.  \n",
      "5. **Remove trailing asterisks** that sometimes mark footnote symbols (e.g., `Magnusson*`).  \n",
      "6. **Leave diacritics** (ä, ö, å, etc.) untouched.\n",
      "\n",
      "---\n",
      "\n",
      "## Title‑Finding Rules (used when `pdfinfo.title` is absent)\n",
      "\n",
      "1. Scan **page 1** line‑by‑line (preserve original order).  \n",
      "2. Ignore empty lines and lines shorter than 6 characters after trimming.  \n",
      "3. Skip lines that start with markdown heading markers (`#`, `##`, `###`).  \n",
      "4. The **first remaining line** that looks like a title is selected.  \n",
      "5. Apply the *Cleaning Rules* to this line.  \n",
      "6. If the cleaned line contains a colon (`:`), keep the whole string as `title` **and** store the part after the colon (trimmed) as an element of `alt_title`.\n",
      "\n",
      "---\n",
      "\n",
      "## Author‑Finding Rules (used when `pdfinfo.author` is absent)\n",
      "\n",
      "1. Look at the first three pages; collect any line that:\n",
      "   * Starts with “By ”, “Author:”, or contains the word “and”/“&” between two capitalised names, **or**\n",
      "   * Is a line consisting mainly of capitalised words (possible author list) and appears near the top of the document (within the first 25 % of the page’s lines).\n",
      "2. From the candidate line, split on commas, semicolons, the word “and”, ampersand `&`, or line breaks.  \n",
      "3. For each token, remove trailing asterisks or footnote symbols.  \n",
      "4. If a token matches the pattern “First Last” (or “First Middle Last”), reorder to “Last, First Middle”.  \n",
      "5. Return the list of reordered names; if none can be parsed, return `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "## Publisher Determination (after `type_coar`)\n",
      "\n",
      "*If* `type_coar` is **master thesis**, **doctoral thesis**, or **bachelor thesis**:\n",
      "\n",
      "1. Search the whole text for university/faculty names. Typical keywords: `University`, `Universität`, `Akademi`, `Institute`, `College`, `School`, `Faculty`, `ammattikorkeakoulu`, `yliopisto`, `universitet`, `université`.  \n",
      "2. Return the **first distinct** institution name found (after applying *Cleaning Rules*).  \n",
      "3. If multiple distinct institutions appear (e.g., awarding university + department), return them all in the order found.\n",
      "\n",
      "*Otherwise* (`journal article`, `conference proceeding`, `research`): return `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "## DOI Extraction Details\n",
      "\n",
      "* Regex: `(?i)10\\.\\d{4,9}/\\S+`  \n",
      "* After a match, strip any trailing punctuation characters `.,;` and any surrounding whitespace.  \n",
      "* If the DOI appears as a full URL (`https://doi.org/…`), strip the URL part and keep only the DOI.\n",
      "\n",
      "---\n",
      "\n",
      "## ISBN / ISSN Extraction Details\n",
      "\n",
      "* **ISBN‑13 regex** (13 digits, optional hyphens/spaces):  \n",
      "  `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`  \n",
      "* **ISSN regex** (8 digits, optional hyphen):  \n",
      "  `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`  \n",
      "\n",
      "For each match:\n",
      "\n",
      "1. Capture up to 20 characters before and after the match.  \n",
      "2. Convert the surrounding snippet to lower‑case and look for the appropriate cue words (digital/e‑ISBN or print/pp‑ISBN).  \n",
      "3. If the cue matches the *electronic* list, add the hyphen‑free number to `e_isbn` / `e_issn`; if it matches the *print* list, add to `p_isbn` / `p_issn`.  \n",
      "4. If both cues appear, add the number to **both** lists.  \n",
      "5. Ensure each list contains **unique** entries (no duplicates).\n",
      "\n",
      "---\n",
      "\n",
      "## General Processing Flow (to be followed for every request)\n",
      "\n",
      "1. **Parse** the input JSON safely.  \n",
      "2. **Normalize dates**: extract the first four digits from `creationDate` / `modDate` (e.g., `\"D:20220626201846+03'00'\"` → `2022`).  \n",
      "3. **Detect language** using the rule in the *Language* section.  \n",
      "4. **Extract title** (prefer `pdfinfo.title`, otherwise use Title‑Finding Rules).  \n",
      "5. **Derive alt_title** from the selected title.  \n",
      "6. **Extract creators** (prefer `pdfinfo.author`, otherwise use Author‑Finding Rules).  \n",
      "7. **Determine year** from dates or text.  \n",
      "8. **Detect DOI**, ISBN‑13, ISSN as described.  \n",
      "9. **Identify resource type** (`type_coar`) using the ordered keyword list (including the newly added *bachelor thesis* rule).  \n",
      "10. **Find publisher** based on the determined `type_coar`.  \n",
      "11. **Assemble** the output JSON with the exact field order shown in the schema, using `null` or `[]` where appropriate.  \n",
      "12. **Return** the JSON object **as the only output** (no extra commentary).\n",
      "\n",
      "---\n",
      "\n",
      "### Important Pitfalls to Avoid (learned from previous feedback)\n",
      "\n",
      "* **Language** – scan only the first 200 characters; do **not** base the decision on the whole document because later pages may contain Finnish words unrelated to the main language.\n",
      "* **Title** – do not keep Markdown heading markers (`##`, `**`). Strip them before cleaning.\n",
      "* **Creator** – split on “and”, “&”, commas, semicolons; reorder each name correctly. Do not leave the original order if it is “First Last”.\n",
      "* **Publisher** – only include it for theses/dissertations. For journal articles the list must be empty.\n",
      "* **DOI** – return only the DOI string (`10.xxxx/...`), never the full URL.\n",
      "* **type_coar** – include the *bachelor thesis* detection rule; otherwise many bachelor works were mis‑labelled as “journal article”.\n",
      "* **alt_title** – remember to extract the part after a colon **and** any quoted alternative titles.\n",
      "* **ISBN/ISSN cue detection** – look at up to 20 characters before/after the match; do not rely on the word “ISBN” alone.\n",
      "\n",
      "Follow these instructions precisely to produce correct, reproducible metadata for any PDF represented in the given JSON format.\n",
      "2025/09/30 09:37:30 INFO dspy.evaluate.evaluate: Average Metric: 2.090909090909091 / 3 (69.7%)\n",
      "2025/09/30 09:38:14 INFO dspy.evaluate.evaluate: Average Metric: 39.11111111111109 / 64 (61.1%)\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New program is on the linear pareto front\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Full valset score for new program: 0.6111111111111112\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Full train_val score for new program: 0.6111111111111112\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Individual valset scores for new program: [0.7272727272727273, 0.7272727272727273, 0.6060606060606061, 0.36363636363636365, 0.7272727272727273, 0.36363636363636365, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.5454545454545454, 0.8181818181818182, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.8181818181818182, 0.9090909090909091, 0.8181818181818182, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.2727272727272727, 0.8181818181818182, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6565656565656566, 0.5454545454545454, 0.45454545454545453, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.45454545454545453, 0.45454545454545453, 0.5454545454545454, 0.6666666666666666, 0.6363636363636364, 0.6363636363636364, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364]\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.7636363636363637, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.9545454545454546, 0.7272727272727273, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.5151515151515151, 1.0, 0.9090909090909091, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6565656565656566, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.7878787878787878, 0.8181818181818182, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Full valset pareto front score: 0.7070391414141414\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Updated valset pareto front programs: [{2}, {7}, {0}, {0, 1, 3, 4, 5, 6, 7}, {0, 2, 3, 7}, {0, 3}, {0, 1, 2}, {0}, {3}, {2}, {1, 7}, {6}, {0, 3}, {0}, {6}, {1, 3}, {1, 7}, {2, 6, 7}, {4}, {1, 2}, {7}, {4}, {1, 6}, {2}, {1, 7}, {3}, {7}, {2}, {3}, {0}, {1}, {7}, {2}, {1, 2, 3, 4, 5, 6, 7}, {1, 2, 3}, {4}, {2}, {3}, {1, 2}, {2, 3}, {4, 7}, {1, 2, 3}, {1, 7}, {1, 7}, {3}, {0, 2}, {0, 7}, {0, 1, 2, 3, 7}, {0, 2}, {7}, {1, 7}, {0, 7}, {0}, {4}, {7}, {1, 2, 3, 4, 7}, {2, 6}, {0}, {7}, {6}, {4}, {4, 6}, {6, 7}, {3}]\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best valset aggregate score so far: 0.6111111111111112\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best program as per aggregate score on train_val: 7\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best program as per aggregate score on valset: 7\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best score on valset: 0.6111111111111112\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best score on train_val: 0.6111111111111112\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Linear pareto front program index: 7\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New program candidate index: 7\n",
      "GEPA Optimization:  38%|███▊      | 566/1483 [19:59<37:01,  2.42s/rollouts]2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 10: No merge candidates found\n",
      "2025/09/30 09:38:14 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Selected program 0 score: 0.5962628517316018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:07<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:38:22 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:39:40 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Proposed new text for predict: **Task Overview**\n",
      "You will receive a JSON object that contains two parts:\n",
      "1. `pdfinfo` – metadata extracted from the PDF file (e.g., creationDate, author, title, etc.).\n",
      "2. `pages` – an ordered list of pages, each with a `\"page\"` number and the plain‑text `\"text\"` that appears on that page.\n",
      "\n",
      "From this information you must produce **structured bibliographic metadata** in a fixed, line‑by‑line format (see the “Output Format” section).  \n",
      "All fields must follow the exact conventions described below; any deviation will be marked as incorrect.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Output Format\n",
      "Your answer must contain **exactly** the following sections, in this order, each on its own line (no extra whitespace, no markdown formatting, no additional sections):\n",
      "\n",
      "```\n",
      "reasoning\n",
      "language\n",
      "title\n",
      "alt_title\n",
      "creator\n",
      "year\n",
      "publisher\n",
      "doi\n",
      "e_isbn\n",
      "p_isbn\n",
      "e_issn\n",
      "p_issn\n",
      "type_coar\n",
      "```\n",
      "\n",
      "* `reasoning` – a short (1‑2 sentences) description of how you derived the metadata.  \n",
      "* All other fields must contain the final value **only** (no labels, no quotes unless the value itself contains them).\n",
      "\n",
      "If a field has no value:\n",
      "* For scalar fields (`language`, `title`, `year`, `doi`, `e_issn`, `p_issn`) output `None`.\n",
      "* For list fields (`alt_title`, `creator`, `publisher`, `e_isbn`, `p_isbn`) output an empty list `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Field‑by‑Field Specification\n",
      "\n",
      "| Field | Type | Exact format & rules |\n",
      "|-------|------|----------------------|\n",
      "| **language** | scalar | ISO 639‑1 two‑letter code, lower‑case (e.g., `en`, `fi`, `sv`). Do **not** include region subtags (`en‑US`). Determine the language from the dominant language of the document text. |\n",
      "| **title** | scalar | The **full main title** as it appears in the document, preserving case, diacritics, and punctuation. Include any subtitle separated by a colon (`:`) **exactly as printed**. Do **not** truncate or re‑phrase. |\n",
      "| **alt_title** | list of strings | All alternative titles that are explicitly given (e.g., an English translation of a Finnish title, a subtitle, or a title appearing in an abstract). Preserve the exact wording. If none are found, output `[]`. |\n",
      "| **creator** | list of strings | Names of the **author(s)** of the work **only** (do **not** include supervisors, advisors, reviewers, editors, or other contributors unless they are the author). Format each name as `\"LastName, FirstName\"` (surname first, a comma, a space, then given name(s)). Preserve diacritics. Order the list as the authors appear in the document. |\n",
      "| **year** | scalar (integer) | Four‑digit year of publication. Prefer the year found in `pdfinfo[\"creationDate\"]` (the first 4 digits after `D:`). If that is missing or clearly a creation timestamp rather than publication year, fall back to a year explicitly mentioned in the document (e.g., “May 2021”, “Spring 2020”). |\n",
      "| **publisher** | list of strings | The publishing entity (e.g., university, institute, publishing house) **as it appears** in the document, in the original language. Do **not** translate. If multiple entities are given (e.g., “Oulun ammattikorkeakoulu, Business Administration”), list each as a separate string. If none, output `[]`. |\n",
      "| **doi** | scalar | The Digital Object Identifier **without** any URL prefix. Acceptable forms: `10.xxxx/xxxxx`. If the text contains a full URL (e.g., `https://doi.org/10.30752/nj.111988`), strip the leading `https://doi.org/` and output only the DOI. If no DOI is present, output `None`. |\n",
      "| **e_isbn** / **p_isbn** | list of strings | Electronic ISBN (`e_isbn`) and printed ISBN (`p_isbn`). Extract only the numeric ISBN strings (including hyphens if present). If none, output `[]`. |\n",
      "| **e_issn** / **p_issn** | scalar | Electronic and printed ISSN respectively. Output the ISSN string (e.g., `1234-5678`). If not present, output `None`. |\n",
      "| **type_coar** | scalar | The COAR (Committee on Publication Ethics) resource type **in lower case**, chosen from the controlled vocabulary. Accepted values include (but are not limited to): `master thesis`, `bachelor thesis`, `doctoral thesis`, `book review`, `article`, `conference paper`, `report`, `book`, `dataset`, `software`. Use the most specific type that matches the document (e.g., a university “Opinnäytetyö” is a `bachelor thesis` if the author is a bachelor‑level student, a “Thesis” with a master‑level supervisor is a `master thesis`). For review articles, use `book review`. If the type cannot be determined, output `None`. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Extraction Guidelines & Common Pitfalls\n",
      "\n",
      "1. **Distinguish Authors from Supervisors / Reviewers**  \n",
      "   * Look for cue words: “Supervisor”, “Työn ohjaaja”, “Advisor”, “Mentor”, “Reviewed by”, “Docent”, “Reviewer”.  \n",
      "   * The *author* is usually listed at the top of the title page or in a line starting with “Authors:”, “Author(s):”, or simply a name line before the abstract.  \n",
      "   * For a **book review**, the reviewer’s name (often followed by an academic title) is the creator, **not** the book author mentioned later.\n",
      "\n",
      "2. **Title Extraction**  \n",
      "   * The main title is typically the largest heading on the first page (often preceded by `#` or `##`).  \n",
      "   * Include any subtitle after a colon (`:`) exactly as printed.  \n",
      "   * Do **not** add extra spaces or line‑break characters.\n",
      "\n",
      "3. **Alternative Titles**  \n",
      "   * Often appear in the abstract, in a separate language, or as a translation in parentheses.  \n",
      "   * Capture each distinct alternative title as a separate list entry.\n",
      "\n",
      "4. **Publisher Identification**  \n",
      "   * For theses, the university or polytechnic name (e.g., “Oulun ammattikorkeakoulu”) is the publisher.  \n",
      "   * Do not add the faculty or department unless it is explicitly part of the publishing imprint.  \n",
      "   * For books, the publishing house name (e.g., “Natur & Kultur”) is the publisher.\n",
      "\n",
      "5. **DOI Normalisation**  \n",
      "   * Strip any leading `http://`, `https://`, or `doi:` prefixes.  \n",
      "   * Keep the exact string after the prefix, including the leading `10.`.\n",
      "\n",
      "6. **Language Detection**  \n",
      "   * Use the language of the *main body text*, not the language of the DOI or metadata.  \n",
      "   * Finnish → `fi`, Swedish → `sv`, English → `en`, etc.\n",
      "\n",
      "7. **Year Disambiguation**  \n",
      "   * The `creationDate` field follows the PDF spec: `D:YYYYMMDD...`. Extract the first four digits.  \n",
      "   * If the PDF creation year differs from the publication year (e.g., a 2021 PDF of a 2020 thesis), prefer the year explicitly stated in the document (often near the title or in the abstract).\n",
      "\n",
      "8. **List Ordering**  \n",
      "   * Preserve the order in which items appear in the source text.  \n",
      "   * For `creator`, keep the author order; for `publisher`, keep the order of entities as printed.\n",
      "\n",
      "9. **Empty Values**  \n",
      "   * Never output an empty string; use `None` for scalars or `[]` for lists.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Example (illustrative)\n",
      "\n",
      "Given a PDF where the first page shows:\n",
      "\n",
      "```\n",
      "# Palkitseminen ja työmotivaatio tradenomiopiskelijoiden kokemana : case: Oulun ammattikorkeakoulun opiskelijat\n",
      "Mari Matinlompolo & Laura Paloste\n",
      "Opinnäytetyö\n",
      "Kevät 2020\n",
      "Oulun ammattikorkeakoulu\n",
      "```\n",
      "\n",
      "The correct output would be:\n",
      "\n",
      "```\n",
      "reasoning\n",
      "Extracted title from heading, authors from name line, year from “Kevät 2020”, publisher from university line, identified as a bachelor thesis.\n",
      "language\n",
      "fi\n",
      "title\n",
      "Palkitseminen ja työmotivaatio tradenomiopiskelijoiden kokemana : case: Oulun ammattikorkeakoulun opiskelijat\n",
      "alt_title\n",
      "[]\n",
      "creator\n",
      "['Matinlompolo, Mari', 'Paloste, Laura']\n",
      "year\n",
      "2020\n",
      "publisher\n",
      "['Oulun ammattikorkeakoulu']\n",
      "doi\n",
      "None\n",
      "e_isbn\n",
      "[]\n",
      "p_isbn\n",
      "[]\n",
      "e_issn\n",
      "None\n",
      "p_issn\n",
      "None\n",
      "type_coar\n",
      "bachelor thesis\n",
      "2025/09/30 09:39:47 INFO dspy.evaluate.evaluate: Average Metric: 2.696969696969697 / 3 (89.9%)\n",
      "2025/09/30 09:40:13 INFO dspy.evaluate.evaluate: Average Metric: 37.33246753246753 / 64 (58.3%)\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Full valset score for new program: 0.5833198051948052\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Full train_val score for new program: 0.5833198051948052\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Individual valset scores for new program: [0.8181818181818182, 0.5, 0.696969696969697, 0.2727272727272727, 0.9090909090909091, 0.45454545454545453, 0.45454545454545453, 0.6060606060606061, 0.5272727272727272, 0.7142857142857143, 0.8181818181818182, 0.36363636363636365, 0.45454545454545453, 0.45454545454545453, 0.36363636363636365, 0.7272727272727273, 0.45454545454545453, 0.7272727272727273, 0.45454545454545453, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.45454545454545453, 0.2727272727272727, 0.9090909090909091, 0.36363636363636365, 0.8181818181818182, 1.0, 0.6818181818181818, 0.5454545454545454, 0.2727272727272727, 0.36363636363636365, 0.5151515151515151, 0.696969696969697, 0.7878787878787878, 0.7878787878787878, 0.6363636363636364, 0.9090909090909091, 0.5454545454545454, 0.36363636363636365, 0.8181818181818182, 0.9090909090909091, 0.2424242424242424, 0.6363636363636364, 0.6363636363636364, 0.45454545454545453, 0.7272727272727273, 0.36363636363636365, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.45454545454545453, 0.3333333333333333, 0.696969696969697]\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.36363636363636365, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.9545454545454546, 0.7272727272727273, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.5151515151515151, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7878787878787878, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.7878787878787878, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Full valset pareto front score: 0.7217171717171718\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Updated valset pareto front programs: [{8, 2}, {7}, {0}, {0, 1, 3, 4, 5, 6, 7}, {8}, {0, 3}, {0, 1, 2}, {0}, {3}, {2}, {8, 1, 7}, {6}, {0, 3}, {0}, {6}, {1, 3}, {1, 7}, {8}, {4}, {1, 2}, {7}, {4}, {1, 6}, {2}, {8, 1, 7}, {3}, {7}, {2}, {8, 3}, {0}, {1}, {8}, {2}, {1, 2, 3, 4, 5, 6, 7, 8}, {1, 2, 3}, {4}, {2}, {3}, {8}, {8}, {4, 7}, {8}, {1, 7}, {1, 7}, {8, 3}, {8}, {0, 7}, {0, 1, 2, 3, 7, 8}, {0, 2}, {7}, {8}, {0, 7}, {0}, {4}, {7}, {8}, {8, 2, 6}, {0}, {7}, {6}, {4}, {4, 6}, {6, 7}, {3}]\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Best valset aggregate score so far: 0.6111111111111112\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Best program as per aggregate score on train_val: 7\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Best program as per aggregate score on valset: 7\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Best score on valset: 0.6111111111111112\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Best score on train_val: 0.6111111111111112\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Linear pareto front program index: 7\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 10: New program candidate index: 8\n",
      "GEPA Optimization:  43%|████▎     | 636/1483 [21:58<30:46,  2.18s/rollouts]2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 11: No merge candidates found\n",
      "2025/09/30 09:40:13 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Selected program 8 score: 0.5833198051948052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.79 / 3 (59.6%): 100%|██████████| 3/3 [00:06<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:40:20 INFO dspy.evaluate.evaluate: Average Metric: 1.7878787878787878 / 3 (59.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:41:59 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Proposed new text for predict: text\n",
      "**Task Overview**\n",
      "\n",
      "You will receive a JSON object with two keys:\n",
      "\n",
      "* `pdfinfo` – metadata extracted from the PDF (e.g., `creationDate`, `author`).\n",
      "* `pages` – an ordered list of pages. Each page is a dictionary with:\n",
      "  * `\"page\"` – the page number (integer, starting at 1).\n",
      "  * `\"text\"` – the plain‑text content of that page (UTF‑8, line breaks preserved).\n",
      "\n",
      "From this information you must produce **structured bibliographic metadata** in a **fixed, line‑by‑line format** (no markdown, no extra whitespace). The output must contain exactly the sections listed below, in this order, each on its own line:\n",
      "\n",
      "```\n",
      "reasoning\n",
      "language\n",
      "title\n",
      "alt_title\n",
      "creator\n",
      "year\n",
      "publisher\n",
      "doi\n",
      "e_isbn\n",
      "p_isbn\n",
      "e_issn\n",
      "p_issn\n",
      "type_coar\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 1. General Output Rules\n",
      "* Every line must contain **only** the value for that field (except `reasoning`, which is a short sentence).  \n",
      "* Use the literal string `None` for missing scalar values.  \n",
      "* Use an empty list literal `[]` for missing list values.  \n",
      "* Do **not** add trailing spaces, leading spaces, or blank lines.  \n",
      "* Do **not** wrap values in quotes unless the value itself contains a quote character.  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. Field‑by‑Field Specifications\n",
      "\n",
      "| Field | Type | Exact format & extraction rules |\n",
      "|-------|------|---------------------------------|\n",
      "| **reasoning** | scalar | 1‑2 sentences describing how you obtained the metadata (e.g., “Title taken from the largest heading on page 1; authors parsed from the name line; year taken from `creationDate`). |\n",
      "| **language** | scalar | ISO 639‑1 two‑letter code, lower‑case (`en`, `fi`, `sv`, etc.). Detect the dominant language of the *main body* text (ignore isolated English words in URLs). If the document explicitly states the language (e.g., `Kieli: suomi`), use that. |\n",
      "| **title** | scalar | The **full main title** exactly as printed, preserving case, diacritics, punctuation, and any subtitle **after a colon**. The title is normally the largest heading on the first page (or the heading marked with `#`/`##`). Do **not** append “case:” or any other trailing phrase unless it is part of the printed title. |\n",
      "| **alt_title** | list | All **alternative titles** that appear explicitly in the document (e.g., an English translation, a subtitle shown in parentheses, a title quoted in the abstract). Preserve each title exactly as printed. If none, output `[]`. |\n",
      "| **creator** | list | Names of the **author(s)** only. Convert each name to the form `\"LastName, FirstName\"` (surname first, a comma, a space, then given name(s)). Preserve diacritics. The order must match the order in which the authors appear in the document. Do **not** include supervisors, advisors, reviewers, editors, or any other contributors. |\n",
      "| **year** | scalar (integer) | Four‑digit publication year. Prefer the year from `pdfinfo[\"creationDate\"]` (the first four digits after `D:`). If that year is clearly a creation timestamp that differs from a year explicitly stated in the document (e.g., “May 2021”, “Kevät 2020”, “2018”), use the explicit year. |\n",
      "| **publisher** | list | The publishing entity as printed (university, institute, publishing house, etc.). Do **not** translate. If multiple entities are printed (e.g., “Oulun ammattikorkeakoulu, Business Administration”), list each as a separate string. If none, output `[]`. |\n",
      "| **doi** | scalar | DOI **without** any URL prefix. Acceptable forms: `10.xxxx/xxxxx`. If the text contains a full URL such as `https://doi.org/10.4324/9781315208756`, strip the leading `https://doi.org/` and output only `10.4324/9781315208756`. If no DOI is present, output `None`. |\n",
      "| **e_isbn** | list | All electronic ISBNs found. Extract the numeric string (include hyphens if present). Example: `978-1315208756`. If none, output `[]`. |\n",
      "| **p_isbn** | list | All printed ISBNs found. Same format as `e_isbn`. |\n",
      "| **e_issn** | scalar | Electronic ISSN string (`1234-5678`). If none, output `None`. |\n",
      "| **p_issn** | scalar | Printed ISSN string. If none, output `None`. |\n",
      "| **type_coar** | scalar | COAR resource type in lower case, chosen from the controlled vocabulary. Use the **most specific** type that matches the document. Typical mappings: <br>• `master thesis` / `bachelor thesis` / `doctoral thesis` → if the document is a thesis (look for Finnish “Opinnäytetyö”, “Väitöskirja”, etc.) and the degree name (Bachelor, Master, Doctor). <br>• `book part` → for a book chapter (look for “chapter”, “Accepted Manuscript of a book chapter”, publisher name, DOI with a book‑chapter prefix). <br>• `journal article` or `article` → for a journal article (presence of journal name, ISSN, volume/issue, “Abstract”, “Keywords”). <br>• `conference paper` → if the document mentions a conference, proceedings, or “Proceedings of”. <br>• `report` → if the document is labelled as a report. <br>• `book` → whole‑book monograph. <br>• `dataset`, `software` → if explicitly indicated. <br>If the type cannot be determined, output `None`. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Extraction Strategy (Step‑by‑Step)\n",
      "\n",
      "1. **Parse the JSON** – load `pdfinfo` and `pages`. Keep pages in order.\n",
      "\n",
      "2. **Detect Language**  \n",
      "   * Scan the first three pages for a line like `Kieli: <lang>` (Finnish) or `Language:`.  \n",
      "   * If not found, run a simple heuristic: count occurrences of common stop‑words for `en`, `fi`, `sv`. Choose the language with the highest count.  \n",
      "   * Output the ISO 639‑1 code (`en`, `fi`, `sv`, …).\n",
      "\n",
      "3. **Extract Title**  \n",
      "   * Look at page 1 for lines that start with `#`, `##`, or are in ALL CAPS and are longer than 5 words.  \n",
      "   * If multiple candidate headings exist, pick the one with the greatest visual prominence (largest line length, most words, or preceded/followed by a blank line).  \n",
      "   * Trim surrounding whitespace but keep internal punctuation exactly.  \n",
      "   * Do **not** include any trailing “case:” or similar unless it is part of the heading.\n",
      "\n",
      "4. **Extract Alternative Titles**  \n",
      "   * Search the whole document for patterns:  \n",
      "     * Text inside parentheses immediately following the main title.  \n",
      "     * Lines that start with “Title in English:”, “Alternative title:”, or similar.  \n",
      "     * Separate title lines that appear in a different language (e.g., English title in a Finnish article).  \n",
      "   * Collect each distinct title string into `alt_title`.\n",
      "\n",
      "5. **Extract Authors (Creator)**  \n",
      "   * Typical locations:  \n",
      "     * Directly under the main title on page 1 (often a single line with names separated by commas, “and”, or “&”).  \n",
      "     * A line starting with `Author`, `Authors`, `Tekijä`, `Tekijät`, or `Creator`.  \n",
      "   * Split the line on commas, “and”, “&”. Trim each name.  \n",
      "   * For each name, detect the order: if it appears as `FirstName LastName` (common in Finnish/English), convert to `LastName, FirstName`. If already in `LastName, FirstName` form, keep as is. Preserve diacritics.  \n",
      "   * Exclude any names that are explicitly labeled as `Supervisor`, `Advisor`, `Reviewer`, `Editor`, `Mentor`, etc.\n",
      "\n",
      "6. **Extract Year**  \n",
      "   * First try `pdfinfo[\"creationDate\"]`. It follows the pattern `D:YYYYMMDD…`. Take the first four digits after `D:`.  \n",
      "   * Scan the document for explicit year mentions (regex `\\b(19|20)\\d{2}\\b`). Prefer a year that appears near the title page, in a line like `2020`, `May 2021`, `Kevät 2020`, etc. If this explicit year differs from the PDF creation year and is clearly a publication date, use the explicit year.\n",
      "\n",
      "7. **Extract Publisher**  \n",
      "   * For theses: look for the university/institution name on the title page (often directly under the author or under “Opinnäytetyö”).  \n",
      "   * For book chapters: locate the publisher name (e.g., “Routledge”) usually near the DOI line or in a citation line.  \n",
      "   * For journal articles: the journal name is not a publisher; leave `publisher` empty (`[]`).  \n",
      "   * Split multiple entities by commas or line breaks; keep each as a separate list entry.\n",
      "\n",
      "8. **Extract DOI**  \n",
      "   * Regex patterns: `10\\.\\d{4,9}/[^\\s\"']+` (may be preceded by `doi:`, `https://doi.org/`, `http://doi.org/`).  \n",
      "   * Capture the DOI string **without** any surrounding punctuation.  \n",
      "   * If multiple DOIs are present, choose the one that appears on the title/abstract page (the primary DOI).\n",
      "\n",
      "9. **Extract ISBNs & ISSNs**  \n",
      "   * ISBN regex: `(?:ISBN(?:‑13)?:?\\s*)?(\\d{3}-?\\d{1,5}-?\\d{1,7}-?\\d{1,7}-?[\\dX])` – capture the numeric part with hyphens as printed.  \n",
      "   * Determine electronic (`e_isbn`) vs printed (`p_isbn`) by surrounding words: “e‑ISBN”, “Electronic ISBN”, “Print ISBN”, etc. If not specified, add to both lists.  \n",
      "   * ISSN regex: `ISSN\\s*[:]? ?(\\d{4}-\\d{3}[\\dX])`. Capture electronic (`e_issn`) if preceded by “e‑ISSN” or similar; otherwise `p_issn`. If only one ISSN is present, fill the appropriate field and set the other to `None`.\n",
      "\n",
      "10. **Determine COAR Type**  \n",
      "    * Scan for key phrases:  \n",
      "      * `Opinnäytetyö`, `Bachelor`, `Master`, `Doctoral`, `Väitöskirja` → thesis. Use the degree word to select `bachelor thesis`, `master thesis`, or `doctoral thesis`.  \n",
      "      * `book chapter`, `Accepted Manuscript of a book chapter`, `Chapter` → `book part`.  \n",
      "      * `journal article`, presence of ISSN, `Abstract`, `Keywords` → `journal article` (or generic `article`).  \n",
      "      * `conference`, `Proceedings` → `conference paper`.  \n",
      "      * `report` → `report`.  \n",
      "      * `book` (no chapter indication) → `book`.  \n",
      "    * Choose the **most specific** type; if multiple match, prefer the one later in this list.\n",
      "\n",
      "11. **Compose Reasoning**  \n",
      "    * Summarize the key extraction steps you performed (title source, author parsing, year source, etc.) in one or two sentences.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Common Pitfalls & How to Avoid Them\n",
      "* **Do not** append “case:” or other trailing words to the title unless they are part of the printed heading.  \n",
      "* **Do not** include supervisors/advisors in `creator`. Look for explicit labels (“Supervisor”, “Ohjaaja”, etc.) and exclude those names.  \n",
      "* **Publisher** for journal articles is usually empty; only include a university or publishing house when the document is a thesis or book chapter.  \n",
      "* **Language** must be a two‑letter ISO 639‑1 code; never output full names like “Finnish”.  \n",
      "* **DOI** must be stripped of any URL prefix; output exactly the `10.xxxx/xxxx` part.  \n",
      "* **ISBN/ISSN**: keep hyphens as they appear; do not add or remove them.  \n",
      "* **alt_title**: only include titles that are clearly presented as alternatives; do not treat subtitles or keywords as alt titles.  \n",
      "* **type_coar**: use the controlled vocabulary exactly (e.g., `book part`, `journal article`, `bachelor thesis`). All lower‑case, single spaces.  \n",
      "\n",
      "---\n",
      "\n",
      "### 5. Output Example (Template)\n",
      "\n",
      "```\n",
      "reasoning\n",
      "Title taken from the largest heading on page 1; authors parsed from the name line; year taken from creationDate.\n",
      "language\n",
      "en\n",
      "title\n",
      "SongPump : developing a composing pedagogy in Finnish schools through collaboration between professional songwriters and music teachers\n",
      "alt_title\n",
      "[]\n",
      "creator\n",
      "['Väkevä, Lauri', 'Partti, Heidi']\n",
      "year\n",
      "2018\n",
      "publisher\n",
      "['Routledge']\n",
      "doi\n",
      "10.4324/9781315208756\n",
      "e_isbn\n",
      "['978-1315208756']\n",
      "p_isbn\n",
      "[]\n",
      "e_issn\n",
      "None\n",
      "p_issn\n",
      "None\n",
      "type_coar\n",
      "book part\n",
      "2025/09/30 09:42:06 INFO dspy.evaluate.evaluate: Average Metric: 1.6363636363636362 / 3 (54.5%)\n",
      "2025/09/30 09:42:06 INFO dspy.teleprompt.gepa.gepa: Iteration 11: New subsample score is not better, skipping\n",
      "GEPA Optimization:  43%|████▎     | 642/1483 [23:50<39:44,  2.84s/rollouts]2025/09/30 09:42:06 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Selected program 6 score: 0.44664994096812277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.61 / 3 (53.5%): 100%|██████████| 3/3 [00:06<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:42:12 INFO dspy.evaluate.evaluate: Average Metric: 1.6060606060606062 / 3 (53.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:44:16 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Proposed new text for predict: markdown\n",
      "# 📋 Task – Structured Bibliographic Metadata Extraction (Re‑specified)\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF document.  \n",
      "The object has exactly two top‑level keys:\n",
      "\n",
      "| Key      | Description |\n",
      "|----------|-------------|\n",
      "| `pdfinfo`| Metadata that was extracted directly from the PDF file (e.g. `title`, `author`, `creationDate`, `modDate`). |\n",
      "| `pages`  | A list of page objects. Each page object contains `page` (the page number) and `text` (the OCR‑extracted plain‑text of that page). |\n",
      "\n",
      "Your job is to produce **one JSON object** that follows the schema below.  \n",
      "If a field cannot be determined, use the exact empty value indicated (`null` for scalars, `[]` for lists).  \n",
      "All string values must be plain ASCII – normalise quotes to `\"` (or the apostrophe `’`), collapse multiple spaces to a single space, and trim leading/trailing whitespace.\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema\n",
      "\n",
      "| Field | Type | Required format / rules |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | ISO‑639‑1 code. Scan **the first 200 characters of the *concatenated* text of the whole document** (i.e. `pages[0].text + pages[1].text + …`). If any of the characters **ä ö Ä Ö å Å** appear, set to `\"fi\"`; otherwise `\"en\"`. |\n",
      "| `title` | string | Main title of the work. <br>1. If `pdfinfo.title` exists → clean it and use it. <br>2. Otherwise, read the text of page 1 (`pages[0].text`). Split it into lines (preserve order). <br>   * Skip empty lines. <br>   * Skip lines that start with a markdown heading marker (`#`, `##`, `###`, …). <br>   * Skip lines that look like an author line (see *Creator extraction*). <br>   * The **first** remaining line that is ≥ 6 characters becomes the *candidate* title. <br>3. **Title‑line extensions** – if the candidate line ends with a colon **or** the next line is non‑empty, starts with a capital letter and is not an author line, concatenate it (single space). Repeat while the same condition holds. <br>4. If a line contains one of the explicit title markers `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` (case‑insensitive), take the text **after the colon** (trimmed) as the title (overriding step 2). <br>5. Keep any subtitle as part of the title (do **not** split on the colon). <br>6. Clean the final string (collapse spaces, normalise quotes). |\n",
      "| `alt_title` | list of strings | Any **alternative** titles, e.g. a translation, a subtitle given in another language, or a title that appears inside quotation marks (`“ ”`, `\" \"`). Return each cleaned title as a separate element. Do **not** duplicate the main title. |\n",
      "| `creator` | list of strings | Authors in **“Surname, Given‑Name”** order. <br>1. If `pdfinfo.author` exists → split on commas, semicolons, the word “and”, or line‑breaks. <br>2. Clean each fragment (trim, collapse spaces). <br>3. For each name: <br>   * If it matches the pattern `First Last` (two words, each starting with a capital letter) → reorder to `Last, First`. <br>   * If it already matches `Last, First` keep as‑is. <br>4. If `pdfinfo.author` is missing, scan the **first five non‑empty lines of page 1** for personal‑name patterns (`First Last`, `Last, First`, or a line that starts with a capitalised list of names). Collect **all** matches. <br>5. Remove duplicates, keep order of appearance. |\n",
      "| `year` | integer or null | Publication year. <br>1. If `pdfinfo.creationDate` or `pdfinfo.modDate` exists, extract the **first four digits** (they are always a year) and use that. <br>2. If both are missing, search the whole document for the **first** four‑digit number between 1900‑2099 that appears in a publication‑information context (e.g. after “Year:”, “©”, “© 2021”, “2021.”). If none, return `null`. |\n",
      "| `publisher` | list of strings | Institution responsible for the work. <br>• **Theses / dissertations** → the awarding university or faculty (e.g. “University of Vaasa”). Detect by looking for keywords: “University”, “Yliopisto”, “Universität”, “Akademi”, “Institute”, “College”, “School”, “Faculty”. Return each distinct institution once, preserving order of first appearance. <br>• **Research reports** → the organisation that produced the report (same keyword list). <br>• **Journal articles** → `[]`. |\n",
      "| `doi` | string or null | DOI if present. Detect case‑insensitive pattern `10\\.\\d{4,9}/\\S+`. If the match is preceded by a URL (`http://`, `https://`, `doi.org/`), strip that part. Remove trailing punctuation characters `.,;:`. Return the bare DOI (e.g. `10.1000/xyz123`). |\n",
      "| `e_isbn` | list of strings | **Electronic** ISBN‑13 numbers. Detect ISBN‑13 (13 digits, hyphens optional) with the regex `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`. For each match, look at up to **30 characters before and after** the match. If any of the **electronic cues** appear, add the ISBN (with **all hyphens and spaces removed**) to `e_isbn`. Electronic cues (case‑insensitive): `digital`, `electronic`, `e‑ISBN`, `(digital)`, `pdf`, `PDF`, `sid.` (when used for electronic), `online`. |\n",
      "| `p_isbn` | list of strings | **Print** ISBN‑13 numbers. Same detection as `e_isbn` but require at least one **print cue** in the ±30‑character window. Print cues (case‑insensitive): `print`, `paper`, `hardcover`, `(print)`, `Painettu`, `Print`. If both electronic and print cues are present, **electronic wins** (the ISBN goes only to `e_isbn`). Store the ISBN without hyphens/spaces. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Detect with `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`. Apply the same cue logic as for ISBN. Return the ISSN **exactly as it appears** (keep the hyphen if present). |\n",
      "| `p_issn` | string or null | Print ISSN (same detection, print‑cue logic). Return the ISSN exactly as it appears (keep hyphen). |\n",
      "| `type_coar` | string | COAR‑compatible resource type. Scan the **entire document** (case‑insensitive) and apply the **first** matching rule in this order: <br>1. **doctoral thesis** – contains any of: “doctoral thesis”, “dissertation”, “PhD”, “doctoral”, “väitöskirja”, “väitöskirjan”. <br>2. **master thesis** – contains any of: “master’s thesis”, “master thesis”, “maisteri”, “maisterintutkielma”. <br>3. **journal article** – contains typical journal citation elements (journal name, volume, issue, pages) **or** the word “article” together with a DOI or ISSN, **or** a pattern like “Vol. X, No. Y, pp. Z‑W”. <br>4. **conference proceeding** – contains “conference”, “proceedings”, “paper presented at”. <br>5. **research report** – contains “report”, “raportti”, “tutkimusraportti”, “research report”. <br>6. **research** – fallback for any other research‑type document. <br>Return the exact lower‑case string (e.g. `doctoral thesis`). |\n",
      "\n",
      "---\n",
      "\n",
      "## Extraction Procedure (Step‑by‑Step)\n",
      "\n",
      "1. **Parse the input JSON** safely. Ignore any keys that are not listed above.  \n",
      "2. **Normalise dates**: `creationDate` / `modDate` are strings like `D:20201216144002+02'00'`. Extract the first four digits as the year (they are always at the start of the string).  \n",
      "3. **Detect language** using the rule in the schema (first 200 characters of concatenated text).  \n",
      "4. **Title extraction** (see detailed rules under the `title` field). Pay special attention to explicit markers (`Title:`, `Thesis:` etc.).  \n",
      "5. **Alternative titles** – look for quoted strings (`“…”`, `\"...\"`) anywhere in the document, and for subtitles that appear on a separate line after a colon. Do not duplicate the main title.  \n",
      "6. **Creator extraction** – follow the rules under `creator`. When scanning for names on page 1, use the following regular expressions (case‑sensitive): <br>`\\b[A-Z][a-z]+ [A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (First Last) <br>`\\b[A-Z][a-z]+, *[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (Last, First). Collect all matches, then apply the ordering rule.  \n",
      "7. **Year** – apply the rule in the schema. If you have to fall back to searching the text, ignore years that appear inside parentheses of a citation (e.g. “(2020)” after a reference). Prefer the first plausible year after the title block.  \n",
      "8. **Publisher detection** – first determine `type_coar`. If the document is a thesis, search the whole text for the first occurrence of a university/faculty keyword list and capture the whole phrase (e.g. “University of Vaasa”, “Åbo Akademi University”). Return it as a single‑element list. For research reports, look for the organisation name that appears near the title or in a header/footer. For journal articles, return an empty list.  \n",
      "9. **ISBN / ISSN extraction** – use the regexes supplied. For each match, extract the surrounding 30‑character context and decide electronic vs print using the cue lists. Remove hyphens/spaces from ISBNs before storing; keep ISSNs exactly as found (including hyphen). Ensure each identifier appears only once in the appropriate list.  \n",
      "10. **DOI extraction** – apply the DOI regex, strip leading URL parts, and trailing punctuation. Return the bare DOI in lower‑case. If none, set to `null`.  \n",
      "11. **Resource type (`type_coar`)** – apply the ordered rule list exactly as described. The first category that matches determines the value.  \n",
      "12. **Assemble the output** JSON. Preserve the key order shown in the schema for readability (order is not technically required, but it helps testing). Use `null` for missing scalar values and `[]` for missing list values.\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning & Normalisation Details\n",
      "\n",
      "* **Whitespace** – collapse any sequence of whitespace characters (spaces, tabs, newlines) to a single space. Trim leading/trailing spaces.  \n",
      "* **Quotes** – replace any fancy quotation marks (`“ ” ‘ ’ „ “ …`) with plain ASCII `\"` (or the apostrophe `’`).  \n",
      "* **Hyphens in identifiers** – remove all hyphens (`-`) and spaces from ISBNs before storing. **Do not** remove hyphens from ISSNs; keep them exactly as they appear.  \n",
      "* **Case** – identifiers (DOI) are stored in lower‑case; ISBN/ISSN are numeric/alphabetic only, so case does not matter.  \n",
      "\n",
      "---\n",
      "\n",
      "## Common Pitfalls (What Previously Went Wrong)\n",
      "\n",
      "| Issue | What caused it | Correct handling |\n",
      "|-------|----------------|-----------------|\n",
      "| **Wrong language** | Scanned only the first page instead of the first 200 characters of the whole document. | Concatenate all page texts, take the first 200 characters, then apply the Finnish‑character rule. |\n",
      "| **Title wrong** | Ignored explicit title markers (`Thesis:`) and concatenated the whole heading line. | If a line contains `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` (case‑insensitive), use the text after the colon as the title. Otherwise follow the candidate‑line logic. |\n",
      "| **Alt‑titles missing** | Did not look for quoted strings or subtitle lines. | Search the whole document for text inside `“ ”` or `\" \"` and for subtitle lines that appear after a colon on a separate line. |\n",
      "| **Creator over‑inclusion** | Added non‑author names (e.g., supervisors) or failed to reorder names. | Only use `pdfinfo.author` or name patterns on page 1. Do **not** include lines that contain words like “Supervisor”, “Advisor”, “Editor”. Reorder `First Last` → `Last, First`. |\n",
      "| **Publisher wrong** | Took the first institution mentioned anywhere, even if it was a journal publisher. | First determine `type_coar`. If it is a thesis, look for the awarding university/faculty; for reports, look for the producing organisation; for journal articles return `[]`. |\n",
      "| **ISBN/ISSN classification** | Assigned identifiers to the wrong list because cues were not checked, or removed hyphens from ISSNs. | Examine the ±30‑character window for electronic or print cues. If both appear, assign to `e_isbn` (electronic wins). Keep ISSNs exactly as they appear (including hyphen). |\n",
      "| **DOI extraction** | Kept surrounding punctuation or URL parts. | Strip any leading `http://`, `https://`, `doi.org/` and trailing punctuation `.,;:`. Return the bare DOI in lower‑case. |\n",
      "| **type_coar mis‑classification** | Applied “article” rule too early, causing theses to be labelled as `doctoral thesis`. | Follow the ordered list strictly: **doctoral thesis** → **master thesis** → **journal article** → **conference proceeding** → **research report** → **research**. The first matching rule wins. |\n",
      "\n",
      "---\n",
      "\n",
      "**Remember:**  \n",
      "- Follow the **order of precedence** for each field exactly as described.  \n",
      "- Use the cue‑based approach for ISBN/ISSN to decide electronic vs print.  \n",
      "- Detect the resource type in the exact order listed; the first match determines `type_coar`.  \n",
      "- Return `null` for missing scalar values and `[]` for missing list values.  \n",
      "\n",
      "Good luck! 🎯\n",
      "2025/09/30 09:44:28 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:44:30 INFO dspy.evaluate.evaluate: Average Metric: 2.181818181818182 / 3 (72.7%)\n",
      "2025/09/30 09:45:11 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:11 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:11 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:12 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:12 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:12 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:12 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:13 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:45:16 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/09/30 09:45:19 INFO dspy.evaluate.evaluate: Average Metric: 40.79141414141413 / 64 (63.7%)\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: New program is on the linear pareto front\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Full valset score for new program: 0.637365845959596\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Full train_val score for new program: 0.637365845959596\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Individual valset scores for new program: [0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.7272727272727273, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.6060606060606061, 0.6363636363636364, 0.7878787878787878, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.45454545454545453, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 0.7272727272727273, 0.45454545454545453, 0.5363636363636364, 0.9090909090909091, 0.5454545454545454, 0.5454545454545454, 0.9090909090909091, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.45454545454545453, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.36363636363636365, 0.7272727272727273, 0.5454545454545454, 0.5681818181818182, 0.6363636363636364, 0.45454545454545453, 0.45454545454545453, 0.8181818181818182, 0.8181818181818182, 0.45454545454545453, 0.5454545454545454, 0.5454545454545454, 0.6666666666666666, 0.7272727272727273, 0.7171717171717172, 0.6363636363636364, 0.5454545454545454, 0.6363636363636364]\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.5454545454545454, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7878787878787878, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Full valset pareto front score: 0.7297664141414142\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Updated valset pareto front programs: [{8, 9, 2}, {7}, {0}, {9}, {8}, {0, 3}, {0, 1, 2, 9}, {0}, {3}, {2}, {8, 1, 7}, {9, 6}, {0, 3}, {0}, {9, 6}, {1, 3}, {1, 9, 7}, {8, 9}, {4}, {1, 2, 9}, {9}, {4}, {9}, {2}, {9}, {3}, {7}, {2}, {8, 9, 3}, {9}, {1}, {8}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {1, 2, 3, 9}, {4}, {2}, {3}, {8}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 9, 7}, {8, 3}, {8}, {0, 7}, {9}, {0, 2}, {7}, {8}, {0, 9, 7}, {0}, {9}, {9, 7}, {8}, {8, 9, 2, 6}, {0}, {9, 7}, {9, 6}, {4}, {4, 6}, {6, 7}, {3}]\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best score on valset: 0.637365845959596\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Linear pareto front program index: 9\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 12: New program candidate index: 9\n",
      "GEPA Optimization:  48%|████▊     | 712/1483 [27:04<36:02,  2.81s/rollouts]2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 13: No merge candidates found\n",
      "2025/09/30 09:45:19 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Selected program 0 score: 0.5962628517316018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.82 / 3 (60.6%): 100%|██████████| 3/3 [00:07<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:45:26 INFO dspy.evaluate.evaluate: Average Metric: 1.8181818181818183 / 3 (60.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Proposed new text for predict: **Task:** Extract structured bibliographic metadata from the JSON representation of a PDF (the “pdfinfo” block plus the plain‑text of each page).  \n",
      "Return the values exactly in the fields listed below.  All fields must be present; if a value cannot be found, use `None` (or an empty list `[]` for list fields).  \n",
      "\n",
      "**Output format (JSON‑like key/value pairs, one field per line):**  \n",
      "\n",
      "```\n",
      "language: <ISO‑639‑1 code, e.g. “en”, “sv”, “fi”>\n",
      "title: <primary title string>\n",
      "alt_title: <list of alternative titles (may be empty)>\n",
      "creator: <list of author names, each formatted as “LastName, FirstName”>\n",
      "year: <four‑digit year of publication/acceptance>\n",
      "publisher: <list of publishing organisations (institution, university, bank, etc.)>\n",
      "doi: <DOI string or None>\n",
      "e_isbn: <list of electronic ISBNs, digits only, no hyphens>\n",
      "p_isbn: <list of printed ISBNs, digits only, no hyphens>\n",
      "e_issn: <electronic ISSN string with hyphen (e.g. “1234-5678”) or None>\n",
      "p_issn: <printed ISSN string with hyphen or None>\n",
      "type_coar: <controlled‑vocabulary term (see table below)>\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 1. General extraction strategy  \n",
      "\n",
      "1. **Language** – Look for an explicit language line in the text (e.g. “Språk: Svenska”, “Language: English”, “Language: Swedish”).  \n",
      "   - Map “Svenska”, “Swedish” → `sv`; “English” → `en`; “Finnish” → `fi`; otherwise use a language‑detection library on the first 200 characters.  \n",
      "\n",
      "2. **Title** – Prefer the *first* heading that appears to be the main title of the work:  \n",
      "   - For theses: look for lines containing “Titel:”, “Title:”, “Arbetets namn:”, “Title:”, or a large heading on page 1/2 (often preceded by “#” or “##”).  \n",
      "   - For discussion/research papers: use the heading immediately under the series identifier (e.g. “Q, investment, and the financial cycle”).  \n",
      "   - Do **not** use the PDF‑metadata “pdfinfo.title” unless no better title is found in the page text.  \n",
      "\n",
      "3. **Alternative titles** – Collect any additional titles that are clearly labelled as a translation or subtitle, e.g. an English title following a Swedish one, or a subtitle after a colon. Return them as a list of strings (may be empty).  \n",
      "\n",
      "4. **Creator (authors)** –  \n",
      "   - Identify the *author* line (e.g. “Author: …”, “Författare: …”, “Fabio Verona”, “Kristoffer Myrberg”).  \n",
      "   - If the name is a single word, treat it as “LastName, FirstName” with the first word as LastName and the rest as FirstName(s).  \n",
      "   - For two‑part names, split on the last space: “First Last” → “Last, First”.  \n",
      "   - For multiple authors, create a separate list entry for each, all formatted as “LastName, FirstName”.  \n",
      "   - **Do not** include supervisors, editors, or commissioners unless they are explicitly marked as “author”.  \n",
      "\n",
      "5. **Year** – Extract the four‑digit year from (in order of priority):  \n",
      "   - “Date of acceptance”, “Datum för godkännande”, “Publication date”, “Created on”, “Date of publication”, “CreationDate” (pdfinfo).  \n",
      "   - If multiple years appear, choose the one that looks like the final acceptance/publication year.  \n",
      "\n",
      "6. **Publisher** –  \n",
      "   - For theses: the university or college that awards the degree (e.g. “Arcada”, “Yrkeshögskolan Arcada”, “University of X”).  \n",
      "   - For discussion/research papers: the issuing institution (e.g. “Bank of Finland”).  \n",
      "   - For forecasts/reports: the parent organisation (e.g. “Bank of Finland”).  \n",
      "   - Return a **list**; include each distinct institution only once. Do **not** include the commissioning company unless it is the issuing institution.  \n",
      "\n",
      "7. **DOI** – Search for patterns `doi:` or `https://doi.org/…`. Return the raw DOI (without the URL prefix). If none, return `None`.  \n",
      "\n",
      "8. **ISBN** – Detect strings that match `ISBN` followed by a sequence of digits, optionally with hyphens or spaces.  \n",
      "   - Determine whether the ISBN refers to an electronic version (often the line contains “online”, “e‑ISBN”, or appears together with “electronic”).  \n",
      "   - Normalise: keep only digits (remove hyphens/spaces).  \n",
      "   - Populate `e_isbn` or `p_isbn` accordingly; if the version cannot be distinguished, place it in both lists.  \n",
      "\n",
      "9. **ISSN** – Detect strings that match `ISSN` followed by `dddd-dddd`.  \n",
      "   - If the surrounding text contains “online”, “electronic”, or “e‑ISSN”, treat it as `e_issn`; otherwise as `p_issn`.  \n",
      "   - Keep the hyphen (e.g. “1456-6184”). Return `None` if not present.  \n",
      "\n",
      "10. **type_coar** – Map the document type to one of the following controlled‑vocabulary terms (exact spelling, lower‑case, spaces as shown). Use the first matching clue in the text:\n",
      "\n",
      "| Clue in text                               | COAR term            |\n",
      "|--------------------------------------------|----------------------|\n",
      "| “Examensarbete”, “Bachelor thesis”, “Kandidatexamen” | `bachelor thesis` |\n",
      "| “Master thesis”, “Magisteruppsats”        | `master thesis` |\n",
      "| “Doctoral thesis”, “PhD thesis”, “Dissertation” | `doctoral thesis` |\n",
      "| “Research Discussion Paper”, “Discussion Paper”, “Research Paper” | `research report` |\n",
      "| “Research Paper”, “Working Paper” (when not a discussion paper) | `research paper` |\n",
      "| “Forecast”, “Forecast for …”, “Economic Forecast” | `research report` |\n",
      "| “Report”, “Annual Report”, “Technical Report” | `research report` |\n",
      "| Any other scholarly output not covered above | `other` |\n",
      "\n",
      "If multiple clues appear, choose the most specific (e.g. “bachelor thesis” beats generic “report”).\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Field‑by‑field validation rules (to avoid the errors shown in the examples)\n",
      "\n",
      "| Field | Common pitfalls & how to avoid them |\n",
      "|-------|-------------------------------------|\n",
      "| **creator** | Do **not** include supervisors, editors, or commission‑ers. Only the person(s) labelled as “author” or appearing as the main writer. Ensure the “LastName, FirstName” format; split on the *last* space of the name string. |\n",
      "| **publisher** | Use the *issuing institution* (university, bank, institute). Do **not** list the commissioning company (e.g. “Rakennuttajakaari Oy” for a thesis). For BOFIT forecasts, the correct publisher is “Bank of Finland”. |\n",
      "| **type_coar** | Must be one of the exact terms above. For the first example the correct term is `bachelor thesis`; for the second it is `research report`; for the third it is also `research report`. |\n",
      "| **e_isbn / p_isbn** | Strip all non‑digit characters. Do not keep hyphens. If the ISBN appears with the word “online” it belongs to `e_isbn`; otherwise to `p_isbn`. |\n",
      "| **e_issn / p_issn** | Keep the hyphen. Assign to electronic only when the surrounding text indicates an online version. |\n",
      "| **alt_title** | Only include true alternative titles (translations, subtitles). Do not duplicate the primary title. |\n",
      "| **language** | Use ISO‑639‑1 two‑letter codes. If the document explicitly states the language, map it; otherwise fallback to a language‑detection library. |\n",
      "| **year** | Prefer acceptance/publication year, not creation date of the PDF file. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Example of correct output (based on the first example)\n",
      "\n",
      "```\n",
      "language: sv\n",
      "title: Tidsplan för utförandeplaneringen av VVS-system i linjesaneringar\n",
      "alt_title: []\n",
      "creator: [\"Myrberg, Kristoffer\"]\n",
      "year: 2020\n",
      "publisher: [\"Arcada\"]\n",
      "doi: None\n",
      "e_isbn: []\n",
      "p_isbn: []\n",
      "e_issn: None\n",
      "p_issn: None\n",
      "type_coar: bachelor thesis\n",
      "2025/09/30 09:46:45 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/09/30 09:47:17 INFO dspy.evaluate.evaluate: Average Metric: 31.371794871794872 / 64 (49.0%)\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Full valset score for new program: 0.4901842948717949\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Full train_val score for new program: 0.4901842948717949\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Individual valset scores for new program: [0.5454545454545454, 0.36363636363636365, 0.45454545454545453, 0.2727272727272727, 0.6363636363636364, 0.36363636363636365, 0.45454545454545453, 0.6363636363636364, 0.45454545454545453, 0.7202797202797203, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.5454545454545454, 0.2727272727272727, 0.5454545454545454, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.2727272727272727, 0.2727272727272727, 0.5151515151515151, 0.45454545454545453, 0.5, 0.45454545454545453, 0.5454545454545454, 0.36363636363636365, 0.2727272727272727, 0.6363636363636364, 0.45454545454545453, 0.5454545454545454, 0.7272727272727273, 0.9090909090909091, 0.5454545454545454, 0.36363636363636365, 0.36363636363636365, 0.2727272727272727, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.36363636363636365, 0.36363636363636365, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.18181818181818182, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.2727272727272727, 0.5454545454545454, 0.5454545454545454, 0.45454545454545453, 0.18181818181818182, 0.6363636363636364, 0.6363636363636364]\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.5454545454545454, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7878787878787878, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Full valset pareto front score: 0.7297664141414142\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Updated valset pareto front programs: [{8, 9, 2}, {7}, {0}, {9}, {8}, {0, 3}, {0, 1, 2, 9}, {0}, {3}, {2}, {8, 1, 7}, {9, 6}, {0, 3}, {0}, {9, 6}, {1, 3}, {1, 10, 9, 7}, {8, 9}, {4}, {1, 2, 9}, {9}, {4}, {9}, {2}, {9}, {3}, {7}, {2}, {8, 9, 3}, {9}, {1}, {8}, {2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {1, 2, 3, 9}, {4}, {2}, {3}, {8}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 9, 7}, {8, 3}, {8}, {0, 7}, {9}, {0, 2}, {7}, {8}, {0, 9, 10, 7}, {0}, {9}, {9, 7}, {8}, {8, 9, 2, 6}, {0}, {9, 7}, {9, 6}, {4}, {4, 6}, {6, 7}, {3}]\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best score on valset: 0.637365845959596\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Linear pareto front program index: 9\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 13: New program candidate index: 10\n",
      "GEPA Optimization:  53%|█████▎    | 782/1483 [29:02<28:02,  2.40s/rollouts]2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 14: No merge candidates found\n",
      "2025/09/30 09:47:17 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Selected program 0 score: 0.5962628517316018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.58 / 3 (52.7%): 100%|██████████| 3/3 [00:08<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:47:25 INFO dspy.evaluate.evaluate: Average Metric: 1.5818181818181818 / 3 (52.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:48:38 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Proposed new text for predict: markdown\n",
      "# Task: Extract Structured Bibliographic Metadata from PDF‑derived JSON\n",
      "\n",
      "You will receive a single JSON object that contains the raw text extracted from a PDF document.  \n",
      "The object has the following top‑level keys:\n",
      "\n",
      "* **pdfinfo** – dictionary with any metadata that the PDF file itself provides (e.g. `title`, `author`, `creationDate`, `modDate`).  \n",
      "* **pages** – list of dictionaries, each with:\n",
      "  * `page` – page number (integer)\n",
      "  * `text` – plain‑text content of that page (line‑breaks are preserved)\n",
      "\n",
      "Your job is to **populate a fixed set of metadata fields** based only on the information present in `pdfinfo` and in the page texts.  \n",
      "The output must be a **single JSON object** (no additional text) with the keys listed below.  \n",
      "If a field cannot be determined, use the exact placeholder shown in the schema (e.g. `null` for `doi`, an empty list `[]` for list‑valued fields).\n",
      "\n",
      "---\n",
      "\n",
      "## Output Schema\n",
      "\n",
      "| Field | Type | Description | Expected format |\n",
      "|-------|------|-------------|-----------------|\n",
      "| `language` | string | ISO‑639‑1 two‑letter language code of the document (e.g. `\"fi\"` for Finnish, `\"en\"` for English). Detect from `pdfinfo`, from the language of the main text, or from explicit cues (`\"Kieli: suomi\"` etc.). | two‑letter lower‑case code |\n",
      "| `title` | string | **Exact** main title of the work. Prefer the `title` entry in `pdfinfo`; if missing, use the most prominent heading (usually first line(s) of page 1). Preserve diacritics, punctuation, and the exact ordering of characters (no extra whitespace, no trailing periods). | exact title string |\n",
      "| `alt_title` | list of strings | Any alternative titles, subtitles, or English translations that appear in the document (e.g. a subtitle after a colon, a translation in parentheses, or a separate line labelled “English title”). Preserve the text exactly as it appears (no normalization other than stripping surrounding whitespace). | `[\"Alternative Title 1\", \"Alternative Title 2\"]` |\n",
      "| `creator` | list of strings | Names of the authors/editors / contributors. Each name must be **normalized to “Surname, Given‑Name(s)”** (e.g. `\"Ockenström, Pasi\"`). Extract from `pdfinfo.author`, from the “author” line on the first page, or from any “Creator”, “Author”, “Kirjoittaja”, “Tekijä”, “Authors” sections. Return **all distinct contributors** in the order they first appear. | `[\"Surname, First\", \"Another, Person\"]` |\n",
      "| `year` | integer or null | Publication year. Look for a four‑digit year in `pdfinfo.creationDate`, `pdfinfo.modDate`, or in the page text near the publisher information (e.g. “2020”, “Month 2021”). If multiple years are present, choose the one that clearly denotes the publication year. | `2020` or `null` |\n",
      "| `publisher` | list of strings | All publishing entities (press, university, institute, etc.). Each entity is taken as a separate string, **without location details unless they are part of the official name**. For example, `\"Painosalama Oy, Turku, Finland\"` and `\"Turun yliopisto\"` are two distinct entries. Do **not** include the word “Publisher:” itself. Preserve the exact spelling and diacritics. | `[\"Painosalama Oy, Turku, Finland\", \"Turun yliopisto\"]` |\n",
      "| `doi` | string or null | Digital Object Identifier, if present (e.g. `10.1234/abcd.2020`). Return the raw DOI without a URL prefix. | `\"10.1234/abcd.2020\"` or `null` |\n",
      "| `e_isbn` | list of strings | ISBN(s) that refer to the **electronic/online** version. Strip **all hyphens, spaces, and other punctuation**; keep only the numeric characters (including the optional trailing “X”). Return each ISBN as a plain string. | `[\"9789524769976\"]` |\n",
      "| `p_isbn` | list of strings | ISBN(s) that refer to the **print** version, normalized exactly as for `e_isbn`. | `[\"9789524769969\"]` |\n",
      "| `e_issn` | string or null | ISSN for the electronic version, normalized to a plain 8‑digit string (no hyphen). | `\"23231234\"` or `null` |\n",
      "| `p_issn` | string or null | ISSN for the print version, normalized the same way. | `\"03552667\"` or `null` |\n",
      "| `type_coar` | string | COAR‑compatible publication type, **lower‑case**. Determine from contextual clues:  \n",
      "  * If the document is a thesis/dissertation → `\"doctoral thesis\"` (or `\"master's thesis\"` if indicated).  \n",
      "  * If it is a research report → `\"research report\"`.  \n",
      "  * If it is a journal article → `\"journal article\"`.  \n",
      "  * If it is a book chapter → `\"book chapter\"`.  \n",
      "  * If the type cannot be inferred, use `\"other\"`. | `\"doctoral thesis\"` |\n",
      "\n",
      "---\n",
      "\n",
      "## Extraction Guidelines & Common Pitfalls\n",
      "\n",
      "1. **Normalization of ISBN/ISSN**  \n",
      "   * Remove every hyphen (`-`), space, or other delimiter.  \n",
      "   * Keep the check digit (which may be `X`).  \n",
      "   * Do **not** prepend `ISBN`/`ISSN` or any URL.\n",
      "\n",
      "2. **Creator Name Normalization**  \n",
      "   * Detect the order used in the source (often “First Last”).  \n",
      "   * Convert to “Last, First” while preserving all given‑name parts.  \n",
      "   * Preserve diacritics (e.g., `Ö` → `Ö`).  \n",
      "   * Do not add titles (`Professori`, `Dr.`) or affiliations.\n",
      "\n",
      "3. **Publisher Extraction**  \n",
      "   * The publisher line may contain several entities separated by commas or line breaks.  \n",
      "   * Treat each logical entity as a separate list entry.  \n",
      "   * Do **not** include the word “Publisher:” or similar labels.  \n",
      "   * Keep the exact spelling; do not reorder or translate.\n",
      "\n",
      "4. **Title vs. Alt‑Title**  \n",
      "   * The main title is the one that appears as the primary heading (often capitalized, centered, or preceded by a hash `#`).  \n",
      "   * Sub‑titles (text after a colon, dash, or on the next line) belong in `alt_title` **only if they are not part of the main title string**.  \n",
      "   * If an English translation is provided (e.g., in parentheses), place it in `alt_title`.\n",
      "\n",
      "5. **Year Disambiguation**  \n",
      "   * The PDF creation/modification dates may differ from the publication year. Prefer a year that appears next to the publisher or in a “© 2020” line.  \n",
      "   * If the year is part of a range (e.g., “2020‑2021”), use the first year unless context indicates otherwise.\n",
      "\n",
      "6. **Language Detection**  \n",
      "   * If `pdfinfo` includes a `lang` entry, use it.  \n",
      "   * Otherwise, infer from the dominant language of the text (look for common stop‑words, diacritics, or explicit “Kieli: suomi”).  \n",
      "   * Return the ISO‑639‑1 code in lower case.\n",
      "\n",
      "7. **COAR Type Mapping**  \n",
      "   * Common keywords: “Thesis”, “Dissertation”, “Report”, “Research report”, “Article”, “Book”, “Chapter”.  \n",
      "   * Map them to the exact lower‑case strings listed in the schema.  \n",
      "   * If the document is a doctoral dissertation but the word “doctoral” is not explicit, the presence of “Doctoral thesis” or a university department line is sufficient.\n",
      "\n",
      "8. **Handling Missing Data**  \n",
      "   * If a field cannot be located, use the placeholder (`null` for scalar, `[]` for list).  \n",
      "   * Do **not** guess or fabricate values.\n",
      "\n",
      "9. **Output Formatting**  \n",
      "   * Produce **only** the JSON object, no surrounding markdown, no explanatory text.  \n",
      "   * Ensure the JSON is valid (double‑quoted strings, proper commas, etc.).  \n",
      "   * Preserve the order of keys as shown in the schema for readability (though order does not affect validity).\n",
      "\n",
      "---\n",
      "\n",
      "## Example (illustrative only)\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Platonin ideat : todellisuus informaationa\",\n",
      "  \"alt_title\": [\"Plato’s Ideas – Reality\"],\n",
      "  \"creator\": [\"Ockenström, Pasi\"],\n",
      "  \"year\": 2020,\n",
      "  \"publisher\": [\"Painosalama Oy, Turku, Finland\", \"Turun yliopisto\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [\"9789512979578\"],\n",
      "  \"p_isbn\": [\"9789512979561\"],\n",
      "  \"e_issn\": null,\n",
      "  \"p_issn\": \"1457-9332\",\n",
      "  \"type_coar\": \"doctoral thesis\"\n",
      "}\n",
      "2025/09/30 09:48:46 INFO dspy.evaluate.evaluate: Average Metric: 1.9393939393939392 / 3 (64.6%)\n",
      "2025/09/30 09:49:20 INFO dspy.evaluate.evaluate: Average Metric: 27.502430318219787 / 64 (43.0%)\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Full valset score for new program: 0.4297254737221842\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Full train_val score for new program: 0.4297254737221842\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Individual valset scores for new program: [0.2727272727272727, 0.45454545454545453, 0.42424242424242425, 0.2727272727272727, 0.36363636363636365, 0.2727272727272727, 0.36363636363636365, 0.36363636363636365, 0.696969696969697, 0.5151515151515151, 0.36363636363636365, 0.36363636363636365, 0.2727272727272727, 0.36363636363636365, 0.5454545454545454, 0.45454545454545453, 0.2727272727272727, 0.42424242424242425, 0.2727272727272727, 0.5454545454545454, 0.45454545454545453, 0.3203463203463203, 0.5454545454545454, 0.7272727272727273, 0.36363636363636365, 0.5454545454545454, 0.45454545454545453, 0.32854864433811803, 0.42424242424242425, 0.18181818181818182, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.5151515151515151, 0.36363636363636365, 0.3333333333333333, 0.36363636363636365, 0.5454545454545454, 0.36363636363636365, 0.18181818181818182, 0.2727272727272727, 0.5454545454545454, 0.36363636363636365, 0.36363636363636365, 0.45454545454545453, 0.45454545454545453, 0.8181818181818182, 0.36363636363636365, 0.7272727272727273, 0.5, 0.6666666666666666, 0.36363636363636365, 0.36363636363636365, 0.36363636363636365, 0.45454545454545453, 0.2727272727272727, 0.3333333333333333, 0.42424242424242425, 0.2727272727272727, 0.6363636363636364, 0.4444444444444445, 0.5454545454545454, 0.7272727272727273, 0.42424242424242425]\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.5454545454545454, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 1.0, 0.7878787878787878, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Full valset pareto front score: 0.7326073232323232\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Updated valset pareto front programs: [{8, 9, 2}, {7}, {0}, {9}, {8}, {0, 3}, {0, 1, 2, 9}, {0}, {3}, {2}, {8, 1, 7}, {9, 6}, {0, 3}, {0}, {9, 6}, {1, 3}, {1, 10, 9, 7}, {8, 9}, {4}, {1, 2, 9}, {9}, {4}, {9}, {2}, {9}, {3}, {7}, {2}, {8, 9, 3}, {9}, {1}, {8}, {2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {1, 2, 3, 9}, {4}, {2}, {3}, {8}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 9, 7}, {8, 3}, {8}, {11}, {9}, {0, 2, 11}, {7}, {8}, {0, 9, 10, 7}, {0}, {9}, {9, 7}, {8}, {8, 9, 2, 6}, {0}, {9, 7}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Best score on valset: 0.637365845959596\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Linear pareto front program index: 9\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: New program candidate index: 11\n",
      "GEPA Optimization:  57%|█████▋    | 852/1483 [31:05<22:57,  2.18s/rollouts]2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 15: No merge candidates found\n",
      "2025/09/30 09:49:20 INFO dspy.teleprompt.gepa.gepa: Iteration 15: Selected program 8 score: 0.5833198051948052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.52 / 3 (50.5%): 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:49:27 INFO dspy.evaluate.evaluate: Average Metric: 1.5151515151515151 / 3 (50.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:51:23 INFO dspy.teleprompt.gepa.gepa: Iteration 15: Proposed new text for predict: text\n",
      "**Task Overview**\n",
      "\n",
      "You will receive a single JSON object with two top‑level keys:\n",
      "\n",
      "* `pdfinfo` – a dictionary of PDF metadata (e.g. `creationDate`, `title`, `author`).\n",
      "* `pages` – an ordered list of pages. Each page is a dictionary with:\n",
      "  * `page` – the page number (integer, starting at 1)\n",
      "  * `text` – the plain‑text content of that page (line breaks are preserved).\n",
      "\n",
      "From this information you must produce **exactly** the 13 lines shown in the “Output Format” section below.  \n",
      "No markdown, no extra whitespace, no additional lines.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Output Format (exact order, one line each)\n",
      "\n",
      "```\n",
      "reasoning\n",
      "language\n",
      "title\n",
      "alt_title\n",
      "creator\n",
      "year\n",
      "publisher\n",
      "doi\n",
      "e_isbn\n",
      "p_isbn\n",
      "e_issn\n",
      "p_issn\n",
      "type_coar\n",
      "```\n",
      "\n",
      "* `reasoning` – 1‑2 short sentences explaining how you obtained the values.  \n",
      "* All other fields must contain **only** the final value (or the prescribed empty marker).  \n",
      "\n",
      "Empty‑value markers  \n",
      "* Scalars (`language`, `title`, `year`, `doi`, `e_issn`, `p_issn`) → `None`  \n",
      "* Lists (`alt_title`, `creator`, `publisher`, `e_isbn`, `p_isbn`) → `[]`\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Field Extraction Rules\n",
      "\n",
      "#### 2.1 language  (scalar)\n",
      "* Determine the dominant language of the **body text** (ignore metadata, DOI URLs, etc.).\n",
      "* Use ISO‑639‑1 two‑letter lower‑case codes:\n",
      "  * Finnish → `fi`\n",
      "  * Swedish → `sv`\n",
      "  * English → `en`\n",
      "  * Northern Sámi → `se`\n",
      "  * (add others if clearly dominant)\n",
      "* Simple heuristic: count occurrences of language‑specific characters/words.\n",
      "  * Finnish/Swedish: `ä`, `ö`, `å` (Swedish only), `å`, `åäö`.\n",
      "  * Sámi: `č`, `đ`, `ŋ`, `š`, `ž`, `á`, `é`, `í`, `ó`, `ú`, `ý`.\n",
      "  * English: none of the above and many common English stop‑words (`the`, `and`, `of`, …).\n",
      "* If the count is tied, prefer the language indicated by the PDF title or by an explicit language label in the text.\n",
      "\n",
      "#### 2.2 title  (scalar)\n",
      "* The **full main title** as it appears on the title page (usually the largest heading on page 1).  \n",
      "* Include **any subtitle** that follows a colon (`:`) **exactly as printed** (including spaces before/after the colon).  \n",
      "* The title may be split over several lines; concatenate them with a single space, preserving original case, punctuation and diacritics.  \n",
      "* Do **not** trim trailing spaces; keep the title exactly as it would appear when read line‑by‑line.\n",
      "\n",
      "#### 2.3 alt_title  (list of strings)\n",
      "* Every **alternative title** that is explicitly given, e.g.:\n",
      "  * A translation in another language placed in parentheses or on a separate line.\n",
      "  * A subtitle that is *not* part of the main title because it appears only in the abstract or a “title in another language” field.\n",
      "* Preserve the exact wording (including punctuation).  \n",
      "* Order as they appear in the document.  \n",
      "* If none are found → `[]`.\n",
      "\n",
      "#### 2.4 creator  (list of strings)\n",
      "* **Only the author(s)** of the work. Do **not** include supervisors, advisors, reviewers, editors, etc.\n",
      "* Identify the author line(s) – typical cues: a line containing only a name, or a line prefixed by “Author(s):”, “Authors:”, or appearing directly under the title.\n",
      "* **Name formatting:**  \n",
      "  * Assume the **last token** of the name string is the surname; everything before it is the given name(s).  \n",
      "  * Output as `\"Surname, GivenNames\"` (single space after the comma).  \n",
      "  * Preserve diacritics and original capitalisation of each name part.  \n",
      "  * If the name already contains a comma (e.g. “Last, First”), keep it as‑is.\n",
      "* Preserve the order of authors as they appear.  \n",
      "* If no author can be identified → `[]`.\n",
      "\n",
      "#### 2.5 year  (scalar integer)\n",
      "* Preferred source: the first four digits after `D:` in `pdfinfo[\"creationDate\"]` (e.g. `D:20210608…` → `2021`).  \n",
      "* If that year is clearly a *creation* timestamp that differs from the publication year (e.g., the PDF was created later), look for an explicit year in the document body:\n",
      "  * Typical locations: near the title, on a line like “2021”, “Spring 2020”, “May 2021”, “Kevät 2020”, etc.\n",
      "* Use the **publication year** (four‑digit integer).  \n",
      "* If no year can be found → `None`.\n",
      "\n",
      "#### 2.6 publisher  (list of strings)\n",
      "* The entity that issued the document.\n",
      "  * For theses/dissertations: the university, polytechnic, or research institute (exact wording, no translation).\n",
      "  * For journal articles: the journal title (if clearly indicated as a journal name).\n",
      "  * For books: the publishing house.\n",
      "* If multiple entities are printed (e.g., “University of Helsinki, Faculty of Science”), split them into separate list items **exactly as they appear**, preserving commas inside an entity only when they are part of the official name.\n",
      "* Order as they appear in the document.  \n",
      "* If none can be located → `[]`.\n",
      "\n",
      "#### 2.7 doi  (scalar)\n",
      "* Look for a DOI pattern:\n",
      "  * A plain DOI beginning with `10.` (e.g., `10.1234/abcd.efg`).\n",
      "  * A URL such as `https://doi.org/10.1234/abcd` or `doi:10.1234/abcd`.\n",
      "* Strip any leading protocol (`http://`, `https://`), domain (`doi.org/`), or prefix (`doi:`).  \n",
      "* Output only the bare DOI string.  \n",
      "* If none found → `None`.\n",
      "\n",
      "#### 2.8 e_isbn / p_isbn  (lists of strings)\n",
      "* Search the whole document for lines containing `ISBN`.  \n",
      "* Extract the numeric ISBN (keep hyphens if present).  \n",
      "* Determine **format** from the surrounding qualifier:\n",
      "  * If the line contains the word **digital**, **PDF**, **electronic**, **online**, or ends with “(digital)”, treat it as **electronic** → add to `e_isbn`.\n",
      "  * If the line contains **printed**, **hardcover**, **paper**, or ends with “(printed)”, treat it as **printed** → add to `p_isbn`.\n",
      "  * If no qualifier is present, assume **printed** and add to `p_isbn`.\n",
      "* Preserve the order of appearance.  \n",
      "* If none → `[]`.\n",
      "\n",
      "#### 2.9 e_issn / p_issn  (scalar)\n",
      "* Same logic as ISBN but for `ISSN`.  \n",
      "* Qualifiers “electronic”, “online”, “e‑ISSN” → `e_issn`; “print”, “p‑ISSN” → `p_issn`.  \n",
      "* If qualifier absent, leave both as `None`.  \n",
      "* Output the ISSN exactly as printed (e.g., `1234-5678`).\n",
      "\n",
      "#### 2.10 type_coar  (scalar)\n",
      "* Choose the most specific COAR resource type in **lower‑case** from the controlled list. Common cues:\n",
      "  * **Thesis / Opinnäytetyö** → look for level words:\n",
      "    * “Bachelor”, “bachelor’s”, “kandidaatti”, “bachelor‑level” → `bachelor thesis`\n",
      "    * “Master”, “master’s”, “maisteri”, “master‑level” → `master thesis`\n",
      "    * “Doctoral”, “PhD”, “doktori”, “doctoral” → `doctoral thesis`\n",
      "  * **Journal article** → presence of an ISSN, article‑style headings, page ranges, and a journal name; no explicit “thesis” wording.\n",
      "  * **Book** → presence of a publisher and ISBN(s) but **no** thesis‑level wording, and usually a cover page with a book title.\n",
      "  * **Report** → words like “Report”, “Technical Report”, “Research Report”.\n",
      "  * **Conference paper** → words like “Proceedings”, “Conference”, “Paper”.\n",
      "  * **Dataset**, **Software**, **Book review** → explicit mentions.\n",
      "* If multiple cues conflict, prefer the most specific (e.g., “Doctoral dissertation” over generic “Report”).\n",
      "* If no type can be deduced → `None`.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. General Extraction Strategy (to be followed for every document)\n",
      "\n",
      "1. **Pre‑process** the `pages` list:\n",
      "   * Concatenate all `text` fields preserving line breaks.\n",
      "   * Keep a mapping of line number → page number for later reference (useful for ordering).\n",
      "\n",
      "2. **Detect language** first (section 2.1) – this influences later decisions (e.g., title language).\n",
      "\n",
      "3. **Locate the title page** (usually page 1, sometimes page 2):\n",
      "   * Scan the first few pages for the largest heading (all‑caps or surrounded by `#`/`##`/`###` markers) and for lines directly under it that look like author names or university names.\n",
      "   * Extract the full title according to section 2.2.\n",
      "\n",
      "4. **Extract authors**:\n",
      "   * After the title, the next non‑empty line(s) that consist mainly of personal names are treated as the author line.\n",
      "   * Apply the name‑splitting rule (section 2.4).  \n",
      "   * Discard any line that contains supervisor/advisor keywords (`Supervisor`, `Työn ohjaaja`, `Advisor`, `Mentor`, `Reviewed by`, `Reviewer`, `Editor`).\n",
      "\n",
      "5. **Extract year**:\n",
      "   * First try `pdfinfo[\"creationDate\"]`.  \n",
      "   * Then search for a 4‑digit year in the vicinity of the title or in a line that looks like a date (e.g., “Spring 2020”).  \n",
      "   * Choose the publication year, not the PDF creation year, if they differ.\n",
      "\n",
      "6. **Extract publisher**:\n",
      "   * Look for university/institute names (common Finnish/Swedish patterns: “…ammattikorkeakoulu”, “…universitetet”, “…University”, “…Universität”, “…Universität”, “…Universidade”).  \n",
      "   * For journal articles, the journal name often appears near the ISSN line or in a header/footer.\n",
      "\n",
      "7. **Extract identifiers (DOI, ISBN, ISSN)**:\n",
      "   * Use regular expressions:\n",
      "     * DOI: `10\\.\\d{4,9}/\\S+`\n",
      "     * ISBN: `ISBN\\s*[:]?[\\s]*([\\d-]+)`\n",
      "     * ISSN: `ISSN\\s*[:]?[\\s]*([\\d]{4}-[\\d]{4})`\n",
      "   * After extraction, apply the qualifier rules (section 2.8/2.9).\n",
      "\n",
      "8. **Determine COAR type** using the keyword hierarchy in section 2.10.\n",
      "\n",
      "9. **Compose the reasoning line** summarising the main steps you used (e.g., “Title taken from the first‑page heading, authors parsed from the line below the title, year from the PDF creation date, publisher identified as the university name, DOI absent, ISBNs classified by their printed/digital qualifiers, document is a doctoral thesis.”).\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Edge Cases & Common Pitfalls\n",
      "\n",
      "| Pitfall | How to avoid |\n",
      "|---------|--------------|\n",
      "| **Wrong language** – e.g., Finnish text with English abstract causing confusion | Count language‑specific characters across the whole body; give priority to the majority. |\n",
      "| **Title missing subtitle** – subtitle placed on a separate line after a colon | When a line ends with a colon, concatenate the next non‑empty line as part of the title. |\n",
      "| **Multiple ISBNs without qualifiers** – treat as printed unless the surrounding page header contains “PDF” | Look at the line itself; if it ends with “(PDF)” or “(digital)”, classify as electronic. |\n",
      "| **Author line includes supervisor** – “John Doe (Supervisor: Prof. …)” | Discard any line that contains the supervisor keywords; only keep the pure name line. |\n",
      "| **Publisher appears as part of a longer address** – “University of Helsinki, Faculty of Science, Helsinki, Finland” | Split at the first comma; keep the institution name only (`University of Helsinki`). |\n",
      "| **COAR type ambiguous** – both “Report” and “Doctoral dissertation” appear | Follow hierarchy: thesis levels > report > article > book. |\n",
      "| **Alternative title inside parentheses** – “Main Title (English: Alternative Title)” | Capture the text after the language label as an `alt_title` entry, preserving the exact wording. |\n",
      "| **Missing DOI but a URL that is not a DOI** – e.g., a link to a PDF | Only accept URLs that contain `doi.org` or match the DOI regex. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Required Output Example (illustrative)\n",
      "\n",
      "```\n",
      "reasoning\n",
      "Extracted the main heading on page 1 as title, author line directly below, year from creationDate, publisher from university line, no DOI, ISBNs classified by qualifiers, identified as a doctoral thesis.\n",
      "language\n",
      "en\n",
      "title\n",
      "Life Cycle Assessment of Plastic Waste, its Treatment, and Application of the Upcycled Product : A Comprehensive Circular Approach\n",
      "alt_title\n",
      "[]\n",
      "creator\n",
      "['Ali, Ashiq Ahamed Hameed Sultan']\n",
      "year\n",
      "2021\n",
      "publisher\n",
      "['Åbo Akademi University']\n",
      "doi\n",
      "None\n",
      "e_isbn\n",
      "['978-952-12-4080-5']\n",
      "p_isbn\n",
      "['978-952-12-4079-9']\n",
      "e_issn\n",
      "None\n",
      "p_issn\n",
      "None\n",
      "type_coar\n",
      "doctoral thesis\n",
      "2025/09/30 09:51:30 INFO dspy.evaluate.evaluate: Average Metric: 1.2727272727272727 / 3 (42.4%)\n",
      "2025/09/30 09:51:30 INFO dspy.teleprompt.gepa.gepa: Iteration 15: New subsample score is not better, skipping\n",
      "GEPA Optimization:  58%|█████▊    | 858/1483 [33:15<30:52,  2.96s/rollouts]2025/09/30 09:51:30 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Selected program 8 score: 0.5833198051948052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.64 / 3 (54.5%): 100%|██████████| 3/3 [00:06<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:51:37 INFO dspy.evaluate.evaluate: Average Metric: 1.6363636363636362 / 3 (54.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:53:15 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Proposed new text for predict: markdown\n",
      "## Task Overview\n",
      "You will be given a JSON object with two keys:\n",
      "\n",
      "1. **pdfinfo** – metadata extracted from the PDF file (e.g., `title`, `author`, `creationDate`).\n",
      "2. **pages** – an ordered list of pages, each containing a `\"page\"` number and the plain‑text `\"text\"` that appears on that page.\n",
      "\n",
      "From this information you must produce **structured bibliographic metadata** in a strict, line‑by‑line format (see the “Output Format” section).  \n",
      "All fields must follow the exact conventions described below; any deviation will be marked as incorrect.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Output Format\n",
      "Your answer must contain **exactly** the following 13 lines, in this order, each on its own line (no extra whitespace, no markdown, no additional sections):\n",
      "\n",
      "```\n",
      "reasoning\n",
      "language\n",
      "title\n",
      "alt_title\n",
      "creator\n",
      "year\n",
      "publisher\n",
      "doi\n",
      "e_isbn\n",
      "p_isbn\n",
      "e_issn\n",
      "p_issn\n",
      "type_coar\n",
      "2025/09/30 09:53:22 INFO dspy.evaluate.evaluate: Average Metric: 1.818181818181818 / 3 (60.6%)\n",
      "2025/09/30 09:53:55 INFO dspy.evaluate.evaluate: Average Metric: 38.17171717171715 / 64 (59.6%)\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full valset score for new program: 0.5964330808080808\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full train_val score for new program: 0.5964330808080808\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Individual valset scores for new program: [0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.36363636363636365, 0.7272727272727273, 0.36363636363636365, 0.5454545454545454, 0.8181818181818182, 0.6363636363636364, 0.5454545454545454, 0.8181818181818182, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.36363636363636365, 0.8181818181818182, 0.45454545454545453, 0.5151515151515151, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.6060606060606061, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.42424242424242425, 0.5151515151515151, 0.6363636363636364, 0.2727272727272727, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.36363636363636365, 0.45454545454545453, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.6565656565656566, 0.6363636363636364, 0.6363636363636364, 0.36363636363636365, 0.6363636363636364, 0.6363636363636364]\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full valset pareto front score: 0.7349747474747474\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Updated valset pareto front programs: [{8, 9, 2}, {12, 7}, {0}, {9}, {8}, {0, 3}, {0, 1, 2, 9}, {0, 12}, {3}, {2}, {8, 1, 12, 7}, {9, 6}, {0, 3}, {0, 12}, {9, 6}, {1, 3, 12}, {1, 10, 9, 7}, {8, 9}, {4}, {1, 2, 9}, {9}, {4}, {9}, {2}, {9}, {3}, {12, 7}, {2}, {8, 9, 3}, {12}, {1}, {8}, {2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {12}, {4}, {2}, {3}, {12}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 9, 7}, {8, 3}, {8}, {11}, {9}, {0, 2, 11}, {7}, {8}, {0, 7, 9, 10, 12}, {0, 12}, {9}, {9, 7}, {8}, {8, 9, 2, 6}, {0}, {9, 7}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best score on valset: 0.637365845959596\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Linear pareto front program index: 9\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 16: New program candidate index: 12\n",
      "GEPA Optimization:  63%|██████▎   | 928/1483 [35:40<24:04,  2.60s/rollouts]2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 17: No merge candidates found\n",
      "2025/09/30 09:53:55 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Selected program 9 score: 0.637365845959596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.55 / 2 (77.3%):  67%|██████▋   | 2/3 [00:08<00:04,  4.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:54:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.09 / 3 (69.7%): 100%|██████████| 3/3 [00:11<00:00,  3.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:54:07 INFO dspy.evaluate.evaluate: Average Metric: 2.090909090909091 / 3 (69.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:55:44 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Proposed new text for predict: markdown\n",
      "# 📋 Task – Structured Bibliographic Metadata Extraction (Fully Specified)\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF document.  \n",
      "The object has exactly two top‑level keys:\n",
      "\n",
      "| Key      | Description |\n",
      "|----------|-------------|\n",
      "| `pdfinfo`| Metadata extracted directly from the PDF file (e.g. `title`, `author`, `creationDate`, `modDate`). |\n",
      "| `pages`  | A list of page objects. Each page object contains `page` (the page number) and `text` (the OCR‑extracted plain‑text of that page). |\n",
      "\n",
      "Your job is to produce **one JSON object** that follows the schema below.  \n",
      "If a field cannot be determined, use the exact empty value indicated (`null` for scalars, `[]` for lists).  \n",
      "All string values must be plain ASCII – normalise quotes to `\"` (or the apostrophe `’`), collapse multiple spaces to a single space, and trim leading/trailing whitespace.\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema (order shown for readability)\n",
      "\n",
      "| Field | Type | Required format / rules |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | ISO‑639‑1 code. Concatenate **all** `pages[i].text` in order, take the first 200 characters, and if any of `ä ö Ä Ö å Å` appear → `\"fi\"` else `\"en\"`. |\n",
      "| `title` | string | Main title of the work. See **Title Extraction** below. |\n",
      "| `alt_title` | list of strings | Alternative titles (translations, quoted titles, subtitles on a separate line after a colon). No duplicate of `title`. |\n",
      "| `creator` | list of strings | Authors in “Surname, Given‑Name” order. See **Creator Extraction** below. |\n",
      "| `year` | integer or null | Publication year (see **Year Extraction**). |\n",
      "| `publisher` | list of strings | Institution responsible for the work (see **Publisher Detection**). |\n",
      "| `doi` | string or null | DOI without URL prefix or trailing punctuation, lower‑cased. |\n",
      "| `e_isbn` | list of strings | Electronic ISBN‑13 numbers **without** hyphens/spaces. |\n",
      "| `p_isbn` | list of strings | Print ISBN‑13 numbers **without** hyphens/spaces. |\n",
      "| `e_issn` | string or null | Electronic ISSN (keep hyphen if present). |\n",
      "| `p_issn` | string or null | Print ISSN (keep hyphen if present). |\n",
      "| `type_coar` | string | COAR‑compatible resource type (lower‑case). See **Resource‑type Detection**. |\n",
      "\n",
      "---\n",
      "\n",
      "## General Normalisation Rules (apply everywhere)\n",
      "\n",
      "1. **Whitespace** – collapse any run of spaces, tabs, newlines into a single space; trim leading/trailing spaces.  \n",
      "2. **Quotes** – replace any fancy quotation marks (`“ ” ‘ ’ „ …`) with plain ASCII `\"` (or the apostrophe `’`).  \n",
      "3. **Hyphens in identifiers** – remove all hyphens (`-`) and spaces **only** from ISBNs. Keep hyphens in ISSNs exactly as they appear.  \n",
      "4. **Case** – store DOI in lower‑case; other identifiers are numeric/alphabetic only, so case does not matter.  \n",
      "\n",
      "---\n",
      "\n",
      "## Extraction Procedures\n",
      "\n",
      "### 1. Language\n",
      "* Concatenate all page texts in order.\n",
      "* Look at the first 200 characters.\n",
      "* If any of the characters **ä ö Ä Ö å Å** are present → `\"fi\"`, otherwise `\"en\"`.\n",
      "\n",
      "### 2. Title Extraction\n",
      "1. **Use PDF metadata if present**  \n",
      "   *If `pdfinfo.title` exists → clean it (apply normalisation) and use it as the title.*  \n",
      "\n",
      "2. **Otherwise work on page 1 (`pages[0].text`)**  \n",
      "   * Split the text into lines, keep original order.  \n",
      "   * Remove empty lines.  \n",
      "   * Remove lines that start with a Markdown heading marker (`#`, `##`, `###`, …).  \n",
      "   * Remove lines that look like an author line (see *Creator Extraction* – any line that matches the author‑name regexes).  \n",
      "\n",
      "3. **Explicit title markers** – before the generic candidate logic, scan the remaining lines for any of the case‑insensitive markers  \n",
      "   `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:`.  \n",
      "   *If a line contains one of these markers, take the text **after the colon** (trim) as the final title, **overriding** everything else.*  \n",
      "\n",
      "4. **Candidate‑line logic** (used only if no explicit marker was found)  \n",
      "   * The **first** remaining line whose length (after trimming) is **≥ 6 characters** becomes the *candidate* title.  \n",
      "   * **Title‑line extensions** – while the candidate line **ends with a colon** **or** the **next line** (if it exists) is non‑empty, starts with a capital letter, and is **not** an author line, concatenate that next line to the candidate with a single space. Repeat the check after each concatenation.  \n",
      "\n",
      "5. **Final cleaning** – collapse spaces, normalise quotes, trim. Do **not** split on colons; keep any subtitle that is part of the concatenated string.\n",
      "\n",
      "### 3. Alternative‑title Extraction\n",
      "* Search the **whole document** (all pages) for:\n",
      "  * Text inside any of the quotation pairs `“ … ”`, `” … “`, `\" … \"`. Each distinct quoted string becomes an element (after normalisation).  \n",
      "  * Subtitle lines that appear **on a separate line** **after a colon** (e.g. `Main Title:` on line N and `Subtitle` on line N+1). The subtitle alone is added as an alternative title *only if* it is **not** already part of the main title.  \n",
      "* Remove duplicates and the main title itself.\n",
      "\n",
      "### 4. Creator Extraction (Authors)\n",
      "1. **If `pdfinfo.author` exists**  \n",
      "   * Split on commas, semicolons, the word “and”, or line‑breaks.  \n",
      "   * Trim each fragment and collapse spaces.  \n",
      "\n",
      "2. **Otherwise scan page 1**  \n",
      "   * Consider the **first five non‑empty lines** of `pages[0].text`.  \n",
      "   * Use the two regexes (case‑sensitive) to find name patterns:  \n",
      "     * `\\b[A-Z][a-z]+ [A-Z][a-z]+(?: [A-Z][a-z]+)*\\b`  (First Last)  \n",
      "     * `\\b[A-Z][a-z]+, *[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (Last, First)  \n",
      "   * **Exclude** any line that contains any of the following keywords (case‑insensitive): `supervisor`, `advisor`, `editor`, `reviewer`, `opponent`, `committee`, `faculty`, `department`.  \n",
      "\n",
      "3. **Re‑order names**  \n",
      "   * For each name fragment:  \n",
      "     * If it matches **First Last** → reorder to `Last, First`.  \n",
      "     * If it already matches **Last, First** → keep as‑is.  \n",
      "   * Preserve the order of first appearance.  \n",
      "\n",
      "4. **Deduplicate** – remove exact duplicates, keep first occurrence.\n",
      "\n",
      "### 5. Year Extraction\n",
      "1. **From PDF dates** – if `pdfinfo.creationDate` or `pdfinfo.modDate` exists, each is of the form `D:YYYY…`. Extract the first four digits (`YYYY`). If both exist, prefer `creationDate`.  \n",
      "2. **Fallback** – search the whole document for the **first** four‑digit number between 1900‑2099 that appears in a *publication‑information context*. Acceptable preceding tokens (case‑insensitive): `Year:`, `©`, `© `, `© `, `©`, `© `, `© `, `©`, `©`, `©`. Also accept a pattern like `2021.` where the year is followed by a period and not inside a citation parentheses.  \n",
      "3. If no year found → `null`.\n",
      "\n",
      "### 6. Publisher Detection\n",
      "1. **First determine `type_coar`** (see section 9).  \n",
      "2. **If the document is a thesis** (`type_coar` = `doctoral thesis` or `master thesis`):  \n",
      "   * Scan the whole text for the **first occurrence** of any of the institution‑keyword list:  \n",
      "     `University`, `Yliopisto`, `Universität`, `Akademi`, `Institute`, `College`, `School`, `Faculty`.  \n",
      "   * Capture the **full phrase** that contains the keyword (e.g. “University of Vaasa”, “Åbo Akademi University”). Return it as a **single‑element list**.  \n",
      "3. **If the document is a research report** (`type_coar` = `research report`):  \n",
      "   * Look for an organisation name near the title block (within the first 3 pages) or in a header/footer line that contains any of the institution keywords. Return each distinct organisation once, preserving order.  \n",
      "4. **If the document is a journal article** (`type_coar` = `journal article`): return an empty list `[]`.  \n",
      "\n",
      "### 7. DOI Extraction\n",
      "* Detect the pattern `10\\.\\d{4,9}/\\S+` (case‑insensitive).  \n",
      "* If the match is preceded by `http://`, `https://`, or `doi.org/`, strip that prefix.  \n",
      "* Remove any trailing punctuation characters `.,;:`.  \n",
      "* Lower‑case the resulting DOI.  \n",
      "* If no DOI found → `null`.\n",
      "\n",
      "### 8. ISBN / ISSN Extraction & Classification\n",
      "**Common regexes** (apply globally on the whole concatenated text):\n",
      "\n",
      "* ISBN‑13: `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`  \n",
      "* ISSN: `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`\n",
      "\n",
      "**For each match**:\n",
      "\n",
      "1. Extract up to **30 characters before** and **30 characters after** the match (if the document boundaries are reached, use what is available).  \n",
      "2. **Electronic cues** (case‑insensitive): `digital`, `electronic`, `e‑ISBN`, `(digital)`, `pdf`, `PDF`, `sid.`, `online`.  \n",
      "3. **Print cues** (case‑insensitive): `print`, `paper`, `hardcover`, `(print)`, `Painettu`, `Print`.  \n",
      "\n",
      "**Classification rules**:\n",
      "\n",
      "* If an electronic cue appears in the window → the ISBN/ISSN belongs to the **electronic** list (`e_isbn` or `e_issn`).  \n",
      "* Else if a print cue appears → it belongs to the **print** list (`p_isbn` or `p_issn`).  \n",
      "* If **both** cues appear, **electronic wins** (store only in the electronic list).  \n",
      "* For ISBNs, **remove all hyphens and spaces** before storing.  \n",
      "* For ISSNs, **keep the hyphen (if any)** exactly as it appears.  \n",
      "\n",
      "**Deduplication** – store each identifier only once, preferring the first classification decided by the rules.\n",
      "\n",
      "### 9. Resource‑type Detection (`type_coar`)\n",
      "Scan the **entire document** (case‑insensitive) and apply the **first** matching rule in this order:\n",
      "\n",
      "1. **doctoral thesis** – contains any of: `doctoral thesis`, `dissertation`, `PhD`, `doctoral`, `väitöskirja`, `väitöskirjan`.  \n",
      "2. **master thesis** – contains any of: `master’s thesis`, `master thesis`, `maisteri`, `maisterintutkielma`.  \n",
      "3. **journal article** – contains typical journal citation elements **or** the word “article” together with a DOI or ISSN, **or** a pattern like `Vol. X, No. Y, pp. Z‑W`.  \n",
      "4. **conference proceeding** – contains `conference`, `proceedings`, `paper presented at`.  \n",
      "5. **research report** – contains `report`, `raportti`, `tutkimusraportti`, `research report`.  \n",
      "6. **research** – any other research‑type document.\n",
      "\n",
      "Return the exact lower‑case string (e.g. `doctoral thesis`).\n",
      "\n",
      "---\n",
      "\n",
      "## Assembly of Output\n",
      "\n",
      "Create a JSON object with the fields listed in the schema, applying the rules above.  \n",
      "Use `null` for missing scalar values, `[]` for missing list values.  \n",
      "All string values must be ASCII‑only, whitespace‑normalised, and quotes normalised.\n",
      "\n",
      "**Example (format only, not real data):**\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"My Main Title : A Subtitle\",\n",
      "  \"alt_title\": [\"My Main Title\", \"Another Translation\"],\n",
      "  \"creator\": [\"Doe, John\", \"Smith, Jane\"],\n",
      "  \"year\": 2021,\n",
      "  \"publisher\": [\"University of Example\"],\n",
      "  \"doi\": \"10.1234/abcd.efg\",\n",
      "  \"e_isbn\": [\"9781234567890\"],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": \"1234-5678\",\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"doctoral thesis\"\n",
      "}\n",
      "2025/09/30 09:55:52 INFO dspy.evaluate.evaluate: Average Metric: 1.7532467532467533 / 3 (58.4%)\n",
      "2025/09/30 09:55:52 INFO dspy.teleprompt.gepa.gepa: Iteration 17: New subsample score is not better, skipping\n",
      "GEPA Optimization:  63%|██████▎   | 934/1483 [37:37<31:02,  3.39s/rollouts]2025/09/30 09:55:52 INFO dspy.teleprompt.gepa.gepa: Iteration 18: Selected program 7 score: 0.6111111111111112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:08<00:00,  2.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:56:01 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:57:59 INFO dspy.teleprompt.gepa.gepa: Iteration 18: Proposed new text for predict: markdown\n",
      "# Bibliographic‑Metadata Extraction – Revised Assistant Instructions\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF file.  \n",
      "Its top‑level keys are:\n",
      "\n",
      "* `pdfinfo` – metadata extracted from the PDF (may contain `title`, `author`,\n",
      "  `creationDate`, `modDate`, …).  \n",
      "* `pages` – an ordered list of page objects, each with:\n",
      "  * `page` – page number (integer)  \n",
      "  * `text` – the OCR‑extracted plain‑text of that page (UTF‑8 string)\n",
      "\n",
      "Your job is to produce **exactly one JSON object** that follows the schema below.\n",
      "If a value cannot be determined, use the exact empty value specified\n",
      "(`null` for a missing string, `[]` for an empty list).  The fields must appear in the\n",
      "order shown in the *Output JSON Schema* table.\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema (order matters)\n",
      "\n",
      "| Field | Type | Required format / notes |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | `\"fi\"` if the document is Finnish, otherwise `\"en\"`. Detect **only the first 200 characters** of the concatenated page texts (preserve page order). If any of the characters `ä ö Ä Ö å Å` appear **or** any Finnish‑specific word appears (`opinnäytetyö`, `ammattikorkeakoulu`, `tutkielma`, `väitöskirja`, `yliopisto`, `koulutus`, etc.) → `fi`; otherwise `en`. |\n",
      "| `title` | string | Main title of the work, **cleaned** (see *Cleaning Rules*). Preference order: 1. `pdfinfo.title` if it exists and is non‑empty. 2. The first “prominent heading” on **page 1** (see *Title‑Finding Rules*). |\n",
      "| `alt_title` | list of strings | Any alternative titles. • If the selected `title` contains a colon (`:`), treat the part **after** the colon as an alternative title (trimmed, cleaned). • Any text that appears inside double quotes (`\"`) or single quotes (`'` or `’`) anywhere in the document is also an alternative title (each distinct occurrence becomes a separate element). Return `[]` if none. |\n",
      "| `creator` | list of strings | Author(s) in **“Surname, Given‑Name [Middle‑Name]”** order. Use `pdfinfo.author` if present; otherwise locate an author line in the text (see *Author‑Finding Rules*). Split multiple authors on commas, semicolons, the word “and”, ampersand `&`, or line breaks. For each token, remove trailing footnote symbols (`*`, `†`, etc.). If a token matches the pattern “First Middle Last” (or “First Last”), reorder to “Last, First Middle”. Preserve all given‑name parts (including middle names). Return `[]` if no author can be identified. |\n",
      "| `year` | integer | Publication year. Extract the **first** four‑digit year in the range 1900‑2099 from (in order of priority): `pdfinfo.creationDate`, `pdfinfo.modDate`, then the whole concatenated text. |\n",
      "| `publisher` | list of strings | Institution(s) that awarded the work **only when** `type_coar` is one of `doctoral thesis`, `master thesis`, or `bachelor thesis`. Search the whole text for university/faculty names. Typical keywords (case‑insensitive): `University`, `Universität`, `Università`, `Université`, `Universiteit`, `Universitet`, `Università`, `Akademi`, `Institute`, `College`, `School`, `Faculty`, `ammattikorkeakoulu`, `yliopisto`, `universitet`, `université`, `korkeakoulu`. Return the **first distinct** institution name(s) found, after applying *Cleaning Rules*. If `type_coar` is any other value, return `[]`. |\n",
      "| `doi` | string or null | DOI **without** any URL prefix. Detect with case‑insensitive regex `10\\.\\d{4,9}/\\S+`. Strip any trailing punctuation characters `.,;` and surrounding whitespace. If the DOI appears as a full URL (`https://doi.org/...`), keep only the part after the last slash. Return `null` if none found. |\n",
      "| `e_isbn` | list of strings | Electronic ISBN‑13 numbers. Find all 13‑digit ISBNs (with optional hyphens/spaces). An ISBN belongs to this list **only if** within **20 characters before or after** the match (case‑insensitive) one of the electronic cue words: `digital`, `electronic`, `e‑isbn`, `e‑isbn:`, `(digital)`, `(online)`, `pdf`, `electronic version`. Store the number **without** hyphens or spaces. |\n",
      "| `p_isbn` | list of strings | Print ISBN‑13 numbers. Same detection as `e_isbn` but the surrounding cue must be one of the print cue words: `print`, `paper`, `hardcover`, `hard cover`, `pp.`, `(print)`, `physical`, `hardcopy`. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Use the same cue logic as for `e_isbn`. Return the number **without** hyphen, or `null` if none. |\n",
      "| `p_issn` | string or null | Print ISSN. Same cue logic as for `p_isbn`. Return the number **without** hyphen, or `null` if none. |\n",
      "| `type_coar` | string | COAR‑compatible resource type, **lower‑case**. Detect **in the order listed**; the first match wins: 1. **doctoral thesis** – contains any of `doctoral thesis`, `dissertation`, `PhD`, `doctoral`, `väitöskirja`. 2. **master thesis** – contains any of `master’s thesis`, `master thesis`, `maisteri`, `maisterintutkielma`. 3. **bachelor thesis** – contains any of `bachelor thesis`, `bachelor’s thesis`, `bachelor`, `opinnäytetyö`. 4. **journal article** – contains a journal‑style citation (journal name + volume/issue/pages) **or** the word “article” together with typical citation fields (year, volume, pages). 5. **conference proceeding** – contains `conference`, `proceedings`, `paper presented at`, `proceedings of`. 6. **book part** – contains the pattern `In ` followed by an editor name (e.g. “In H. Partti & L. Coutts (Eds.)”) **or** a page range indicated by “pp.” or “pages”. 7. **research** – fallback for any other research report or article. |\n",
      "| `type_coar` must be exactly one of the strings above (lower‑case). |\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning Rules (apply to **every** string you output)\n",
      "\n",
      "1. **Trim** leading and trailing whitespace.  \n",
      "2. **Collapse** any sequence of whitespace characters (space, tab, newline) to a single space.  \n",
      "3. **Normalize quotation marks**: replace curly double quotes (`“”`) and curly single quotes (`‘’`) with plain `\"` and `'` respectively; then replace any remaining single quotes that function as apostrophes with the typographic apostrophe `’`.  \n",
      "4. **Remove Markdown formatting**: strip leading heading markers (`#`, `##`, `###`), surrounding asterisks `*` or underscores `_`, and surrounding double‑asterisks `**`.  \n",
      "5. **Remove trailing asterisks** that are used as footnote symbols (e.g., `Magnusson*`).  \n",
      "6. **Leave diacritics** (ä, ö, å, é, ñ, etc.) untouched.  \n",
      "\n",
      "All cleaned strings must obey the rules above before being placed in the output JSON.\n",
      "\n",
      "---\n",
      "\n",
      "## Title‑Finding Rules (used when `pdfinfo.title` is absent)\n",
      "\n",
      "1. Concatenate the `text` of **page 1** and split it into lines, preserving order.  \n",
      "2. Discard empty lines and lines that become shorter than 6 characters after trimming.  \n",
      "3. Discard lines that start with Markdown heading markers (`#`, `##`, `###`).  \n",
      "4. The **first remaining line** is considered the title.  \n",
      "5. Apply the *Cleaning Rules* to this line.  \n",
      "6. If the cleaned line contains a colon (`:`), keep the whole line as `title` **and** store the part after the colon (trimmed) as an element of `alt_title`.\n",
      "\n",
      "---\n",
      "\n",
      "## Author‑Finding Rules (used when `pdfinfo.author` is absent)\n",
      "\n",
      "1. Examine pages 1‑3. For each line, consider it a candidate if **any** of the following is true:  \n",
      "   * It starts with “By ”, “Author:”, or “Authors:”.  \n",
      "   * It contains the word “and” or `&` **between** two capitalised words (e.g., “John Doe and Jane Smith”).  \n",
      "   * It consists mainly of capitalised words and appears within the first 25 % of the page’s lines.  \n",
      "2. From the first candidate line found, split the line on commas, semicolons, the word “and”, ampersand `&`, or line breaks.  \n",
      "3. For each token, remove trailing footnote symbols (`*`, `†`, etc.).  \n",
      "4. If a token matches the pattern “First Middle Last” (or “First Last”), reorder to “Last, First Middle”. Preserve all given‑name parts (including middle names).  \n",
      "5. Return the list of reordered names; if none can be parsed, return `[]`.\n",
      "\n",
      "*If `pdfinfo.author` is present*, apply the same splitting/reordering logic to that string (it may contain multiple authors separated by commas, semicolons, “and”, or `&`).\n",
      "\n",
      "---\n",
      "\n",
      "## Publisher Determination (after `type_coar`)\n",
      "\n",
      "*Only when `type_coar` is `doctoral thesis`, `master thesis`, or `bachelor thesis`*:\n",
      "\n",
      "1. Scan the whole concatenated text for institution names using the keyword list in the schema.  \n",
      "2. When a keyword is found, capture the **full phrase** that contains it (typically the surrounding words up to the next line break or punctuation).  \n",
      "3. Apply the *Cleaning Rules* to each captured phrase.  \n",
      "4. Return the **first distinct** institution name(s) in the order they appear.  \n",
      "5. If no institution is found, return `[]`.\n",
      "\n",
      "For all other `type_coar` values, return `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "## DOI Extraction Details\n",
      "\n",
      "* Regex (case‑insensitive): `(?i)10\\.\\d{4,9}/\\S+`  \n",
      "* After a match, strip any trailing punctuation characters `.,;` and surrounding whitespace.  \n",
      "* If the DOI appears as a full URL (`https://doi.org/...` or `http://dx.doi.org/...`), keep only the part after the last slash.  \n",
      "* Return `null` if no match is found.\n",
      "\n",
      "---\n",
      "\n",
      "## ISBN / ISSN Extraction Details\n",
      "\n",
      "### General pattern matching\n",
      "* **ISBN‑13** (13 digits, optional hyphens/spaces):  \n",
      "  `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`\n",
      "* **ISSN** (8 digits, optional hyphen):  \n",
      "  `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`\n",
      "\n",
      "### Cue‑based classification\n",
      "1. For each match, capture up to **20 characters** before and after the match (inclusive).  \n",
      "2. Convert this surrounding snippet to lower‑case.  \n",
      "3. If any electronic cue word (`digital`, `electronic`, `e‑isbn`, `e‑issn`, `pdf`, `online`, `(digital)`, `(online)`) appears → classify as **electronic**.  \n",
      "4. If any print cue word (`print`, `paper`, `hardcover`, `hard cover`, `pp.`, `physical`, `(print)`) appears → classify as **print**.  \n",
      "5. If both cue sets appear, add the number to **both** appropriate lists.  \n",
      "6. Strip all hyphens and spaces from the matched number before storing it.  \n",
      "7. Ensure each list (`e_isbn`, `p_isbn`) contains **unique** entries (no duplicates).  \n",
      "\n",
      "For ISSN, the same cue logic applies, but the result is a **single string** (or `null`) for each of `e_issn` and `p_issn`.\n",
      "\n",
      "---\n",
      "\n",
      "## General Processing Flow (to be followed for every request)\n",
      "\n",
      "1. **Parse** the input JSON safely.  \n",
      "2. **Normalize dates**: from `creationDate` / `modDate`, extract the first four‑digit year (e.g., `\"D:20220626201846+03'00'\"` → `2022`).  \n",
      "3. **Detect language** using only the first 200 characters of the concatenated page texts (apply the rule in the *Language* section).  \n",
      "4. **Extract title** (prefer `pdfinfo.title`; otherwise use Title‑Finding Rules).  \n",
      "5. **Derive `alt_title`** from the selected title and any quoted strings.  \n",
      "6. **Extract creators** (prefer `pdfinfo.author`; otherwise use Author‑Finding Rules).  \n",
      "7. **Determine year** from dates or, if absent, from the full text.  \n",
      "8. **Detect DOI**, ISBN‑13, ISSN using the detailed extraction rules.  \n",
      "9. **Identify resource type** (`type_coar`) using the ordered keyword list (including the newly added *book part* rule).  \n",
      "10. **Find publisher** based on the determined `type_coar`.  \n",
      "11. **Assemble** the output JSON with the exact field order shown in the schema, using `null` or `[]` where appropriate.  \n",
      "12. **Return** the JSON object **as the only output** (no extra commentary, no surrounding markdown, no explanations).\n",
      "\n",
      "---\n",
      "\n",
      "## Important Pitfalls (to avoid)\n",
      "\n",
      "* **Language** – limit scanning to the first 200 characters; later Finnish words must not flip the language to `fi`.  \n",
      "* **Title** – strip all Markdown heading markers and surrounding asterisks/underscores before cleaning.  \n",
      "* **Creator** – always output the full surname + all given names, reordered to “Surname, Given Names”. Do **not** truncate middle names.  \n",
      "* **Publisher** – only include it for theses/dissertations; for journal articles, conference papers, book parts, etc., return an empty list.  \n",
      "* **DOI** – return only the bare DOI string (`10.xxxx/...`), never a full URL.  \n",
      "* **ISBN/ISSN** – use the 20‑character cue window; hyphens and spaces must be removed from the stored number; classify correctly as electronic vs. print.  \n",
      "* **type_coar** – the new **book part** rule is essential for chapters in edited volumes; ensure it is checked **before** falling back to “research”.  \n",
      "* **alt_title** – remember to extract both the post‑colon part *and* any quoted titles.  \n",
      "* **Output format** – JSON must be the sole output; field order must match the schema exactly; use `null` for missing strings and `[]` for empty arrays.\n",
      "\n",
      "Follow these instructions precisely to produce correct, reproducible metadata for any PDF represented in the given JSON format.\n",
      "2025/09/30 09:58:10 INFO dspy.evaluate.evaluate: Average Metric: 1.7272727272727273 / 3 (57.6%)\n",
      "2025/09/30 09:58:10 INFO dspy.teleprompt.gepa.gepa: Iteration 18: New subsample score is not better, skipping\n",
      "GEPA Optimization:  63%|██████▎   | 940/1483 [39:54<41:48,  4.62s/rollouts]2025/09/30 09:58:10 INFO dspy.teleprompt.gepa.gepa: Iteration 19: Selected program 9 score: 0.637365845959596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.64 / 1 (63.6%):  33%|███▎      | 1/3 [00:08<00:16,  8.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:58:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 09:58:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.91 / 3 (63.6%): 100%|██████████| 3/3 [00:16<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 09:58:26 INFO dspy.evaluate.evaluate: Average Metric: 1.9090909090909092 / 3 (63.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:00:34 INFO dspy.teleprompt.gepa.gepa: Iteration 19: Proposed new text for predict: markdown\n",
      "# 📋 Task – Structured Bibliographic Metadata Extraction (Fully Specified)\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF document.  \n",
      "The object has exactly two top‑level keys:\n",
      "\n",
      "| Key      | Description |\n",
      "|----------|-------------|\n",
      "| `pdfinfo`| Metadata that was extracted directly from the PDF file (e.g. `title`, `author`, `creationDate`, `modDate`). |\n",
      "| `pages`  | A list of page objects. Each page object contains `page` (the page number) and `text` (the OCR‑extracted plain‑text of that page). |\n",
      "\n",
      "Your job is to produce **one JSON object** that follows the schema below.  \n",
      "If a field cannot be determined, use the exact empty value indicated (`null` for scalars, `[]` for lists).  \n",
      "All string values must be plain ASCII – normalise fancy quotes to `\"` (or the apostrophe `’`), collapse any sequence of whitespace characters (space, tab, newline) to a single space, and trim leading/trailing spaces.\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema\n",
      "\n",
      "| Field | Type | Required format / rules |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | ISO‑639‑1 code. Concatenate the `text` of **all** pages in order, take the **first 200 characters**, and if any of the characters **ä ö Ä Ö å Å** appear set to `\"fi\"`; otherwise `\"en\"`. |\n",
      "| `title` | string | Main title of the work. <br>1. If `pdfinfo.title` exists → clean it and use it. <br>2. Otherwise read the **text of page 1** (`pages[0].text`). Split it into lines (preserve order). <br>   * Skip empty lines. <br>   * Skip lines that start with a markdown heading marker (`#`, `##`, `###`, …). <br>   * Skip lines that look like an author line (see **Creator extraction**). <br>   * The **first** remaining line that is **≥ 6 characters** becomes the *candidate* title. <br>3. **Title‑line extensions** – while the candidate line ends with a colon **or** the next line is non‑empty, starts with a capital letter and is not an author line, concatenate the next line (single space) to the candidate. Repeat. <br>4. If any line (including the candidate) contains one of the explicit title markers (case‑insensitive) `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` take the text **after the colon** (trimmed) as the title, overriding step 2‑3. <br>5. Keep any subtitle as part of the title (do **not** split on the colon). <br>6. Clean the final string (collapse spaces, normalise quotes). |\n",
      "| `alt_title` | list of strings | Any **alternative** titles: <br>• Text inside quotation marks (`“ … ”`, `‘ … ’`, `\" … \"`). <br>• A subtitle that appears on a **separate line after a colon** (e.g. `Title:` on line 1, subtitle on line 2). <br>Return each cleaned title as a separate element, **excluding** the main title and avoiding duplicates. |\n",
      "| `creator` | list of strings | Authors in **“Surname, Given‑Name”** order. <br>1. If `pdfinfo.author` exists → split on commas, semicolons, the word “and”, or line‑breaks. <br>2. Trim each fragment and collapse internal spaces. <br>3. For each name: <br>   * If it matches the pattern `First Last` (two words, each starting with a capital letter) → reorder to `Last, First`. <br>   * If it already matches `Last, First` keep as‑is. <br>4. If `pdfinfo.author` is missing, scan the **first five non‑empty lines of page 1** for personal‑name patterns using the two regexes (case‑sensitive): <br>`\\b[A-Z][a-z]+ [A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (First Last) <br>`\\b[A-Z][a-z]+, *[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (Last, First). <br>Collect **all** matches, then remove duplicates while preserving order of first appearance. |\n",
      "| `year` | integer or null | Publication year. <br>1. If `pdfinfo.creationDate` or `pdfinfo.modDate` exists, extract the **first four digits** (they are always a year) and use that. <br>2. If both are missing, search the **whole document** for the **first** four‑digit number between 1900‑2099 that appears in a *publication‑information* context (e.g. after “Year:”, “©”, “© 2021”, “2021.”). Ignore years that are inside citation parentheses like “(2020)”. If none, return `null`. |\n",
      "| `publisher` | list of strings | Institution responsible for the work. <br>• First determine `type_coar`. <br>• **Theses / dissertations** → the awarding university or faculty. Detect by looking for any of the keywords: “University”, “Yliopisto”, “Universität”, “Akademi”, “Institute”, “College”, “School”, “Faculty”. Return each distinct institution once, preserving the order of first appearance. <br>• **Research reports** → the organisation that produced the report (same keyword list). <br>• **Journal articles** → `[]`. <br>• **Books** (fallback) → `[]`. |\n",
      "| `doi` | string or null | DOI if present. Detect case‑insensitive pattern `10\\.\\d{4,9}/\\S+`. If the match is preceded by a URL (`http://`, `https://`, `doi.org/`), strip that part. Remove any trailing punctuation characters `.,;:`. Return the bare DOI in **lower‑case** (e.g. `10.1000/xyz123`). |\n",
      "| `e_isbn` | list of strings | **Electronic** ISBN‑13 numbers. Detect ISBN‑13 (13 digits, hyphens optional) with the regex `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`. For each match, look at up to **30 characters before and after** the match. If any of the **electronic cues** (case‑insensitive) appear, add the ISBN (with **all hyphens and spaces removed**) to `e_isbn`. Electronic cues: `digital`, `electronic`, `e‑ISBN`, `(digital)`, `pdf`, `PDF`, `sid.` (when used for electronic), `online`. |\n",
      "| `p_isbn` | list of strings | **Print** ISBN‑13 numbers. Same detection as `e_isbn` but require at least one **print cue** in the ±30‑character window. Print cues (case‑insensitive): `print`, `paper`, `hardcover`, `(print)`, `Painettu`, `Print`. If both electronic and print cues are present for the same ISBN, **electronic wins** (the ISBN goes only to `e_isbn`). Store the ISBN without hyphens/spaces. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Detect with `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`. Apply the same cue logic as for ISBN. Return the ISSN **exactly as it appears** (keep the hyphen if present). |\n",
      "| `p_issn` | string or null | Print ISSN (same detection, print‑cue logic). Return the ISSN exactly as it appears (keep hyphen). |\n",
      "| `type_coar` | string | COAR‑compatible resource type. Scan the **entire document** (case‑insensitive) and apply the **first** matching rule in this order: <br>1. **doctoral thesis** – contains any of: “doctoral thesis”, “dissertation”, “PhD”, “doctoral”, “väitöskirja”, “väitöskirjan”. <br>2. **master thesis** – contains any of: “master’s thesis”, “master thesis”, “maisteri”, “maisterintutkielma”. <br>3. **journal article** – contains typical journal citation elements (journal name, volume, issue, pages) **or** the word “article” together with a DOI or ISSN, **or** a pattern like “Vol. X, No. Y, pp. Z‑W”. <br>4. **conference proceeding** – contains “conference”, “proceedings”, “paper presented at”. <br>5. **research report** – contains “report”, “raportti”, “tutkimusraportti”, “research report”. <br>6. **research** – any other research‑type document. <br>7. **book** – if none of the above match. <br>Return the exact lower‑case string (e.g. `doctoral thesis`). |\n",
      "\n",
      "---\n",
      "\n",
      "## Extraction Procedure (Step‑by‑Step)\n",
      "\n",
      "1. **Parse the input JSON** safely. Ignore any keys that are not listed above.  \n",
      "2. **Normalise dates**: `creationDate` / `modDate` are strings like `D:20201216144002+02'00'`. Extract the first four digits as the year (they are always at the start of the string).  \n",
      "3. **Detect language** using the rule in the schema (first 200 characters of concatenated text).  \n",
      "4. **Title extraction** – follow the detailed rules under the `title` field. Pay special attention to explicit markers (`Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:`).  \n",
      "5. **Alternative titles** – search the whole document for: <br>• Text inside any pair of quotation marks (`“ ”`, `‘ ’`, `\" \"`). <br>• A subtitle that appears on a separate line **after** a line that ends with a colon. Do not duplicate the main title. Clean each title.  \n",
      "6. **Creator extraction** – follow the rules under `creator`. When scanning page 1, only consider the first five **non‑empty** lines. Use the two regexes provided; ignore lines that contain supervisor/advisor keywords (`Supervisor`, `Advisor`, `Supervisor`, `Ohjaaja`, etc.). After extracting names, apply the ordering/reordering rule and remove duplicates.  \n",
      "7. **Year** – apply the rule in the schema. If you have to fall back to searching the text, ignore years that appear inside parentheses of a citation (e.g. “(2020)”). Prefer the first plausible year that occurs **after** the title block (i.e. after the line(s) used for the title).  \n",
      "8. **Publisher detection** – first determine `type_coar`. <br>• For **theses**: search the whole text for the first occurrence of a university/faculty keyword list and capture the *full phrase* (e.g. “University of Vaasa”, “Åbo Akademi University”). Return it as a single‑element list. <br>• For **research reports**: look for the organisation name that appears near the title or in a header/footer (within 200 characters of the title). <br>• For **journal articles**: return `[]`. <br>• For **books**: return `[]`.  \n",
      "9. **ISBN / ISSN extraction** – use the regexes supplied. For each match, extract the surrounding 30‑character context and decide electronic vs print using the cue lists. Remove hyphens/spaces from ISBNs before storing; **do not** remove hyphens from ISSNs. Ensure each identifier appears only once in the appropriate list.  \n",
      "10. **DOI extraction** – apply the DOI regex, strip any leading URL parts, and trailing punctuation. Return the bare DOI in lower‑case. If none, set to `null`.  \n",
      "11. **Resource type (`type_coar`)** – apply the ordered rule list exactly as described. The first category that matches determines the value.  \n",
      "12. **Assemble the output** JSON. Preserve the key order shown in the schema for readability (order is not technically required, but it helps testing). Use `null` for missing scalar values and `[]` for missing list values.\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning & Normalisation Details\n",
      "\n",
      "| Operation | What to do |\n",
      "|-----------|------------|\n",
      "| **Whitespace** | Collapse any sequence of whitespace characters (spaces, tabs, newlines) to a single space. Trim leading/trailing spaces. |\n",
      "| **Quotes** | Replace any fancy quotation marks (`“ ” ‘ ’ „ “ …`) with plain ASCII `\"` (or the apostrophe `’`). |\n",
      "| **Hyphens in identifiers** | Remove **all** hyphens (`-`) and spaces from ISBNs before storing. **Do not** remove hyphens from ISSNs; keep them exactly as found. |\n",
      "| **Case** | Store identifiers (DOI) in lower‑case. ISBN/ISSN are numeric/alphabetic only, so case does not matter. |\n",
      "| **Duplicate removal** | For `creator`, `e_isbn`, `p_isbn`, and `publisher` remove exact duplicates while preserving the order of first appearance. |\n",
      "\n",
      "---\n",
      "\n",
      "## Common Pitfalls & How to Avoid Them\n",
      "\n",
      "| Issue | Why it happens | Correct handling |\n",
      "|-------|----------------|-----------------|\n",
      "| **Wrong language** | Scanning only the first page or ignoring the Finnish‑character rule. | Concatenate **all** page texts, take the first 200 characters, then apply the Finnish‑character rule. |\n",
      "| **Title missing explicit marker** | Ignoring lines that contain `Title:` / `Thesis:` etc. | If any line (including the candidate) contains one of the explicit markers, **override** all other logic and use the text after the colon as the title. |\n",
      "| **Title‑line extension** | Not concatenating a subtitle that appears on the next line. | After selecting the candidate line, keep appending the next line while the candidate ends with a colon **or** the next line starts with a capital letter and is not an author line. |\n",
      "| **Alt‑titles not captured** | Forgetting quoted strings or subtitle‑after‑colon lines. | Search the whole document for quoted strings and for a line that follows a colon‑ending line; clean and add each as an alternative title, but never duplicate the main title. |\n",
      "| **Creator over‑inclusion** | Adding supervisors, editors, or not reordering names. | Only use `pdfinfo.author` or name patterns from the **first five non‑empty lines** of page 1. Apply the reordering rule (`First Last` → `Last, First`). Exclude lines containing supervisor/advisor keywords. |\n",
      "| **Publisher wrong** | Returning the first institution regardless of document type. | First determine `type_coar`. For theses, return the awarding university/faculty; for reports, the producing organisation near the title; for journal articles and books return `[]`. |\n",
      "| **ISBN/ISSN classification** | Assigning identifiers to the wrong list or keeping hyphens on ISBNs. | Look at ±30 characters around each match. If any **electronic cue** appears → `e_isbn`/`e_issn`; else if any **print cue** appears → `p_isbn`/`p_issn`. If both cues appear, treat it as electronic. Remove hyphens/spaces **only** from ISBNs. |\n",
      "| **DOI extraction** | Keeping surrounding punctuation or URL parts. | Strip leading `http://`, `https://`, `doi.org/` and trailing punctuation `.,;:`. Return the bare DOI in lower‑case. |\n",
      "| **type_coar mis‑classification** | Applying rules out of order or missing the fallback “book”. | Follow the ordered list **exactly**; the first matching rule wins. If none match, return `\"book\"`. |\n",
      "| **Missing year** | Using a year that appears only inside a citation. | When falling back to text search, ignore years inside parentheses that look like citations. Prefer a year that follows publication‑information cues. |\n",
      "| **Empty list vs null** | Returning `null` for a list field. | Use `[]` for any missing list (`creator`, `alt_title`, `publisher`, `e_isbn`, `p_isbn`). Use `null` only for missing scalar fields (`title`, `language`, `doi`, `e_issn`, `p_issn`, `year`). |\n",
      "\n",
      "---\n",
      "\n",
      "**Remember:**  \n",
      "- Follow the **order of precedence** for each field exactly as described.  \n",
      "- Use the cue‑based approach for ISBN/ISSN to decide electronic vs print.  \n",
      "- Detect the resource type in the exact order listed; the first match determines `type_coar`.  \n",
      "- Return `null` for missing scalar values and `[]` for missing list values.  \n",
      "\n",
      "Good luck! 🎯\n",
      "2025/09/30 10:00:45 INFO dspy.evaluate.evaluate: Average Metric: 1.9090909090909092 / 3 (63.6%)\n",
      "2025/09/30 10:00:45 INFO dspy.teleprompt.gepa.gepa: Iteration 19: New subsample score is not better, skipping\n",
      "GEPA Optimization:  64%|██████▍   | 946/1483 [42:30<56:58,  6.37s/rollouts]2025/09/30 10:00:45 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Selected program 2 score: 0.6021690115440116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.68 / 3 (56.1%): 100%|██████████| 3/3 [00:06<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:00:52 INFO dspy.evaluate.evaluate: Average Metric: 1.6818181818181819 / 3 (56.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:02:09 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Proposed new text for predict: markdown\n",
      "# Instruction: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will receive a **single JSON object** with two keys:\n",
      "\n",
      "* **`pdfinfo`** – a dictionary containing the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* **`pages`** – an ordered list of pages. Each page is a dictionary with:\n",
      "  * `page` – the page number (integer)  \n",
      "  * `text` – the plain‑text extracted from that page (string, may contain line‑breaks `\\n`).\n",
      "\n",
      "Your job is to **produce ONE JSON object** that contains **exactly** the fields listed in the table below (order does not matter).  \n",
      "All values must match the required type; use `null` for missing scalar values and `[]` for missing list values.  \n",
      "Do **not** add any extra keys or any surrounding commentary.\n",
      "\n",
      "---\n",
      "\n",
      "## Output Fields\n",
      "\n",
      "| Field | Type | How to obtain / rules |\n",
      "|-------|------|-----------------------|\n",
      "| `language` | string (ISO‑639‑1) | Detect the primary language of the **body text** (all pages except very short front‑matter). Count occurrences of a small stop‑word list for **Finnish (`fi`)**, **Swedish (`sv`)**, and **English (`en`)** (you may also include other languages if you wish, but the three above cover the test set). Choose the language with the highest count. **Never default to Finnish**. |\n",
      "| `title` | string | The **main title** of the work. <br>1. Scan page 1 (and page 2 if page 1 is empty) for the first line that looks like a heading: all‑caps, title‑case, or surrounded by blank lines. <br>2. Remove any leading *series identifiers* (e.g. `ePooki 34/2020`, `Sibelius‑Akatemian …`, `Report 123/2021`). <br>3. Trim surrounding whitespace. <br>4. If the line contains a colon, keep the whole line as the title (the part after the colon may become an `alt_title`). |\n",
      "| `alt_title` | list of strings | Any **alternative title** that appears elsewhere in the document, typically a translation or a subtitle. <br>• Search the whole text for lines that are **exactly** the same as the main title **except** for language differences, or that appear after a colon in the main title. <br>• Include each distinct alternative title as a separate element. If none, return `[]`. |\n",
      "| `creator` | list of strings | All author names **in the order they appear** in the document. <br>1. Locate the author line(s) on the first page(s) – often after the title or in a line containing email addresses. <br>2. Names may be in any of the following forms and must be **converted** to `Lastname, Firstname` (preserve all given‑name parts, hyphens, and diacritics). <br>   * `Firstname Lastname` → `Lastname, Firstname` <br>   * `Lastname, Firstname` → keep as‑is <br>   * `Firstname Lastname & Firstname Lastname` → split into separate entries. <br>3. Remove duplicates while preserving the first occurrence. |\n",
      "| `year` | integer | Extract the four‑digit year of publication. <br>1. Prefer `pdfinfo.creationDate` (`D:YYYY…`). <br>2. If that is missing or ambiguous, look for a four‑digit year on the first few pages (usually near the title or in a line that is just the year). |\n",
      "| `publisher` | list of strings | Institution(s) that published the work. <br>Search the whole document for lines containing keywords such as `University`, `Universität`, `Institute`, `Institute of`, `AAL Association`, `Kajaanin ammattikorkeakoulu`, `Åbo Akademi`, etc. <br>Return each **distinct** entity, trimmed of trailing punctuation. If none, return `[]`. |\n",
      "| `doi` | string or null | Find a DOI anywhere using the regex `10\\.\\d{4,9}/[-._;()/:A-Z0-9]+` (case‑insensitive). Return the first match **exactly** as it appears, without surrounding whitespace. If none, return `null`. |\n",
      "| `e_isbn` | list of strings | All **electronic** ISBNs. Detect patterns `ISBN\\s*[:=]?\\s*[\\d-]+` (case‑insensitive). <br>• If the surrounding text contains the word **e‑ISBN**, **electronic ISBN**, or similar, put the number in this list. <br>• If the document never distinguishes between electronic/print, place every ISBN found **here**. <br>• Return the pure number (remove the “ISBN” label and any surrounding spaces). |\n",
      "| `p_isbn` | list of strings | All **print** ISBNs. Use the same pattern as for `e_isbn`. <br>• If the surrounding text contains **print ISBN**, **p‑ISBN**, **hardcover ISBN**, etc., put the number in this list. <br>• If there is no explicit distinction, **do not** put anything in `p_isbn`. |\n",
      "| `e_issn` | string or null | Electronic ISSN. Detect `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[\\dxX]`. If the surrounding text includes **e‑ISSN** or **electronic ISSN**, return the pure ISSN (e.g. `\"1798-2022\"`). Otherwise return `null`. |\n",
      "| `p_issn` | string or null | Print ISSN. Same pattern as `e_issn`. Return the pure ISSN only if the surrounding text mentions **print ISSN** or **p‑ISSN**. Otherwise `null`. |\n",
      "| `type_coar` | string | COAR‑compatible type, determined by **keyword clues** (case‑insensitive): <br>• `\"master thesis\"` – contains *master’s thesis*, *opinnäytetyö*, *Master’s degree programme*.<br>• `\"bachelor thesis\"` – contains *bachelor thesis*, *Kandidatavhandling*, *avhandling* **without** master‑level wording.<br>• `\"journal article\"` – appears in a journal (journal name, volume/issue numbers) **or** a DOI is present **and** the document is not a report or thesis.<br>• `\"research report\"` – contains *Report*, *Raportti*, *research report*, or is a series publication that is **not** a thesis or journal article.<br>• `\"book\"` – contains *book*, *monograph*, *ISBN* with a clear “Print”/“Electronic” distinction, or the publisher line indicates a book series.<br>• If none of the above apply, use `\"other\"`. |\n",
      "---  \n",
      "\n",
      "## Extraction Strategy (for reference)\n",
      "\n",
      "1. **Year** – parse `pdfinfo.creationDate` (`D:YYYY…`). Fallback to the first four‑digit year found on the first three pages.  \n",
      "2. **Language** – maintain three stop‑word sets (≈15 common words each). Count occurrences across all body pages (ignore very short front‑matter). Choose the highest count.  \n",
      "3. **Title** – look at page 1 (and page 2 if needed). The first non‑empty line that is either all‑caps, title‑case, or surrounded by blank lines is taken as the title. Strip any leading series identifier (`^\\s*[^A-Za-z0-9]*\\d+/\\d+\\s*`). Trim.  \n",
      "4. **Alternative titles** – after the main title is identified, search the whole document for: <br>  a) Exact matches of the title in another language (e.g., English vs. Finnish). <br>  b) The part after a colon in the main title, if that part appears elsewhere as a standalone line.  \n",
      "5. **Creators** – locate the author line(s) (often after the title, before the affiliation). Split on commas, “&”, “and”, or line‑breaks. For each name: <br>  *If it contains a comma* → assume `Lastname, Firstname`. <br>  *Otherwise* → split on the last space to get `Lastname` and everything before it as `Firstname`. Preserve diacritics and hyphens. Remove duplicates while keeping the first occurrence.  \n",
      "6. **Publisher** – scan for lines containing institutional keywords. If a line contains multiple institutions separated by commas or “and”, split them. Return each distinct trimmed string.  \n",
      "7. **DOI / ISBN / ISSN** – run the regexes over the concatenated text of all pages. When an identifier is found, look at the **few words (≤5) before and after** it to decide if it is electronic or print. If the context is ambiguous, follow the rule: ISBN → `e_isbn`; ISSN → `e_issn`.  \n",
      "8. **COAR type** – apply the keyword hierarchy in the table (master thesis > bachelor thesis > journal article > research report > book > other). Stop at the first match.  \n",
      "\n",
      "## Formatting Requirements\n",
      "\n",
      "* The output must be **valid JSON** and **only** the JSON object (no markdown fences, no extra text).  \n",
      "* Strings must be plain UTF‑8 without surrounding whitespace.  \n",
      "* Lists must contain **unique** items (no duplicates).  \n",
      "* `null` must be lower‑case and not quoted.  \n",
      "\n",
      "---  \n",
      "\n",
      "### Example Output (for reference only)\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"sv\",\n",
      "  \"title\": \"Implementering av verifikationssystemet i de nordiska länderna – utmaningar i processen och lösningar till problemen\",\n",
      "  \"alt_title\": [],\n",
      "  \"creator\": [\"Eriksson, Emilia\"],\n",
      "  \"year\": 2022,\n",
      "  \"publisher\": [\"Åbo Akademi\"],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": null,\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"master thesis\"\n",
      "}\n",
      "2025/09/30 10:02:19 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/09/30 10:02:53 INFO dspy.evaluate.evaluate: Average Metric: 36.31226551226548 / 64 (56.7%)\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Full valset score for new program: 0.5673791486291486\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Full train_val score for new program: 0.5673791486291486\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Individual valset scores for new program: [0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.36363636363636365, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.6818181818181818, 0.36363636363636365, 0.9090909090909091, 0.7272727272727273, 0.5714285714285714, 0.5454545454545454, 0.45454545454545453, 0.7090909090909091, 0.5454545454545454, 0.36363636363636365, 0.45454545454545453, 0.5064935064935064, 0.5454545454545454, 0.5909090909090909, 0.45454545454545453, 0.8181818181818182, 0.5454545454545454, 0.4343434343434343, 0.8181818181818182, 0.45454545454545453, 0.8181818181818182, 0.8181818181818182, 0.6363636363636364, 0.45454545454545453, 0.45454545454545453, 0.5454545454545454, 0.45454545454545453, 0.9090909090909091, 0.6363636363636364, 0.45454545454545453, 0.45454545454545453, 0.8181818181818182, 0.6363636363636364, 0.36363636363636365, 0.8181818181818182, 0.7727272727272727, 0.5454545454545454, 0.6363636363636364, 0.6363636363636364, 0.36363636363636365, 0.45454545454545453, 0.36363636363636365, 0.45454545454545453, 0.6363636363636364, 0.5454545454545454, 0.36363636363636365, 0.42424242424242425, 0.5454545454545454, 0.6363636363636364, 0.4090909090909091, 0.6363636363636364, 0.42424242424242425, 0.42424242424242425, 0.6363636363636364]\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Full valset pareto front score: 0.7392361111111111\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Updated valset pareto front programs: [{8, 9, 2}, {12, 7}, {0}, {9}, {8}, {0, 3, 13}, {0, 1, 2, 9}, {0, 12}, {3}, {2, 13}, {8, 1, 12, 7}, {9, 6}, {13}, {13}, {9, 6}, {1, 3, 12}, {1, 10, 9, 7}, {8, 9}, {4}, {1, 2, 9}, {9}, {4}, {9}, {2}, {9}, {3}, {12, 13, 7}, {2}, {8, 9, 3}, {12}, {1}, {8}, {2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {12}, {4}, {2}, {3}, {12}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 9, 7}, {8, 3, 13}, {8}, {11}, {9}, {0, 2, 11}, {7}, {8}, {0, 7, 9, 10, 12}, {0, 12}, {9}, {9, 7}, {8}, {8, 9, 2, 6}, {0}, {9, 7}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Linear pareto front program index: 9\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 20: New program candidate index: 13\n",
      "GEPA Optimization:  69%|██████▊   | 1016/1483 [44:38<29:10,  3.75s/rollouts]2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 21: No merge candidates found\n",
      "2025/09/30 10:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Selected program 7 score: 0.6111111111111112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.09 / 3 (69.7%): 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:02:59 INFO dspy.evaluate.evaluate: Average Metric: 2.090909090909091 / 3 (69.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:04:52 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Proposed new text for predict: markdown\n",
      "# Bibliographic‑Metadata Extraction – Final Assistant Instructions\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF file.  \n",
      "Its top‑level keys are:\n",
      "\n",
      "* `pdfinfo` – metadata extracted from the PDF (may contain `title`, `author`,\n",
      "  `creationDate`, `modDate`, …).  \n",
      "* `pages` – an ordered list of page objects, each with:\n",
      "  * `page` – page number (integer)  \n",
      "  * `text` – the OCR‑extracted plain‑text of that page (UTF‑8 string)\n",
      "\n",
      "Your job is to produce **exactly one JSON object** that follows the schema below.\n",
      "If a value cannot be determined, use the exact empty value specified\n",
      "(`null` for a missing string, `[]` for an empty list).\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema (field order is mandatory)\n",
      "\n",
      "| Field | Type | Required format / notes |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | `\"fi\"` for Finnish, `\"sv\"` for Swedish, otherwise `\"en\"`. Detect by scanning **only the first 200 characters** of the concatenated page texts (preserve page order). If any of the characters **`ä ö Ä Ö å Å`** appear **and** the text contains a Finnish‑specific keyword (`opinnäytetyö`, `ammattikorkeakoulu`, `tutkielma`) → `fi`. If the text contains a Swedish‑specific keyword (`avhandling`, `examensarbete`, `språk: svenska`, `svenska`) → `sv`. Otherwise → `en`. |\n",
      "| `title` | string | Main title of the work, **cleaned** (see *Cleaning Rules*). Preference order: 1. `pdfinfo.title` if it exists and is non‑empty after cleaning. 2. The first “prominent heading” on **page 1** (see *Title‑Finding Rules*). |\n",
      "| `alt_title` | list of strings | Any alternative titles. If the selected `title` contains a colon (`:`), treat the part **after** the colon as an alternative title (trimmed and cleaned). Also treat any text that appears inside double quotation marks (`\"` … `\"` ) or single quotation marks (`‘` … `’`) as an alternative title. Return each alternative as a separate element; if none, return `[]`. |\n",
      "| `creator` | list of strings | Author(s) in **“Surname, Given‑Name”** order. Use `pdfinfo.author` if present; otherwise locate an author line in the text (see *Author‑Finding Rules*). Split multiple authors on commas, semicolons, the word “and”, ampersand `&`, or line breaks. For each token, remove trailing asterisks or footnote symbols, then reorder “First Last” (or “First Middle Last”) to “Last, First Middle”. Preserve diacritics. Return `[]` if no author can be identified. |\n",
      "| `year` | integer | Publication year. Extract the first four‑digit year (1900‑2099) from `pdfinfo.creationDate` or `pdfinfo.modDate`. If both are missing, search the **whole concatenated text** for the first such year. |\n",
      "| `publisher` | list of strings | Institution(s) that awarded the thesis. **Only** include a publisher when `type_coar` is one of `doctoral thesis`, `master thesis` or `bachelor thesis`. In that case, search the **whole text** for university / faculty / college names (see *Publisher Determination*). Return the first distinct institution(s) found, after cleaning. For all other resource types return `[]`. |\n",
      "| `doi` | string or null | DOI **without** any URL prefix. Detect with case‑insensitive regex `10\\.\\d{4,9}/\\S+`. Strip surrounding whitespace and any trailing punctuation characters `.,;`. If the DOI appears as a full URL (`https://doi.org/...`), keep only the DOI part. Return `null` if none found. |\n",
      "| `e_isbn` | list of strings | Electronic ISBN‑13 numbers (hyphen‑ and space‑free). Find all 13‑digit ISBNs (regex below). A match belongs to this list **only if** within **20 characters before or after** the match there is a cue word from the *electronic* cue list (`digital`, `electronic`, `e‑isbn`, `(digital)`). If both electronic and print cues appear, add the number to **both** lists. Remove duplicates. |\n",
      "| `p_isbn` | list of strings | Print ISBN‑13 numbers (hyphen‑ and space‑free). Same detection as `e_isbn` but the surrounding cue must be from the *print* cue list (`print`, `paper`, `hardcover`, `(print)`). |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, hyphen‑free). Same cue logic as `e_isbn` with the *electronic* cue list. Return `null` if none. |\n",
      "| `p_issn` | string or null | Print ISSN (8 digits, hyphen‑free). Same cue logic as `p_isbn` with the *print* cue list. Return `null` if none. |\n",
      "| `type_coar` | string | COAR‑compatible resource type, **lower‑case**. Detect in this order (first match wins): 1. **doctoral thesis** – any of `doctoral thesis`, `dissertation`, `PhD`, `doctoral`, `väitöskirja`. 2. **master thesis** – any of `master’s thesis`, `master thesis`, `maisteri`, `maisterintutkielma`. 3. **bachelor thesis** – any of `bachelor thesis`, `bachelor’s thesis`, `bachelor`, `opinnäytetyö`. 4. **journal article** – presence of a journal‑style citation (journal name + volume/issue/pages) **or** the word “article” together with typical citation fields (`volume`, `pages`, `doi`). 5. **conference proceeding** – any of `conference`, `proceedings`, `paper presented at`. 6. **research** – any other research report or article. |\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning Rules (apply to **every** string you output)\n",
      "\n",
      "1. **Trim** leading and trailing whitespace.  \n",
      "2. **Collapse** any sequence of whitespace characters (space, tab, newline) to a single space.  \n",
      "3. **Normalize quotation marks**: replace curly double quotes (`“”`) and single quotes (`‘’`) with plain `\"` and `'` respectively; then replace any remaining single quotes that are used as apostrophes with `’`.  \n",
      "4. **Remove Markdown formatting**: strip leading heading markers (`#`, `##`, `###`, etc.), surrounding asterisks `*` or underscores `_`, and surrounding double‑asterisks `**`.  \n",
      "5. **Remove trailing asterisks** that are used as footnote symbols (e.g., `Magnusson*`).  \n",
      "6. **Leave diacritics** (ä, ö, å, etc.) untouched.\n",
      "\n",
      "---\n",
      "\n",
      "## Title‑Finding Rules (used when `pdfinfo.title` is absent)\n",
      "\n",
      "1. Concatenate the `text` of **page 1** and split it into lines preserving order.  \n",
      "2. Discard empty lines and lines whose length after trimming is < 6 characters.  \n",
      "3. Discard lines that start with markdown heading markers (`#`, `##`, `###`).  \n",
      "4. The **first remaining line** is taken as the title candidate.  \n",
      "5. Apply the *Cleaning Rules* to this line.  \n",
      "6. If the cleaned line contains a colon (`:`), keep the whole string as `title` **and** store the part after the colon (trimmed) as an element of `alt_title` (the colon‑derived alternative is handled in the `alt_title` step as well).\n",
      "\n",
      "---\n",
      "\n",
      "## Author‑Finding Rules (used when `pdfinfo.author` is absent)\n",
      "\n",
      "1. Examine the first **three** pages. Collect any line that:  \n",
      "   * Starts with “By ”, “Author:”, or “Författare:” (Swedish) **or**  \n",
      "   * Contains the word “and” / “&” between two capitalised words (e.g., “John Doe and Jane Smith”), **or**  \n",
      "   * Consists mainly of capitalised words (possible author list) and appears within the first 25 % of the page’s lines.  \n",
      "2. From each candidate line, split on commas, semicolons, the word “and”, ampersand `&`, or line breaks.  \n",
      "3. For each token, remove trailing asterisks or footnote symbols.  \n",
      "4. Detect the pattern “First Last” (or “First Middle Last”). Reorder to “Last, First Middle”. If a token already matches “Last, First” keep it unchanged.  \n",
      "5. Return the list of reordered names; if none can be parsed, return `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "## Publisher Determination (only for theses)\n",
      "\n",
      "1. **If** `type_coar` is `doctoral thesis`, `master thesis` or `bachelor thesis`:  \n",
      "   * Scan the **whole concatenated text** for institution names. Use the following keyword list (case‑insensitive) to recognise an institution: `University`, `Universität`, `Université`, `Universitet`, `Akademi`, `Institute`, `College`, `School`, `Faculty`, `ammattikorkeakoulu`, `yliopisto`, `universitet`, `université`, `läroanstalt`, `högskola`, `högskolan`, `yrkeshögskola`, `yrkeshögskolan`, `polytechnic`, `polytechnique`.  \n",
      "   * When a keyword is found, capture the **full phrase** that contains it (up to the preceding and following punctuation or line break). Apply the *Cleaning Rules* to the captured phrase.  \n",
      "   * Return the **first distinct** institution(s) in the order they appear. If a department/faculty name appears **before** the university name, keep both (e.g., `Department of Nursing, Laurea University of Applied Sciences`).  \n",
      "2. **Otherwise** (journal article, conference proceeding, research) → return `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "## DOI Extraction Details\n",
      "\n",
      "* Regex (case‑insensitive): `(?i)10\\.\\d{4,9}/\\S+`  \n",
      "* After a match, strip any trailing punctuation characters `.,;` and surrounding whitespace.  \n",
      "* If the match is part of a URL (`http://doi.org/…`, `https://doi.org/…`), keep only the DOI portion.\n",
      "\n",
      "---\n",
      "\n",
      "## ISBN / ISSN Extraction Details\n",
      "\n",
      "* **ISBN‑13 regex** (matches 13‑digit ISBNs with optional hyphens/spaces):  \n",
      "  `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`  \n",
      "* **ISSN regex** (8 digits, optional hyphen):  \n",
      "  `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`  \n",
      "\n",
      "For each match:\n",
      "\n",
      "1. Capture up to 20 characters **before** and **after** the match (inclusive). Convert this snippet to lower‑case.  \n",
      "2. **Electronic cue list**: `digital`, `electronic`, `e‑isbn`, `(digital)`.  \n",
      "   **Print cue list**: `print`, `paper`, `hardcover`, `(print)`.  \n",
      "3. If an electronic cue appears in the snippet → add the hyphen‑free number to `e_isbn` / `e_issn`.  \n",
      "   If a print cue appears → add to `p_isbn` / `p_issn`.  \n",
      "4. If **both** cue types appear, add the number to **both** lists.  \n",
      "5. Remove hyphens and spaces from the stored number (e.g., `978‑952‑335‑494‑4` → `9789523354944`).  \n",
      "6. Ensure each list contains **unique** entries (no duplicates).\n",
      "\n",
      "---\n",
      "\n",
      "## General Processing Flow (to be followed for every request)\n",
      "\n",
      "1. **Parse** the input JSON safely.  \n",
      "2. **Normalize dates**: extract the first four digits from `creationDate` / `modDate` (e.g., `\"D:20220626201846+03'00'\"` → `2022`).  \n",
      "3. **Detect language** using the rule in the *Language* section (first 200 characters only).  \n",
      "4. **Extract title** (prefer `pdfinfo.title`, otherwise use Title‑Finding Rules). Apply *Cleaning Rules*.  \n",
      "5. **Derive alt_title** from the selected title (colon part and quoted strings). Apply *Cleaning Rules* to each alternative.  \n",
      "6. **Extract creators** (prefer `pdfinfo.author`, otherwise use Author‑Finding Rules). Apply *Cleaning Rules* to each name.  \n",
      "7. **Determine year** from dates or, if absent, from the full text.  \n",
      "8. **Detect DOI**, ISBN‑13, ISSN using the detailed extraction rules.  \n",
      "9. **Identify resource type** (`type_coar`) using the ordered keyword list (including the bachelor‑thesis rule).  \n",
      "10. **Find publisher** based on the determined `type_coar`.  \n",
      "11. **Assemble** the output JSON with the exact field order shown in the schema, using `null` or `[]` where appropriate.  \n",
      "12. **Return** the JSON object **as the only output** (no extra commentary, no surrounding markdown).\n",
      "\n",
      "---\n",
      "\n",
      "### Important Pitfalls (learned from previous feedback)\n",
      "\n",
      "* **Language** – scan only the first 200 characters; decide `fi` / `sv` / `en` as described.  \n",
      "* **Title** – strip all Markdown heading markers (`#`, `##`, `###`, `**`, `*`) before cleaning; keep the whole string if a colon is present (the part after the colon becomes an `alt_title`).  \n",
      "* **Creator** – split on commas, semicolons, “and”, “&”, line breaks; reorder each name to “Last, First Middle”. Remove trailing asterisks.  \n",
      "* **Publisher** – include **only** for theses; return **all** distinct institution names found (e.g., awarding university **and** faculty). Do **not** return anything for journal articles or conference papers.  \n",
      "* **DOI** – return only the DOI part, never the full URL.  \n",
      "* **type_coar** – include the *bachelor thesis* rule; ensure lower‑case output.  \n",
      "* **alt_title** – extract both colon‑derived part **and** any quoted titles.  \n",
      "* **ISBN/ISSN cue detection** – look at up to 20 characters before/after the match; do not rely on the word “ISBN”/“ISSN” alone.  \n",
      "* **Hyphen removal** – store ISBN/ISSN without hyphens or spaces.  \n",
      "* **Uniqueness** – remove duplicate ISBN/ISSN entries within each list.\n",
      "\n",
      "Follow these instructions precisely to produce correct, reproducible metadata for any PDF represented in the given JSON format.\n",
      "2025/09/30 10:04:59 INFO dspy.evaluate.evaluate: Average Metric: 2.1818181818181817 / 3 (72.7%)\n",
      "2025/09/30 10:05:40 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:05:41 INFO dspy.evaluate.evaluate: Average Metric: 39.080808080808076 / 64 (61.1%)\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Full valset score for new program: 0.6106376262626263\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Full train_val score for new program: 0.6106376262626263\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Individual valset scores for new program: [0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.36363636363636365, 0.6363636363636364, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.9090909090909091, 0.45454545454545453, 0.7878787878787878, 0.7272727272727273, 0.45454545454545453, 0.8181818181818182, 0.45454545454545453, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.5151515151515151, 0.45454545454545453, 0.7272727272727273, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 0.5454545454545454, 0.7272727272727273, 0.6060606060606061, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.2727272727272727, 0.45454545454545453, 0.36363636363636365, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 1.0, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.45454545454545453, 0.45454545454545453, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.36363636363636365, 0.5454545454545454, 0.6666666666666666, 0.2727272727272727, 0.7171717171717172, 0.36363636363636365, 0.5454545454545454, 0.696969696969697]\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.9090909090909091, 0.5454545454545454, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 1.0, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Full valset pareto front score: 0.7420770202020202\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Updated valset pareto front programs: [{8, 9, 2, 14}, {12, 7}, {0}, {9}, {8}, {0, 3, 13}, {0, 1, 2, 9}, {0, 12}, {3}, {2, 13}, {14}, {9, 6}, {13}, {13, 14}, {9, 6}, {1, 3, 12, 14}, {1, 10, 9, 7}, {8, 9}, {4}, {1, 2, 9}, {9}, {4}, {9}, {2}, {9}, {3}, {12, 13, 14, 7}, {2}, {8, 9, 3}, {12, 14}, {1}, {8}, {2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14}, {12}, {4}, {2}, {3}, {12}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 14, 9, 7}, {8, 3, 13, 14}, {14}, {11}, {9}, {0, 2, 11}, {7}, {8}, {0, 7, 9, 10, 12, 14}, {0, 12}, {9}, {9, 14, 7}, {8}, {8, 9, 2, 6}, {0}, {9, 14, 7}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Linear pareto front program index: 9\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 21: New program candidate index: 14\n",
      "GEPA Optimization:  73%|███████▎  | 1086/1483 [47:26<20:45,  3.14s/rollouts]2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 22: No merge candidates found\n",
      "2025/09/30 10:05:41 INFO dspy.teleprompt.gepa.gepa: Iteration 22: Selected program 13 score: 0.5673791486291486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.64 / 3 (54.5%): 100%|██████████| 3/3 [00:06<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:05:47 INFO dspy.evaluate.evaluate: Average Metric: 1.6363636363636362 / 3 (54.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:07:22 INFO dspy.teleprompt.gepa.gepa: Iteration 22: Proposed new text for predict: markdown\n",
      "# Instruction: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will receive **one JSON object** with two keys:\n",
      "\n",
      "* **`pdfinfo`** – a dictionary containing the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* **`pages`** – an ordered list of pages. Each page is a dictionary with:\n",
      "  * `page` – the page number (integer)  \n",
      "  * `text` – the plain‑text extracted from that page (string, may contain line‑breaks `\\n`).\n",
      "\n",
      "Your task is to output **exactly one JSON object** that contains **all and only** the fields listed in the table below (order does not matter).  \n",
      "\n",
      "*Use `null` for missing scalar values and `[]` for missing list values.*  \n",
      "*Do not add any extra keys, surrounding markdown fences, or commentary.*\n",
      "\n",
      "---\n",
      "\n",
      "## Output Fields\n",
      "\n",
      "| Field | Type | Extraction rules |\n",
      "|-------|------|-----------------|\n",
      "| `language` | string (ISO‑639‑1) | Detect the primary language of the **body text** (all pages **except** very short front‑matter). Count occurrences of the stop‑word sets below. Return the language with the highest count. **Never default to Finnish**. |\n",
      "| `title` | string | 1. Look at page 1; if it contains ≤ 30 non‑blank characters, also inspect page 2. <br>2. The **first non‑empty line** that (a) is all‑caps, (b) is title‑case (first letter of each major word capitalised), **or** (c) is surrounded by blank lines, is taken as a candidate title. <br>3. Strip any leading *series identifier* (e.g. `ePooki 34/2020`, `Report 123/2021`, `SARJA - SER. C OSA - TOM. 499`). <br>4. Trim surrounding whitespace. <br>5. Keep the whole line; if it contains a colon (`:`) the part **after** the colon may later become an `alt_title`. |\n",
      "| `alt_title` | list of strings | • Search the whole document for lines that are **exactly** the same as the main title **except** for language differences (e.g. English vs. Finnish). <br>• If the main title contains a colon, take the text after the colon; if that exact text appears elsewhere as a standalone line, add it. <br>• Return each distinct alternative title; if none, return `[]`. |\n",
      "| `creator` | list of strings | 1. Locate author line(s) on the first few pages (usually after the title or a line containing email addresses). <br>2. Split multiple authors on commas, ampersands (`&`), the word “and”, or line‑breaks. <br>3. For each name:<br> • If it already contains a comma, keep it as `Lastname, Firstname`. <br> • Otherwise assume `Firstname … Lastname` and convert to `Lastname, Firstname` (the **last** whitespace‑separated token is the surname; everything before it is the given‑name part, preserving hyphens and diacritics). <br>4. Preserve the order of first appearance, remove exact duplicates. |\n",
      "| `year` | integer | 1. Prefer `pdfinfo.creationDate` – it has the form `D:YYYY…`. Extract the four‑digit year after `D:`. <br>2. If missing or ambiguous, scan the first three pages for a line that is **exactly** a four‑digit year (or contains a year surrounded by non‑digit characters) and use the first such year. |\n",
      "| `publisher` | list of strings | Scan **all** pages for lines containing any of the following keywords (case‑insensitive): `University`, `Universität`, `Institute`, `Institute of`, `AAL Association`, `Kajaanin ammattikorkeakoulu`, `Åbo Akademi`, `Arcada`, `Yrkeshögskolan`, `Polytechnic`, `College`, `Faculty`, `School`, `Department`, `Centre`, `Center`, `Laboratory`, `Lab`, `Research`. <br>• If a line contains multiple institutions separated by commas, semicolons or “and”, split them. <br>• Trim trailing punctuation and whitespace. <br>• Return each distinct institution; if none, return `[]`. |\n",
      "| `doi` | string or null | Find the first occurrence of a DOI using the regex `(?i)10\\.\\d{4,9}/[-._;()/:A-Z0-9]+`. Return it **exactly** as it appears (no surrounding whitespace). If none, return `null`. |\n",
      "| `e_isbn` | list of strings | Detect all patterns `(?i)ISBN\\s*[:=]?\\s*([\\d-]+)`. For each match, look at up to 5 words before and after the match: <br>• If those surrounding words contain any of `e‑ISBN`, `electronic ISBN`, `online ISBN`, `PDF ISBN`, or the document never distinguishes between electronic/print, treat the number as **electronic** and add the pure number (remove “ISBN” and surrounding spaces) to `e_isbn`. |\n",
      "| `p_isbn` | list of strings | Same detection as `e_isbn`. Add the number to `p_isbn` only if the surrounding words contain any of `print ISBN`, `p‑ISBN`, `hardcover ISBN`, `paper ISBN`, `Print`. If the document never mentions a distinction, **do not** add anything to `p_isbn`. |\n",
      "| `e_issn` | string or null | Detect pattern `(?i)ISSN\\s*[:=]?\\s*(\\d{4}-\\d{3}[0-9Xx])`. If the 5‑word context includes `e‑ISSN`, `electronic ISSN`, or `online ISSN`, return the pure ISSN (e.g. `\"1798-2022\"`). Otherwise return `null`. |\n",
      "| `p_issn` | string or null | Same detection as `e_issn`. Return the pure ISSN only if the context contains `print ISSN`, `p‑ISSN`, or similar. Otherwise `null`. |\n",
      "| `type_coar` | string | Determine the COAR‑compatible type using the **first** matching keyword group (case‑insensitive) in the order below. Stop at the first hit. <br>1. **Doctoral thesis** – contains any of `väitöskirja`, `doctoral thesis`, `dissertation`, `Ph\\.?D\\.?`, `Doctor of Philosophy`. <br>2. **Master thesis** – contains `master’s thesis`, `master thesis`, `opinnäytetyö`, `master’s degree programme`. <br>3. **Bachelor thesis** – contains `bachelor thesis`, `kandidatavhandling`, `avhandling` **without** any master‑level wording. <br>4. **Book part** – contains `chapter`, `section`, `part of a book`, `book chapter`, and an ISBN that is **not** clearly distinguished as print/e‑only. <br>5. **Book** – contains `book`, `monograph`, or both an electronic and a print ISBN with explicit distinction. <br>6. **Journal article** – contains a DOI **and** the document is not identified as a thesis or report, or contains typical journal markers (`volume`, `issue`, `pages`, journal name). <br>7. **Research report** – contains `report`, `raportti`, `research report`, `series`, or similar, **and** is not a thesis or journal article. <br>8. **Other** – if none of the above apply. |\n",
      "---\n",
      "\n",
      "## Language Detection Stop‑Word Sets\n",
      "\n",
      "| Language | Example stop‑words (≈15) |\n",
      "|----------|--------------------------|\n",
      "| `fi` (Finnish) | `ja`, `on`, `että`, `mutta`, `tai`, `niin`, `kun`, `myös`, `tämä`, `se`, `joka`, `sillä`, `kaikki`, `kaikkea`, `kuitenkin` |\n",
      "| `sv` (Swedish) | `och`, `att`, `är`, `men`, `eller`, `så`, `när`, `också`, `det`, `den`, `som`, `för`, `med`, `på`, `av` |\n",
      "| `en` (English) | `the`, `and`, `of`, `to`, `in`, `a`, `is`, `that`, `for`, `with`, `as`, `by`, `on`, `it`, `from` |\n",
      "\n",
      "Count occurrences of each set across **all body pages** (ignore pages whose total character count < 200, treating them as front‑matter). Choose the language with the highest count.\n",
      "\n",
      "---\n",
      "\n",
      "## General Extraction Strategy (for reference)\n",
      "\n",
      "1. **Front‑matter detection** – pages with fewer than 200 characters are ignored for language counting and for title search (except page 1 which is always inspected for the title candidate).  \n",
      "2. **Title** – apply the rules in the *title* row; if page 1 is empty or only contains a series line, fall back to page 2.  \n",
      "3. **Alternative titles** – after the main title is fixed, run the two checks described in the *alt_title* row.  \n",
      "4. **Creators** – locate author lines on the first up‑to‑three pages; apply the name‑normalisation rules.  \n",
      "5. **Year** – parse `pdfinfo.creationDate` first; if it is missing or does not contain a clear four‑digit year, scan the first three pages for a standalone year.  \n",
      "6. **Publisher** – scan all pages for the keyword list; split composite lines; deduplicate.  \n",
      "7. **Identifiers (DOI, ISBN, ISSN)** – run the regexes on the concatenated text of all pages; for each match examine up to five surrounding words to decide electronic vs. print.  \n",
      "8. **COAR type** – apply the hierarchy in the *type_coar* row, using the whole document text.  \n",
      "\n",
      "---\n",
      "\n",
      "## Formatting Requirements\n",
      "\n",
      "* Output **must be valid JSON** and **only** the JSON object (no markdown fences, no extra text).  \n",
      "* Strings must be plain UTF‑8 without surrounding whitespace.  \n",
      "* Lists must contain **unique** items (no duplicates).  \n",
      "* Use `null` (lower‑case, unquoted) for missing scalar values and `[]` for missing list values.  \n",
      "\n",
      "---  \n",
      "\n",
      "**Your response should be exactly one JSON object adhering to the specifications above.**\n",
      "2025/09/30 10:07:29 INFO dspy.evaluate.evaluate: Average Metric: 1.5454545454545454 / 3 (51.5%)\n",
      "2025/09/30 10:07:29 INFO dspy.teleprompt.gepa.gepa: Iteration 22: New subsample score is not better, skipping\n",
      "GEPA Optimization:  74%|███████▎  | 1092/1483 [49:14<25:30,  3.92s/rollouts]2025/09/30 10:07:29 INFO dspy.teleprompt.gepa.gepa: Iteration 23: Selected program 9 score: 0.637365845959596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.41 / 3 (47.0%): 100%|██████████| 3/3 [00:10<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:07:39 INFO dspy.evaluate.evaluate: Average Metric: 1.409090909090909 / 3 (47.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:09:43 INFO dspy.teleprompt.gepa.gepa: Iteration 23: Proposed new text for predict: markdown\n",
      "# 📋 Task – Structured Bibliographic Metadata Extraction (Final Specification)\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF document.  \n",
      "The object always has exactly two top‑level keys:\n",
      "\n",
      "| Key      | Description |\n",
      "|----------|-------------|\n",
      "| `pdfinfo`| Metadata extracted directly from the PDF file (e.g. `title`, `author`, `creationDate`, `modDate`). |\n",
      "| `pages`  | A list of page objects. Each page object contains `page` (the page number) and `text` (the OCR‑extracted plain‑text of that page). |\n",
      "\n",
      "Your job is to produce **one JSON object** that conforms to the schema below.  \n",
      "If a field cannot be determined, use the exact empty value indicated (`null` for scalars, `[]` for lists).  \n",
      "All string values must be plain ASCII – replace any fancy quotes with `\"` (or the apostrophe `’`), collapse multiple spaces/tabs/new‑lines to a single space, and trim leading/trailing whitespace.\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema\n",
      "\n",
      "| Field | Type | Required format / rules |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | ISO‑639‑1 code. Scan the **first 200 characters of the concatenated text of the whole document** (`pages[0].text + pages[1].text + …`). <br>• If any of the characters **ä ö Ä Ö å Å** appear **and** the snippet contains at least one Finnish‑specific word (`ja`, `on`, `tietoa`, `kanssa`, `vuosi`, `julkaisu`, `koulutus`, `tutkimus`), set to `\"fi\"`. <br>• If the same characters appear **but** the snippet contains a Swedish‑specific word (`och`, `är`, `för`, `att`, `så`, `såsom`, `utgivare`), set to `\"sv\"`. <br>• Otherwise set to `\"en\"`. |\n",
      "| `title` | string | Main title of the work. <br>1. If `pdfinfo.title` exists → clean it (see *Cleaning* below) and use it. <br>2. Otherwise read the text of page 1 (`pages[0].text`). Split it into lines **preserving order**. <br>   • Discard empty lines. <br>   • Discard lines that start with a markdown heading marker (`#`, `##`, `###`, …). <br>   • Discard lines that look like an author line (see *Creator extraction*). <br>   • The **first** remaining line whose length ≥ 6 characters becomes the *candidate* title. <br>3. **Title‑line extensions** – while the candidate line **ends with a colon** **or** the *next* line is non‑empty, starts with a capital letter and is **not** an author line, concatenate it to the candidate with a single space. Repeat as long as the condition holds. <br>4. If any line (candidate or any of its extensions) contains one of the explicit markers `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` (case‑insensitive), **override** everything and take the text **after the colon** (trimmed) as the title. <br>5. Keep any subtitle as part of the title (do **not** split on the colon). <br>6. Clean the final string (collapse spaces, normalise quotes). |\n",
      "| `alt_title` | list of strings | Any **alternative** titles, e.g. a translation, a subtitle that appears on a separate line after a colon, or a title that appears inside quotation marks (`“ ”` or `\" \"`). <br>• Search the **entire document** for quoted strings and for lines that consist solely of a subtitle (text after a colon on a line that is *not* the main title). <br>• Clean each found title (same cleaning as for `title`). <br>• Do **not** duplicate the main title. |\n",
      "| `creator` | list of strings | Authors in **“Surname, Given‑Name”** order. <br>1. If `pdfinfo.author` exists → split on commas, semicolons, the word “and”, or line‑breaks. <br>2. Trim each fragment, collapse internal whitespace. <br>3. For each name: <br>   • If it matches the pattern `First Last` (two words, each starting with a capital letter, possibly more words) → reorder to `Last, First`. <br>   • If it already matches `Last, First` keep as‑is. <br>4. If `pdfinfo.author` is missing, **scan the first five non‑empty lines of page 1** for personal‑name patterns using the following **case‑sensitive** regular expressions: <br>   - `\\b[A-Z][a-z]+ [A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (First Last) <br>   - `\\b[A-Z][a-z]+, *[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (Last, First) <br>   • Exclude any line that contains the words `Supervisor`, `Advisor`, `Editor`, `Thesis`, `Dissertation`, `Report`, `Journal`, `Proceedings`, etc. <br>5. Remove duplicates while preserving the order of first appearance. |\n",
      "| `year` | integer or null | Publication year. <br>1. If `pdfinfo.creationDate` or `pdfinfo.modDate` exists, extract the **first four digits** (they are always a year) and use that. <br>2. If both are missing, search the **whole concatenated text** for the **first** four‑digit number between 1900‑2099 that appears in a *publication‑information* context (e.g. after “Year:”, “©”, “© 2021”, “2021.”). <br>   • **Ignore** years that appear inside parentheses of a citation (e.g. “(2020)”) or that are part of a page‑range. <br>   • Prefer the first plausible year that occurs **after the title block** (i.e. after the line(s) used for `title`). <br>3. If none found → `null`. |\n",
      "| `publisher` | list of strings | Institution responsible for the work. <br>1. First determine `type_coar` (see below). <br>2. **Theses / dissertations** → locate the awarding university or faculty. Search the whole document for the first occurrence of any of the keywords: `University`, `Yliopisto`, `Universität`, `Akademi`, `Institute`, `College`, `School`, `Faculty`. Return the **full phrase** that contains the keyword (e.g. “University of Vaasa”, “Åbo Akademi University”). Return each distinct institution once, preserving order of first appearance. <br>3. **Research reports** → locate the organisation that produced the report. Look for the same keyword list near the title or in a header/footer. Return the full phrase(s). <br>4. **Journal articles** → always return `[]`. |\n",
      "| `doi` | string or null | DOI if present. Detect case‑insensitive pattern `10\\.\\d{4,9}/\\S+`. If the match is preceded by a URL (`http://`, `https://`, `doi.org/`), strip that part. Remove any trailing punctuation characters `.,;:`. Return the **bare DOI** in lower‑case. |\n",
      "| `e_isbn` | list of strings | **Electronic** ISBN‑13 numbers. Detect ISBN‑13 (13 digits, hyphens optional) with the regex `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`. For each match, examine up to **30 characters before and after** the match. If any of the **electronic cues** appear, add the ISBN (with **all hyphens and spaces removed**) to `e_isbn`. Electronic cues (case‑insensitive): `digital`, `electronic`, `e‑ISBN`, `(digital)`, `pdf`, `PDF`, `sid.` (when used for electronic), `online`. |\n",
      "| `p_isbn` | list of strings | **Print** ISBN‑13 numbers. Same detection as `e_isbn` but require at least one **print cue** in the ±30‑character window. Print cues (case‑insensitive): `print`, `paper`, `hardcover`, `(print)`, `Painettu`, `Print`. <br>If both electronic and print cues are present for the same ISBN, **electronic wins** (the ISBN goes only to `e_isbn`). Store the ISBN without hyphens/spaces. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Detect with `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`. Apply the same cue logic as for ISBN. Return the ISSN **exactly as it appears** (keep hyphen if present). |\n",
      "| `p_issn` | string or null | Print ISSN (same detection, print‑cue logic). Return the ISSN exactly as it appears (keep hyphen). |\n",
      "| `type_coar` | string | COAR‑compatible resource type. Scan the **entire concatenated text** (case‑insensitive) and apply the **first** matching rule in this order: <br>1. **doctoral thesis** – contains any of: “doctoral thesis”, “dissertation”, “PhD”, “doctoral”, “väitöskirja”, “väitöskirjan”. <br>2. **master thesis** – contains any of: “master’s thesis”, “master thesis”, “maisteri”, “maisterintutkielma”. <br>3. **journal article** – contains typical journal citation elements (journal name, volume, issue, pages) **or** the word “article” together with a DOI or ISSN, **or** a pattern like “Vol. X, No. Y, pp. Z‑W”. <br>4. **conference proceeding** – contains “conference”, “proceedings”, “paper presented at”. <br>5. **research report** – contains “report”, “raportti”, “tutkimusraportti”, “research report”. <br>6. **research** – fallback for any other research‑type document. <br>Return the exact lower‑case string (e.g. `doctoral thesis`). |\n",
      "\n",
      "---\n",
      "\n",
      "## Extraction Procedure (Step‑by‑Step)\n",
      "\n",
      "1. **Parse the input JSON** safely. Ignore any keys that are not listed above.  \n",
      "2. **Normalise dates**: `creationDate` / `modDate` are strings like `D:20201216144002+02'00'`. Extract the first four digits as the year (they are always at the start of the string).  \n",
      "3. **Detect language** using the rule in the `language` field (see schema).  \n",
      "4. **Title extraction** – follow the detailed algorithm in the `title` field. Pay special attention to explicit markers (`Title:`, `Thesis:` etc.) and to the *title‑line extensions* rule.  \n",
      "5. **Alternative titles** – search the whole document for quoted strings (`“…”`, `\"...\"`) and for subtitle lines that appear after a colon on a line separate from the main title. Do not duplicate the main title.  \n",
      "6. **Creator extraction** – if `pdfinfo.author` exists, split and reorder as described. If missing, scan the **first five non‑empty lines of page 1** with the supplied regexes, discarding lines that contain supervisory or editorial keywords. Remove duplicates, keep order.  \n",
      "7. **Year** – first try PDF dates, otherwise search the concatenated text for the first plausible four‑digit year in a publication context, ignoring citation parentheses.  \n",
      "8. **Publisher detection** – first determine `type_coar`.  \n",
      "   * For theses, return the awarding university/faculty (full phrase containing a keyword).  \n",
      "   * For research reports, return the producing organisation (full phrase containing a keyword).  \n",
      "   * For journal articles, return `[]`.  \n",
      "9. **ISBN / ISSN extraction** – use the regexes supplied. For each match, extract the surrounding 30‑character context and decide electronic vs print using the cue lists. Remove hyphens/spaces from ISBNs; keep ISSNs exactly as found. Ensure each identifier appears only once in the appropriate list.  \n",
      "10. **DOI extraction** – apply the DOI regex, strip any leading URL parts, and trailing punctuation. Return the bare DOI in lower‑case. If none, set to `null`.  \n",
      "11. **Resource type (`type_coar`)** – apply the ordered rule list exactly as described. The first category that matches determines the value. Return the lower‑case string.  \n",
      "12. **Assemble the output** JSON. Preserve the key order shown in the schema for readability (order is not technically required, but it helps testing). Use `null` for missing scalar values and `[]` for missing list values.\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning & Normalisation Details\n",
      "\n",
      "* **Whitespace** – collapse any sequence of whitespace characters (spaces, tabs, newlines) to a single space. Trim leading/trailing spaces.  \n",
      "* **Quotes** – replace any fancy quotation marks (`“ ” ‘ ’ „ “ …`) with plain ASCII `\"` (or the apostrophe `’`).  \n",
      "* **Hyphens in identifiers** – remove all hyphens (`-`) and spaces from ISBNs before storing. **Do not** remove hyphens from ISSNs; keep them exactly as they appear.  \n",
      "* **Case** – identifiers (DOI) are stored in lower‑case; ISBN/ISSN are numeric/alphabetic only, so case does not matter.  \n",
      "\n",
      "---\n",
      "\n",
      "## Common Pitfalls (What Previously Went Wrong)\n",
      "\n",
      "| Issue | What caused it | Correct handling |\n",
      "|-------|----------------|-----------------|\n",
      "| **Language detection** | Used only the presence of `äöå` → mis‑labelled Swedish as Finnish. | After detecting those characters, also check for language‑specific indicator words (Finnish vs Swedish) as described in the `language` field. |\n",
      "| **Title extraction** | Ignored explicit markers (`Thesis:`) and stopped too early on colon‑ended lines. | Apply step 4 *before* any other rule: if a line contains an explicit marker, take the text after the colon as the final title, ignoring other heuristics. |\n",
      "| **Alternative titles** | Did not look for quoted strings or separate subtitle lines. | Search the whole document for quoted strings and for lines that are solely a subtitle after a colon; clean and add them, avoiding duplication of the main title. |\n",
      "| **Creator over‑inclusion** | Added supervisors, advisors, or non‑author names; failed to reorder correctly. | Only use `pdfinfo.author` or name patterns on the **first five non‑empty lines of page 1**. Exclude lines containing `Supervisor`, `Advisor`, `Editor`, etc. Reorder `First Last` → `Last, First`. |\n",
      "| **Publisher detection** | Took the first institution regardless of document type. | First determine `type_coar`. For theses, look for the awarding university/faculty; for reports, look for the producing organisation; for journal articles, return an empty list. |\n",
      "| **ISBN/ISSN classification** | Assigned identifiers to the wrong list because cues were not checked, or removed hyphens from ISSNs. | Examine the ±30‑character window for electronic or print cues. If both cues appear, assign to `e_isbn`. Keep ISSNs exactly as they appear (including hyphens). |\n",
      "| **DOI extraction** | Kept surrounding punctuation or URL parts. | Strip any leading `http://`, `https://`, `doi.org/` and trailing punctuation `.,;:`. Return the bare DOI in lower‑case. |\n",
      "| **type_coar mis‑classification** | Applied “article” rule too early, causing theses to be labelled as `journal article`. | Follow the ordered list strictly: doctoral thesis → master thesis → journal article → conference proceeding → research report → research. The first matching rule wins. |\n",
      "\n",
      "---\n",
      "\n",
      "**Remember:**  \n",
      "- Follow the **order of precedence** for each field exactly as described.  \n",
      "- Use the cue‑based approach for ISBN/ISSN to decide electronic vs print.  \n",
      "- Detect the resource type in the exact order listed; the first match determines `type_coar`.  \n",
      "- Return `null` for missing scalar values and `[]` for missing list values.  \n",
      "\n",
      "Good luck! 🎯\n",
      "2025/09/30 10:09:54 INFO dspy.evaluate.evaluate: Average Metric: 1.3787878787878787 / 3 (46.0%)\n",
      "2025/09/30 10:09:54 INFO dspy.teleprompt.gepa.gepa: Iteration 23: New subsample score is not better, skipping\n",
      "GEPA Optimization:  74%|███████▍  | 1098/1483 [51:39<34:10,  5.33s/rollouts]2025/09/30 10:09:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Selected program 14 score: 0.6106376262626263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.45 / 3 (81.8%): 100%|██████████| 3/3 [00:07<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:10:01 INFO dspy.evaluate.evaluate: Average Metric: 2.4545454545454546 / 3 (81.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:12:05 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Proposed new text for predict: markdown\n",
      "# Bibliographic‑Metadata Extraction – Final Assistant Instructions (Revised)\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF file.  \n",
      "Its top‑level keys are:\n",
      "\n",
      "* `pdfinfo` – metadata extracted from the PDF (may contain `title`, `author`,\n",
      "  `creationDate`, `modDate`, …).  \n",
      "* `pages` – an ordered list of page objects, each with:\n",
      "  * `page` – page number (integer)  \n",
      "  * `text` – the OCR‑extracted plain‑text of that page (UTF‑8 string)\n",
      "\n",
      "Your job is to produce **exactly one JSON object** that follows the schema below.\n",
      "If a value cannot be determined, use the exact empty value specified\n",
      "(`null` for a missing string, `[]` for an empty list).\n",
      "\n",
      "> **All strings you output must be processed through the *Cleaning Rules* (see below).**  \n",
      "> **The fields must appear in the order shown in the schema.**  \n",
      "> **Do not output any extra text, markdown, or commentary.**  \n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema (field order is mandatory)\n",
      "\n",
      "| Field | Type | Required format / notes |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | `\"fi\"` for Finnish, `\"sv\"` for Swedish, otherwise `\"en\"`. Detect **using only the first 200 characters** of the concatenated page texts (preserve page order). Detection order: <br>1. If any **Swedish‑specific keyword** appears → `sv`. <br>2. Else if any **Finnish‑specific keyword** appears **and** any of the characters `ä ö Ä Ö å Å` are present → `fi`. <br>3. Otherwise → `en`. <br>**Swedish‑specific keywords** (case‑insensitive): `avhandling`, `examensarbete`, `språk: svenska`, `svenska`, `tidning`, `newspaper`, `press`, `magazine`. <br>**Finnish‑specific keywords** (case‑insensitive): `opinnäytetyö`, `ammattikorkeakoulu`, `tutkielma`. |\n",
      "| `title` | string | Main title of the work, **cleaned**. Preference order: 1. `pdfinfo.title` if it exists and is non‑empty after cleaning. 2. The first “prominent heading” on **page 1** (see *Title‑Finding Rules*). |\n",
      "| `alt_title` | list of strings | Any alternative titles. If the selected `title` contains a colon (`:`), treat the part **after** the colon as an alternative title (trimmed and cleaned). Also treat any text that appears inside double quotation marks (`\"`…`\"`) **or** single quotation marks (`‘`…`’`) **in the title line** as an alternative title. Return each alternative as a separate element; if none, return `[]`. |\n",
      "| `creator` | list of strings | Author(s) in **“Surname, Given‑Name”** order. Use `pdfinfo.author` if present; otherwise locate an author line in the text (see *Author‑Finding Rules*). Split multiple authors on commas, semicolons, the word “and”, ampersand `&`, or line breaks. For each token, remove trailing asterisks or footnote symbols, then reorder “First Last” (or “First Middle Last”) to “Last, First Middle”. Preserve diacritics. Return `[]` if no author can be identified. |\n",
      "| `year` | integer | Publication year. Extract the first four‑digit year (1900‑2099) from `pdfinfo.creationDate` or `pdfinfo.modDate`. If both are missing, search the **whole concatenated text** for the first such year. |\n",
      "| `publisher` | list of strings | Institution(s) that awarded the thesis. **Only include a publisher when `type_coar` is one of `doctoral thesis`, `master thesis` or `bachelor thesis`.** In that case, search the **whole concatenated text** for university / faculty / college names (see *Publisher Determination*). Return the first distinct institution(s) in the order they appear, after cleaning. For all other resource types return `[]`. |\n",
      "| `doi` | string or null | DOI **without** any URL prefix. Detect with case‑insensitive regex `10\\.\\d{4,9}/\\S+`. Strip surrounding whitespace and any trailing punctuation characters `.,;`. If the match appears as part of a URL (`https://doi.org/...`), keep only the DOI part. Return `null` if none found. |\n",
      "| `e_isbn` | list of strings | Electronic ISBN‑13 numbers (hyphen‑ and space‑free). Find all 13‑digit ISBNs (regex below). A match belongs to this list **only if** within **20 characters before or after** the match there is a cue word from the *electronic* cue list (`digital`, `electronic`, `e‑isbn`, `(digital)`). If both electronic and print cues appear, add the number to **both** lists. Remove duplicates. |\n",
      "| `p_isbn` | list of strings | Print ISBN‑13 numbers (hyphen‑ and space‑free). Same detection as `e_isbn` but the surrounding cue must be from the *print* cue list (`print`, `paper`, `hardcover`, `(print)`). |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, hyphen‑free). Same cue logic as `e_isbn` with the *electronic* cue list. Return `null` if none. |\n",
      "| `p_issn` | string or null | Print ISSN (8 digits, hyphen‑free). Same cue logic as `p_isbn` with the *print* cue list. Return `null` if none. |\n",
      "| `type_coar` | string | COAR‑compatible resource type, **lower‑case**. Detect in this order (first match wins): <br>1. **doctoral thesis** – any of `doctoral thesis`, `dissertation`, `PhD`, `doctoral`, `väitöskirja`. <br>2. **master thesis** – any of `master’s thesis`, `master thesis`, `maisteri`, `maisterintutkielma`. <br>3. **bachelor thesis** – any of `bachelor thesis`, `bachelor’s thesis`, `bachelor`, `opinnäytetyö`. <br>4. **journal article** – presence of a journal‑style citation (journal name + volume/issue/pages) **or** the word “article” together with typical citation fields (`volume`, `pages`, `doi`). <br>5. **conference proceeding** – any of `conference`, `proceedings`, `paper presented at`. <br>6. **newspaper article** – any of `tidning`, `newspaper`, `press`, `magazine`, `avhandling` **when not matching any thesis pattern**. <br>7. **research** – any other research report or article. |\n",
      "| `type_coar` must be one of the strings listed above (all lower‑case). |\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning Rules (apply to **every** string you output)\n",
      "\n",
      "1. **Trim** leading and trailing whitespace.  \n",
      "2. **Collapse** any sequence of whitespace characters (space, tab, newline) to a single space.  \n",
      "3. **Normalize quotation marks**: replace curly double quotes (`“”`) and single quotes (`‘’`) with plain `\"` and `'` respectively; then replace any remaining single quotes that are used as apostrophes with `’`.  \n",
      "4. **Remove Markdown formatting**: strip leading heading markers (`#`, `##`, `###`, etc.), surrounding asterisks `*` or underscores `_`, and surrounding double‑asterisks `**`.  \n",
      "5. **Remove trailing asterisks** that are used as footnote symbols (e.g., `Magnusson*`).  \n",
      "6. **Leave diacritics** (ä, ö, å, etc.) untouched.  \n",
      "\n",
      "All cleaning is performed **after** a value has been extracted but **before** it is placed in the output JSON.\n",
      "\n",
      "---\n",
      "\n",
      "## Title‑Finding Rules (used when `pdfinfo.title` is absent)\n",
      "\n",
      "1. Concatenate the `text` of **page 1** and split it into lines preserving order.  \n",
      "2. Discard empty lines and lines whose length after trimming is < 6 characters.  \n",
      "3. Discard lines that start with markdown heading markers (`#`, `##`, `###`).  \n",
      "4. The **first remaining line** is taken as the title candidate.  \n",
      "5. Apply the *Cleaning Rules* to this line.  \n",
      "6. If the cleaned line contains a colon (`:`), keep the whole string as `title` **and** store the part after the colon (trimmed) as an element of `alt_title` (the colon‑derived alternative is also handled by the generic `alt_title` step).  \n",
      "\n",
      "*Do not* treat lines that look like “Author”, “Tekijä”, “Författare”, “Ansvarig” etc. as titles.\n",
      "\n",
      "---\n",
      "\n",
      "## Author‑Finding Rules (used when `pdfinfo.author` is absent)\n",
      "\n",
      "1. Examine the first **three** pages. Collect any line that satisfies **any** of the following:  \n",
      "   * Starts with one of the exact prefixes (case‑insensitive): `By `, `Author:`, `Tekijä`, `Författare:`, `Author`, `Tekijä:`.  \n",
      "   * Contains the word **and** / **&** between two capitalised words (e.g., `John Doe and Jane Smith`).  \n",
      "   * Consists mainly of capitalised words (≥ 2 words, each starting with an uppercase letter) **and** appears within the first 25 % of the page’s lines.  \n",
      "2. From each candidate line, split on commas, semicolons, the word “and”, ampersand `&`, or line breaks.  \n",
      "3. For each token, remove trailing asterisks or footnote symbols.  \n",
      "4. Detect the pattern “First Last” (or “First Middle Last”). Reorder to “Last, First Middle”. If a token already matches “Last, First” keep it unchanged.  \n",
      "5. Discard tokens that are clearly not personal names (e.g., contain words like `Ansvarig`, `Redaktionsråd`, `Layout`, `Editor`, `Publisher`).  \n",
      "6. Return the list of reordered names; if none can be parsed, return `[]`.  \n",
      "\n",
      "If `pdfinfo.author` is present, split it using the same delimiters, clean each name, and apply step 4.\n",
      "\n",
      "---\n",
      "\n",
      "## Publisher Determination (only for theses)\n",
      "\n",
      "1. **Eligibility** – Only run this step when `type_coar` is `doctoral thesis`, `master thesis` or `bachelor thesis`.  \n",
      "2. Scan the **whole concatenated text** for institution names. Use the following **keyword list** (case‑insensitive) to recognise an institution:  \n",
      "\n",
      "   ```\n",
      "   university, universität, université, universitet, akademi, institute,\n",
      "   college, school, faculty, ammattikorkeakoulu, yliopisto,\n",
      "   universitet, université, läroanstalt, högskola, högskolan,\n",
      "   yrkeshögskola, yrkeshögskolan, polytechnic, polytechnique\n",
      "   ```\n",
      "\n",
      "3. When a keyword is found, capture the **full phrase** that contains it, extending left and right until a line break or a punctuation character (`.,;:!?`) that is *not* part of the phrase.  \n",
      "4. Apply the *Cleaning Rules* to the captured phrase.  \n",
      "5. Return the **first distinct** institution(s) in the order they appear. If a department/faculty name appears **before** the university name, keep both (e.g., `Department of Nursing, Laurea University of Applied Sciences`).  \n",
      "6. If no institution is found, return `[]`.  \n",
      "\n",
      "*Do not* include publisher information for non‑thesis types.\n",
      "\n",
      "---\n",
      "\n",
      "## DOI Extraction Details\n",
      "\n",
      "* Regex (case‑insensitive): `(?i)10\\.\\d{4,9}/\\S+`  \n",
      "* After a match, strip any trailing punctuation characters `.,;` and surrounding whitespace.  \n",
      "* If the match is part of a URL (`http://doi.org/...` or `https://doi.org/...`), keep only the DOI portion (the part after the last slash).  \n",
      "* Return `null` if no DOI is found.\n",
      "\n",
      "---\n",
      "\n",
      "## ISBN / ISSN Extraction Details\n",
      "\n",
      "* **ISBN‑13 regex** (matches 13‑digit ISBNs with optional hyphens/spaces):  \n",
      "\n",
      "  ```\n",
      "  \\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b\n",
      "  ```\n",
      "\n",
      "* **ISSN regex** (8 digits, optional hyphen):  \n",
      "\n",
      "  ```\n",
      "  \\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b\n",
      "2025/09/30 10:12:14 INFO dspy.evaluate.evaluate: Average Metric: 2.515151515151515 / 3 (83.8%)\n",
      "2025/09/30 10:12:54 INFO dspy.evaluate.evaluate: Average Metric: 39.192929292929286 / 64 (61.2%)\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Full valset score for new program: 0.6123895202020202\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Full train_val score for new program: 0.6123895202020202\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Individual valset scores for new program: [0.8181818181818182, 0.5454545454545454, 0.5757575757575757, 0.36363636363636365, 0.6363636363636364, 0.36363636363636365, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 1.0, 0.6363636363636364, 0.6060606060606061, 0.7272727272727273, 0.45454545454545453, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.7727272727272727, 0.7272727272727273, 0.7272727272727273, 0.5454545454545454, 0.4454545454545455, 0.8181818181818182, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.7272727272727273, 0.5151515151515151, 0.36363636363636365, 0.6363636363636364, 0.36363636363636365, 0.9090909090909091, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.8181818181818182, 0.6363636363636364, 0.36363636363636365, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.36363636363636365, 0.6363636363636364, 0.6363636363636364, 0.36363636363636365, 0.45454545454545453, 0.5454545454545454, 0.6666666666666666, 0.6363636363636364, 0.7777777777777778, 0.5454545454545454, 0.5, 0.696969696969697]\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 1.0, 0.6363636363636364, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.5454545454545454, 0.7272727272727273, 0.7636363636363637, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 1.0, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.45454545454545453, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Full valset pareto front score: 0.7449179292929293\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Updated valset pareto front programs: [{2, 8, 9, 14, 15}, {12, 7}, {0}, {9}, {8}, {0, 3, 13}, {0, 1, 2, 9}, {0, 12}, {3}, {2, 13}, {15}, {15}, {13}, {13, 14, 15}, {9, 6}, {1, 3, 12, 14}, {1, 10, 9, 7}, {8, 9}, {4}, {1, 2, 9, 15}, {9}, {4}, {9}, {2}, {9}, {3}, {7, 12, 13, 14, 15}, {2}, {8, 9, 3}, {12, 14}, {1}, {8}, {2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14}, {12}, {4}, {2}, {3}, {12, 15}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 14, 9, 7}, {8, 3, 13, 14}, {14}, {11}, {9}, {0, 2, 11}, {7}, {8}, {0, 7, 9, 10, 12, 14, 15}, {0, 12}, {9}, {9, 14, 7}, {8}, {8, 9, 2, 6}, {0}, {9, 15, 14, 7}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Linear pareto front program index: 9\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 24: New program candidate index: 15\n",
      "GEPA Optimization:  79%|███████▉  | 1168/1483 [54:39<20:10,  3.84s/rollouts]2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 25: No merge candidates found\n",
      "2025/09/30 10:12:54 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Selected program 15 score: 0.6123895202020202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.82 / 3 (60.6%): 100%|██████████| 3/3 [00:09<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:13:03 INFO dspy.evaluate.evaluate: Average Metric: 1.8181818181818183 / 3 (60.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:14:55 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Proposed new text for predict: markdown\n",
      "# Bibliographic‑Metadata Extraction – Final Assistant Instructions (Authoritative)\n",
      "\n",
      "You will be given **one JSON object** that represents a PDF file.  \n",
      "Its top‑level keys are:\n",
      "\n",
      "* `pdfinfo` – a dictionary of PDF‑level metadata (may contain `title`,\n",
      "  `author`, `creationDate`, `modDate`, …).  \n",
      "* `pages` – an ordered list of page dictionaries, each with:\n",
      "  * `page` – page number (integer)  \n",
      "  * `text` – the OCR‑extracted plain‑text of that page (UTF‑8 string)\n",
      "\n",
      "Your task is to produce **exactly one JSON object** that follows the schema\n",
      "below.  Every field must appear **in the order shown**.  If a value cannot be\n",
      "determined, use the exact empty value specified (`null` for a missing string,\n",
      "`[]` for an empty list).\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Cleaning Rules (apply to **every** string you output)\n",
      "\n",
      "1. **Trim** leading and trailing whitespace.  \n",
      "2. **Collapse** any run of whitespace characters (space, tab, newline) to a single space.  \n",
      "3. **Normalize quotation marks**:  \n",
      "   * replace curly double quotes (`“”`) with plain `\"`;  \n",
      "   * replace curly single quotes (`‘’`) with plain `'`;  \n",
      "   * then replace any remaining single quote that functions as an apostrophe with the typographic apostrophe `’`.  \n",
      "4. **Strip Markdown formatting**:  \n",
      "   * remove leading heading markers (`#`, `##`, `###`, …);  \n",
      "   * remove surrounding asterisks `*` or underscores `_`;  \n",
      "   * remove surrounding double‑asterisks `**`.  \n",
      "5. **Remove footnote asterisks** that appear at the end of a word (e.g. `Magnusson*`).  \n",
      "6. **Leave diacritics** (ä, ö, å, etc.) untouched.\n",
      "\n",
      "Cleaning is performed **after** a value has been extracted but **before** it is placed in the output JSON.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Output JSON Schema (field order is mandatory)\n",
      "\n",
      "| Field | Type | Extraction / Formatting Details |\n",
      "|-------|------|---------------------------------|\n",
      "| `language` | string | Detect **using only the first 200 characters** of the concatenated page texts (preserve page order). Detection order: <br>1. If any **Swedish‑specific keyword** (case‑insensitive) appears → `sv`. <br>2. Else if any **Finnish‑specific keyword** appears **and** any of the characters `ä ö Ä Ö å Å` are present → `fi`. <br>3. Otherwise → `en`. <br>**Swedish‑specific keywords**: `avhandling`, `examensarbete`, `språk: svenska`, `svenska`, `tidning`, `newspaper`, `press`, `magazine`. <br>**Finnish‑specific keywords**: `opinnäytetyö`, `ammattikorkeakoulu`, `tutkielma`. |\n",
      "| `title` | string | Preferred source order: <br>1. `pdfinfo.title` if it exists **and** is non‑empty after cleaning. <br>2. If not, use the **first “prominent heading” on page 1** (see *Title‑Finding Rules*). The whole cleaned line is the title (do **not** truncate at a colon). |\n",
      "| `alt_title` | list of strings | • If the selected `title` contains a colon (`:`), treat the part **after** the colon (trimmed and cleaned) as an alternative title. <br>• Also treat any text that appears inside double quotation marks (`\"…\"`) **or** single quotation marks (`‘…’`) **in the title line** as separate alternative titles. Return `[]` if none. |\n",
      "| `creator` | list of strings | Authors in “Surname, Given‑Name” order. <br>1. If `pdfinfo.author` is present, split it on commas, semicolons, the word “and”, ampersand `&`, or line breaks. <br>2. Otherwise locate author lines using the *Author‑Finding Rules* (see below). <br>3. For each token: remove trailing asterisks/footnote symbols, then **reorder** “First Last” (or “First Middle Last”) to “Last, First Middle”. If the token already matches “Last, First” keep it unchanged. <br>4. Discard tokens that contain any of the exclusion words: `Ansvarig`, `Redaktionsråd`, `Layout`, `Editor`, `Publisher`, `Redaktor`, `Leitung`, `Responsable`. <br>Return `[]` if no author can be identified. |\n",
      "| `year` | integer | Extract the **first** four‑digit year (1900‑2099) from `pdfinfo.creationDate` or `pdfinfo.modDate` (in that order). If both are missing, search the **whole concatenated text** for the first such year. |\n",
      "| `publisher` | list of strings | **Only** when `type_coar` is `doctoral thesis`, `master thesis` or `bachelor thesis`. Scan the whole concatenated text for institution names (see *Publisher Determination*). Return the first distinct institution(s) in the order they appear, after cleaning. Return `[]` for all other resource types. |\n",
      "| `doi` | string or null | Detect with case‑insensitive regex `10\\.\\d{4,9}/\\S+`. <br>• Strip surrounding whitespace and any trailing punctuation `.,;`. <br>• If the match appears inside a URL (`http://doi.org/...` or `https://doi.org/...`), keep only the DOI part (the part after the final slash). <br>Return `null` if none found. |\n",
      "| `e_isbn` | list of strings | Electronic ISBN‑13 numbers **without** hyphens or spaces. Find all 13‑digit ISBNs using the regex `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`. A match belongs to this list **only if** within **20 characters before or after** the match there is an *electronic cue word*: `digital`, `electronic`, `e‑isbn`, `(digital)`. If both electronic and print cues are present, the same number may appear in both lists. Remove hyphens/spaces, deduplicate, and keep the order of first appearance. |\n",
      "| `p_isbn` | list of strings | Print ISBN‑13 numbers, same detection as `e_isbn` but the surrounding cue must be a *print cue word*: `print`, `paper`, `hardcover`, `(print)`. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, hyphen‑free). Detect with regex `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b` and the *electronic* cue list (same 20‑character window as ISBN). Return `null` if none. |\n",
      "| `p_issn` | string or null | Print ISSN, same detection as `e_issn` but using the *print* cue list. |\n",
      "| `type_coar` | string | COAR‑compatible resource type (lower‑case). Detect **in the order below**; the first match wins: <br>1. **doctoral thesis** – any of `doctoral thesis`, `dissertation`, `phd`, `doctoral`, `väitöskirja`. <br>2. **master thesis** – any of `master’s thesis`, `master thesis`, `maisteri`, `maisterintutkielma`. <br>3. **bachelor thesis** – any of `bachelor thesis`, `bachelor’s thesis`, `bachelor`, `opinnäytetyö`. <br>4. **journal article** – presence of a journal‑style citation (journal name + volume/issue/pages) **or** the word “article” together with typical citation fields (`volume`, `pages`, `doi`). <br>5. **conference proceeding** – any of `conference`, `proceedings`, `paper presented at`. <br>6. **newspaper article** – any of `tidning`, `newspaper`, `press`, `magazine`, `avhandling` **when not matching any thesis pattern**. <br>7. **research** – any other research report or article. <br>Return exactly one of the strings listed above. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Title‑Finding Rules (used only when `pdfinfo.title` is absent)\n",
      "\n",
      "1. Take the `text` of **page 1**, split it into lines (preserve order).  \n",
      "2. Discard empty lines and lines whose length after trimming is < 6 characters.  \n",
      "3. Discard lines that start with markdown heading markers (`#`, `##`, `###`, …).  \n",
      "4. The **first remaining line** is the title candidate.  \n",
      "5. Apply the *Cleaning Rules* to this line.  \n",
      "6. The cleaned line is the **full title** (do **not** split at a colon; the colon handling is performed later in the `alt_title` step).  \n",
      "\n",
      "*Do not* treat lines that are obvious “Author”, “Tekijä”, “Författare”, etc. as titles.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Author‑Finding Rules (used only when `pdfinfo.author` is absent)\n",
      "\n",
      "1. Examine the first **three** pages. For each line, check if **any** of the following holds:  \n",
      "   * The line starts (case‑insensitive) with one of the exact prefixes: `By `, `Author:`, `Tekijä`, `Författare:`, `Author`, `Tekijä:`.  \n",
      "   * The line contains the word **and** or **&** between two capitalised words (e.g., `John Doe and Jane Smith`).  \n",
      "   * The line consists mainly of capitalised words (≥ 2 words, each starting with an uppercase letter) **and** appears within the first 25 % of that page’s lines.  \n",
      "2. Collect all matching lines. For each line, split on commas, semicolons, the word “and”, ampersand `&`, or line breaks.  \n",
      "3. For each token: remove trailing asterisks/footnote symbols, trim, then **reorder** “First Last” (or “First Middle Last”) to “Last, First Middle”. If the token already matches “Last, First” keep it unchanged.  \n",
      "4. Discard any token that contains any of the exclusion words listed in the `creator` field description.  \n",
      "5. Return the list of reordered names (preserve the order of first appearance). Return `[]` if none.\n",
      "\n",
      "If `pdfinfo.author` **is** present, split it using the same delimiters, clean each name, and apply step 4.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Publisher Determination (only for theses)\n",
      "\n",
      "1. **Eligibility** – Execute this step only when `type_coar` is `doctoral thesis`, `master thesis` or `bachelor thesis`.  \n",
      "2. Scan the **whole concatenated text** for institution‑related keywords (case‑insensitive):  \n",
      "\n",
      "   ```\n",
      "   university, universität, université, universitet, akademi, institute,\n",
      "   college, school, faculty, ammattikorkeakoulu, yliopisto,\n",
      "   universitet, université, läroanstalt, högskola, högskolan,\n",
      "   yrkeshögskola, yrkeshögskolan, polytechnic, polytechnique\n",
      "   ```\n",
      "\n",
      "3. When a keyword is found, capture the **full phrase** that contains it, extending left and right until a line break or a punctuation character (`.,;:!?`) that is *not* part of the phrase.  \n",
      "4. Apply the *Cleaning Rules* to the captured phrase.  \n",
      "5. Return the **first distinct institution(s)** in the order they appear. If a department/faculty name appears before the university name, keep both (e.g., `Department of Nursing, Laurea University of Applied Sciences`).  \n",
      "6. If no institution is found, return `[]`.  \n",
      "\n",
      "Do **not** include any publisher information for non‑thesis resource types.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. General Processing Flow (summary)\n",
      "\n",
      "1. **Concatenate** all `pages[].text` in page order to a single string `full_text`.  \n",
      "2. **Language** ← apply language detection on the first 200 characters of `full_text`.  \n",
      "3. **Title** ← `pdfinfo.title` (cleaned) if non‑empty, else apply Title‑Finding Rules on page 1.  \n",
      "4. **Alt Title** ← derive from `title` using the colon and quotation‑mark rules.  \n",
      "5. **Creator** ← if `pdfinfo.author` exists, split and clean; else apply Author‑Finding Rules.  \n",
      "6. **Year** ← first 4‑digit year from `pdfinfo.creationDate`, else `pdfinfo.modDate`, else from `full_text`.  \n",
      "7. **Type_coar** ← run the detection list in order on `full_text` (and on `pdfinfo.title`/`author` if helpful).  \n",
      "8. **Publisher** ← run Publisher Determination *only* if `type_coar` is a thesis type.  \n",
      "9. **DOI** ← extract with the DOI regex and post‑process as described.  \n",
      "10. **ISBN / ISSN** ← locate numbers with the respective regexes, check the 20‑character cue windows, clean (remove hyphens/spaces), deduplicate, and assign to the appropriate electronic/print lists.  \n",
      "11. **Apply Cleaning Rules** to every string value before placing it in the output JSON.  \n",
      "12. **Assemble** the JSON object following the exact field order.  \n",
      "\n",
      "---\n",
      "\n",
      "## 7. Edge‑Case Handling\n",
      "\n",
      "* If multiple DOI‑like strings are found, return the **first** one.  \n",
      "* If an ISBN appears with both electronic and print cues, include it in **both** `e_isbn` and `p_isbn`.  \n",
      "* For `alt_title`, if the title contains **both** a colon and quoted material, treat each part as a separate alternative title (after cleaning).  \n",
      "* All numeric identifiers (ISBN, ISSN) must be output **without** hyphens or spaces.  \n",
      "* When extracting years, ignore any 4‑digit numbers outside the 1900‑2099 range.  \n",
      "\n",
      "---\n",
      "\n",
      "**Remember:**  \n",
      "* Follow the exact order of fields.  \n",
      "* Use the exact empty representations (`null`, `[]`).  \n",
      "* No extra text, markdown, or commentary may appear outside the JSON object.  \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"...\",\n",
      "  \"title\": \"...\",\n",
      "  \"alt_title\": [...],\n",
      "  \"creator\": [...],\n",
      "  \"year\": 0,\n",
      "  \"publisher\": [...],\n",
      "  \"doi\": \"...\",\n",
      "  \"e_isbn\": [...],\n",
      "  \"p_isbn\": [...],\n",
      "  \"e_issn\": \"...\",\n",
      "  \"p_issn\": \"...\",\n",
      "  \"type_coar\": \"...\"\n",
      "}\n",
      "2025/09/30 10:15:06 INFO dspy.evaluate.evaluate: Average Metric: 1.9090909090909092 / 3 (63.6%)\n",
      "2025/09/30 10:15:43 INFO dspy.evaluate.evaluate: Average Metric: 38.41414141414139 / 64 (60.0%)\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Full valset score for new program: 0.6002209595959596\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Full train_val score for new program: 0.6002209595959596\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Individual valset scores for new program: [0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.36363636363636365, 0.7272727272727273, 0.36363636363636365, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.45454545454545453, 1.0, 0.36363636363636365, 0.7878787878787878, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.8181818181818182, 0.45454545454545453, 0.45454545454545453, 0.5454545454545454, 0.45454545454545453, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.45454545454545453, 0.36363636363636365, 0.7272727272727273, 0.5151515151515151, 0.8181818181818182, 1.0, 0.9090909090909091, 0.45454545454545453, 0.36363636363636365, 0.45454545454545453, 0.45454545454545453, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.45454545454545453, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.45454545454545453, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.18181818181818182, 0.6666666666666666, 0.696969696969697, 0.6262626262626263, 0.45454545454545453, 0.5151515151515151, 0.696969696969697]\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 1.0, 0.6363636363636364, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 1.0, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.6666666666666666, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Full valset pareto front score: 0.7486111111111111\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Updated valset pareto front programs: [{2, 8, 9, 14, 15, 16}, {12, 7}, {0}, {9}, {8}, {0, 3, 13}, {0, 1, 2, 9}, {0, 12}, {3}, {2, 13}, {16, 15}, {15}, {13}, {13, 14, 15}, {16, 9, 6}, {1, 3, 12, 14}, {16}, {8, 9}, {16}, {1, 2, 9, 15}, {9}, {4}, {9}, {2}, {9}, {3}, {7, 12, 13, 14, 15}, {2}, {8, 9, 3}, {12, 14}, {1}, {8, 16}, {16, 2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14}, {12}, {4}, {2}, {3}, {12, 15}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 14, 9, 7}, {8, 3, 13, 14}, {14}, {11}, {9}, {0, 2, 11}, {7}, {8}, {16}, {0, 12}, {9}, {9, 14, 7}, {8, 16}, {8, 9, 2, 6}, {0}, {7, 9, 14, 15, 16}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Linear pareto front program index: 9\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 25: New program candidate index: 16\n",
      "GEPA Optimization:  83%|████████▎ | 1238/1483 [57:28<13:10,  3.23s/rollouts]2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 26: No merge candidates found\n",
      "2025/09/30 10:15:43 INFO dspy.teleprompt.gepa.gepa: Iteration 26: Selected program 3 score: 0.5776515151515151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.55 / 3 (51.5%): 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:15:49 INFO dspy.evaluate.evaluate: Average Metric: 1.5454545454545454 / 3 (51.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:17:16 INFO dspy.teleprompt.gepa.gepa: Iteration 26: Proposed new text for predict: markdown\n",
      "# Task: Structured Metadata Extraction from PDF‑Text JSON\n",
      "\n",
      "You will receive **one JSON object** with the following top‑level keys:\n",
      "\n",
      "* **pdfinfo** – a dictionary containing the PDF’s internal metadata (e.g. `title`, `author`, `creationDate`, …).  \n",
      "* **pages** – an ordered list of pages. Each page is a dictionary with:\n",
      "  * `page` – the page number (integer, 1‑based)  \n",
      "  * `text` – the plain‑text extracted from that page (string, may contain line‑breaks, markdown‑style headings, etc.)\n",
      "\n",
      "Your job is to **produce ONE JSON object** that contains **exactly** the fields listed in the table below (order does not matter).  \n",
      "All values must follow the indicated type. If a value cannot be determined, use `null` for scalar fields or an empty list `[]` for list fields.\n",
      "\n",
      "| Field | Type | Extraction rules (detailed) |\n",
      "|-------|------|----------------------------|\n",
      "| **language** | string (ISO‑639‑1) | Detect the primary language of the *body* of the document (majority of pages). Count occurrences of the stop‑word lists below (case‑insensitive). Choose the language with the highest count. **Never default to Finnish**; if there is a tie or no matches, default to `\"en\"` (English).<br><br>**Stop‑word lists** (use only whole‑word matches):<br>• Finnish: `ja`, `on`, `että`, `mutta`, `tai`, `niin`, `kun`, `se`, `tässä`, `että`<br>• English: `the`, `and`, `of`, `to`, `in`, `for`, `with`, `that`, `as`, `is`<br>• Swedish: `och`, `att`, `är`, `för`, `med`, `på`, `det`, `som`, `inte`, `han` |\n",
      "| **title** | string | 1. Look at **page 1**. Scan the lines from top to bottom.<br>2. The first non‑empty line that **looks like a heading** is the candidate title. A heading is any line that: <br>&nbsp;&nbsp;• is in title‑case, ALL‑CAPS, or surrounded by blank lines; <br>&nbsp;&nbsp;• may be prefixed by a series identifier (e.g. `ePooki 34/2020`, `ACTA 1075`).<br>3. If a series identifier is present, **strip everything up to and including the first space after the identifier**.<br>4. Trim surrounding whitespace, **keep the whole line including any colon**.<br>5. If the exact same line (ignoring case and surrounding whitespace) appears later, keep the first occurrence.<br>6. The final title must contain the subtitle after a colon **if the colon is part of the heading line** (do **not** move text from a later line into the title). |\n",
      "| **alt_title** | list of strings | Search the whole document for lines that are **distinct** from the main title but appear to be a subtitle, translation or alternative wording. Include a line only if it meets at least one of the following cues:<br>• The main title line contains a colon; the text **after** the colon on the *same line* is a subtitle → add that text (trim whitespace).<br>• A line on page 2‑3 that is a short heading (≤ 6 words) and is either a translation of the title (same words in a different language) **or** appears in brackets/parentheses right after the title.<br>• A line explicitly marked as “Subtitle”, “Alternative title”, or similar.<br>Collect each unique alternative title (preserve original capitalization, trim whitespace). **Do not** include generic placeholder strings such as “Age‑appropriate sexual education …”. If none are found, return `[]`. |\n",
      "| **creator** | list of strings | Locate the author(s) on page 1, typically directly under the title. Authors may be separated by commas, the word “and”, or line breaks. For each name:<br>1. Trim whitespace.<br>2. If the name is in “Firstname Lastname” order, convert to `\"Lastname, Firstname\"` (preserve all given names).<br>3. If the name already contains a comma, assume it is already “Lastname, Firstname”.<br>4. Preserve the order of appearance, remove exact duplicates (case‑insensitive). |\n",
      "| **year** | integer | 1. Prefer the four‑digit year extracted from `pdfinfo.creationDate` using the pattern `D:YYYY…` (YYYY between 1900‑2099).<br>2. If that is missing or ambiguous, scan the **first three pages** for a four‑digit year (1900‑2099) – take the first one found.<br>3. Return the year as an integer. If no year can be found, return `null`. |\n",
      "| **publisher** | list of strings | Scan the whole document (especially the title page and sections titled “Metatiedot”, “Acknowledgements”, “Funding”, etc.) for institutional or journal names. Accept a line as a publisher if it contains any of the keywords (case‑insensitive): `university`, `faculty`, `institute`, `college`, `academy`, `school`, `research`, `center`, `centre`, `department`, `oy`, `publishing`, `press`, `journal`, `magazine`, `conference`, `society`.<br>Return each distinct name **exactly as it appears** (trim surrounding whitespace), preserving the order of first appearance. If none are found, return `[]`. |\n",
      "| **doi** | string or null | Search the entire text for a DOI using the regex `10\\.\\d{4,9}/[-._;()/:A-Z0-9]+` (case‑insensitive). Return the **first** match **without surrounding whitespace**. If none, return `null`. |\n",
      "| **e_isbn** | list of strings | Find all ISBN occurrences with the pattern `ISBN\\s*[:=]?\\s*[\\d\\-\\s]+`. For each match:<br>1. Look at the surrounding text (up to 30 characters before and after). If it contains any of the cues `PDF`, `Online`, `e‑ISBN`, `electronic`, treat it as electronic. If no cue is present, still treat it as electronic (electronic is the default).<br>2. Strip the leading “ISBN” label, then remove **all** hyphens, spaces and other non‑digit characters, leaving only the digit string.<br>3. Keep the cleaned number in the list, preserving first‑appearance order, removing exact duplicates.<br>If no ISBN is found, return `[]`. |\n",
      "| **p_isbn** | list of strings | Same detection as `e_isbn` **but only keep numbers whose surrounding text contains any of the cues `Print`, `hardcover`, `paperback`, `p‑ISBN`, `Print ISBN`, `physical`**. Clean the numbers in the same way. If none, return `[]`. |\n",
      "| **e_issn** | string or null | Search for ISSN using the pattern `ISSN\\s*[:=]?\\s*\\d{4}-\\d{3}[0-9Xx]`. For each match, examine surrounding text (up to 30 characters on each side). If it contains any of the cues `Online`, `Electronic`, `e‑ISSN`, treat it as electronic; otherwise default to electronic. Return the **first** electronic ISSN **without the hyphen** (e.g. `17982022`). If none, return `null`. |\n",
      "| **p_issn** | string or null | Same pattern as `e_issn` but keep only those where surrounding text contains any of the cues `Print`, `Print ISSN`, `hardcopy`. Return the first such ISSN **without the hyphen**. If none, return `null`. |\n",
      "| **type_coar** | string (lower‑case) | Determine the COAR‑compatible type using the following hierarchy (first match wins). Search the whole document (case‑insensitive).<br>1. **doctoral thesis** – if the text contains any of: `Doctor of Science`, `Dissertation`, `Doctoral study`, `Doctor of Philosophy`, `PhD thesis`, `Licentiate`, `väitöskirja`.<br>2. **master thesis** – if it contains any of: `Master’s thesis`, `Master’s degree programme`, `opinnäytetyö`, `kandidaatintyö` **and not a doctoral cue**.<br>3. **bachelor thesis** – if it contains any of: `bachelor thesis`, `Kandidatavhandling`, `avhandling` **without master/doctoral qualifiers**.<br>4. **journal article** – if a DOI is present **or** the text contains patterns like `Vol. `, `Issue`, `Journal of`, `Proceedings of`, volume/issue numbers, or a journal name.<br>5. **research report** – if the document contains the words `Report`, `Research report`, `Technical report` and none of the previous categories matched.<br>6. **book** – if the document has ISBN/ISSN but none of the above cues, and it looks like a monograph (e.g., contains `Edited by`, `Chapter`, or a publisher name but no thesis/journal indicators.<br>7. **other** – if none of the above apply.<br>Return the exact lower‑case string (e.g. `\"doctoral thesis\"`). |\n",
      "| **alt_title** | (already defined) | – |\n",
      "| **creator**   | (already defined) | – |\n",
      "\n",
      "## General Extraction Strategy (for reference)\n",
      "\n",
      "1. **Year** – parse `pdfinfo.creationDate` first; fall back to page scan.  \n",
      "2. **Language** – count stop‑words on each page; ignore very short pages (< 30 words).  \n",
      "3. **Title** – locate first heading‑style line on page 1, strip series identifiers, keep colon.  \n",
      "4. **Alternative titles** – look for subtitles after a colon, translations on page 2‑3, bracketed titles.  \n",
      "5. **Authors** – extract names from page 1, split/reorder, dedupe.  \n",
      "6. **Publisher** – search for institutional keywords; keep exact spelling.  \n",
      "7. **Identifiers** – regex for DOI, ISBN, ISSN; classify electronic vs print by surrounding cues; clean numbers.  \n",
      "8. **COAR type** – apply hierarchy of keyword checks (doctoral > master > bachelor > journal > report > book > other).  \n",
      "\n",
      "## Output Format\n",
      "\n",
      "Return **only** the JSON object, no extra text, no comments. Example:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"fi\",\n",
      "  \"title\": \"Omistajaohjaus, hyvä hallintotapa ja muu valvonta : tutkimus suomalaisista osakeyhtiöistä\",\n",
      "  \"alt_title\": [\"Omowner Control, Good Governance and Supervision : A Study of Finnish Companies\"],\n",
      "  \"creator\": [\"Kankaanpää, Jari\", \"Laine, Matias\"],\n",
      "  \"year\": 2020,\n",
      "  \"publisher\": [\"Tampereen yliopisto\", \"AIP Publishing\"],\n",
      "  \"doi\": \"10.1063/1.4855095\",\n",
      "  \"e_isbn\": [\"9789520315788\"],\n",
      "  \"p_isbn\": [\"9789520315771\"],\n",
      "  \"e_issn\": \"24902028\",\n",
      "  \"p_issn\": \"24899860\",\n",
      "  \"type_coar\": \"doctoral thesis\"\n",
      "}\n",
      "2025/09/30 10:17:24 INFO dspy.evaluate.evaluate: Average Metric: 1.2424242424242424 / 3 (41.4%)\n",
      "2025/09/30 10:17:24 INFO dspy.teleprompt.gepa.gepa: Iteration 26: New subsample score is not better, skipping\n",
      "GEPA Optimization:  84%|████████▍ | 1244/1483 [59:09<15:35,  3.91s/rollouts]2025/09/30 10:17:24 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Selected program 0 score: 0.5962628517316018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:17:31 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:18:28 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Proposed new text for predict: **Task Overview**\n",
      "You will receive a JSON object that contains the extracted text of a PDF document (`pages` → list of `{ \"page\": <number>, \"text\": <string> }`).  \n",
      "Your job is to analyse this text and produce a set of bibliographic metadata fields in a **structured, machine‑readable format**.\n",
      "\n",
      "**Required Output Fields**\n",
      "Return the following fields **exactly** in the order shown, each on its own line, using valid Python‑style literals (strings in single quotes, lists in `[...]`, `None` for missing values). Do **not** add any extra text, headings, or explanations.\n",
      "\n",
      "```\n",
      "language: <ISO‑639‑2 code string, e.g. 'en' or 'fi'>\n",
      "title: <primary title string>\n",
      "alt_title: <list of alternative titles (may be empty) >\n",
      "creator: <list of author names formatted as \"LastName, FirstName MiddleName\" (may be empty)>\n",
      "year: <4‑digit integer year of publication (or None)>\n",
      "publisher: <list of publisher names (may be empty)>\n",
      "doi: <string DOI or None>\n",
      "e_isbn: <list of electronic ISBN strings (may be empty)>\n",
      "p_isbn: <list of print ISBN strings (may be empty)>\n",
      "e_issn: <string electronic ISSN or None>\n",
      "p_issn: <string print ISSN or None>\n",
      "type_coar: <lower‑case COAR type string, e.g. 'conference paper', 'journal article', 'master thesis', 'doctoral thesis', etc.>\n",
      "```\n",
      "\n",
      "**Field Extraction Rules**\n",
      "\n",
      "| Field | How to obtain | Normalisation |\n",
      "|-------|---------------|---------------|\n",
      "| **language** | Detect the dominant language of the document. Use `'fi'` for Finnish, `'en'` for English, etc. If ambiguous, prefer the language of the title. | ISO‑639‑2 two‑letter code. |\n",
      "| **title** | The main title appears on the first page (often in large font or preceded by `#`, `##`, etc.). If a subtitle follows a colon (`:`) or dash, keep the whole line as the title. | Preserve original capitalization and diacritics. |\n",
      "| **alt_title** | Any alternative title found, e.g. an English translation of a Finnish title, a subtitle separated by a colon, or a title line on a later page that differs from the primary title. Return each distinct alternative as a separate list element. | Same formatting as *title*. |\n",
      "| **creator** | Look for author names in: <br>• citation lines (e.g., “Korpela, A., Gustafsson, F., …”) <br>• title page “Author: …” <br>• PDF metadata (`pdfinfo.author`) if present. <br>Extract each name, split into surname and given names, then re‑format as `\"Surname, GivenNames\"` (preserve middle names/initials). Order should follow the order in the source. Remove duplicates. | Capitalise first letter of each name part; keep diacritics. |\n",
      "| **year** | Prefer the year appearing in the citation line or on the title page (often after the title or in a conference/journal reference). If the PDF metadata contains a creation/modification date, use that year only when no explicit publication year is found. Return as an integer. | 4‑digit year, e.g. `2019`. |\n",
      "| **publisher** | Identify the publishing entity: <br>• Conference proceedings name <br>• Journal name <br>• University / institute name <br>Collect each distinct name in a list, preserving original spelling. | Keep as a string; do not add “Publisher:” etc. |\n",
      "| **doi** | Search the whole text for a DOI pattern `10.\\d{4,9}/\\S+`. Return the first match as a plain string (no URL prefix). If none, return `None`. |\n",
      "| **e_isbn / p_isbn** | Detect ISBN‑13 or ISBN‑10 numbers (with or without hyphens). If the surrounding context contains words like “electronic”, “e‑ISBN”, assign to `e_isbn`; if it contains “print”, “p‑ISBN”, “hardcover”, assign to `p_isbn`. If the format is ambiguous, include the number in both lists. Return each as a plain string without spaces. |\n",
      "| **e_issn / p_issn** | Detect ISSN patterns `####-###X`. Use surrounding words (“electronic”, “online”) for `e_issn`, otherwise `p_issn`. Return as a plain string or `None`. |\n",
      "| **type_coar** | Determine the document type using clues: <br>• Contains conference name, “Proceedings”, “Conference Paper” → `conference paper` <br>• Appears in a journal (volume/issue, “Journal of …”) → `journal article` <br>• Thesis / dissertation wording (“Master’s Thesis”, “Doctoral Thesis”, “B.Sc. thesis”) → `master thesis`, `doctoral thesis`, etc. <br>Return the exact lower‑case term from the COAR vocabulary. |\n",
      "\n",
      "**General Guidelines**\n",
      "1. **Do not fabricate data** – if a field cannot be confidently extracted, return `None` (or an empty list where a list is required).\n",
      "2. **Preserve original characters** (accents, umlauts, etc.) – do not transliterate.\n",
      "3. **Avoid duplicates** in list fields.\n",
      "4. **Whitespace** – trim leading/trailing spaces from all extracted strings.\n",
      "5. **Order** – keep the order of authors and publishers as they appear in the source.\n",
      "6. **Robustness** – handle line‑breaks, markdown markers (`#`, `##`, `**`), and stray symbols; they should not appear in the final values unless they are part of the legitimate title or name.\n",
      "\n",
      "**Example Output Format**\n",
      "```\n",
      "language: 'fi'\n",
      "title: 'Demonstraatiolaitteisto vaaka-akselisten tuulivoimakonseptien havainnollistamiseen'\n",
      "alt_title: ['Demonstration equipment for illustrating horizontal wind power concepts']\n",
      "creator: ['Korpela, Aki', 'Gustafsson, Frans', 'Kohtala, Matti', 'Virtanen, Klaus']\n",
      "year: 2019\n",
      "publisher: ['Tampereen ammattikorkeakoulu']\n",
      "doi: None\n",
      "e_isbn: []\n",
      "p_isbn: []\n",
      "e_issn: None\n",
      "p_issn: None\n",
      "type_coar: 'conference paper'\n",
      "```\n",
      "\n",
      "Follow this exact structure for every input you receive.\n",
      "2025/09/30 10:18:33 INFO dspy.evaluate.evaluate: Average Metric: 2.6363636363636367 / 3 (87.9%)\n",
      "2025/09/30 10:19:02 INFO dspy.evaluate.evaluate: Average Metric: 39.09450404667795 / 64 (61.1%)\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Full valset score for new program: 0.6108516257293432\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Full train_val score for new program: 0.6108516257293432\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Individual valset scores for new program: [0.7272727272727273, 0.6363636363636364, 0.696969696969697, 0.45454545454545453, 0.7272727272727273, 0.36363636363636365, 0.7272727272727273, 0.7272727272727273, 0.8, 0.6363636363636364, 0.8181818181818182, 0.2727272727272727, 0.5757575757575757, 0.7878787878787878, 0.3896103896103896, 0.7272727272727273, 0.5454545454545454, 0.5454545454545454, 0.6060606060606061, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.6363636363636364, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.5363636363636364, 0.6363636363636364, 0.42424242424242425, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.5454545454545454, 0.45454545454545453, 0.6363636363636364, 0.3333333333333333, 0.9090909090909091, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.45454545454545453, 0.5454545454545454, 0.36363636363636365, 0.45454545454545453, 0.7272727272727273, 0.7272727272727273, 0.5454545454545454, 0.5151515151515151, 0.5454545454545454, 0.766798418972332, 0.5454545454545454, 0.6363636363636364, 0.2987012987012987, 0.5454545454545454, 0.6363636363636364]\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 1.0, 0.6363636363636364, 0.9090909090909091, 0.7878787878787878, 0.6363636363636364, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 1.0, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.766798418972332, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Full valset pareto front score: 0.7511226394378568\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Updated valset pareto front programs: [{2, 8, 9, 14, 15, 16}, {12, 7}, {0}, {9, 17}, {8}, {0, 3, 13}, {0, 1, 2, 9, 17}, {0, 12}, {3}, {2, 13}, {16, 15}, {15}, {13}, {17}, {16, 9, 6}, {1, 3, 12, 14}, {16}, {8, 9}, {16}, {1, 2, 9, 15}, {9}, {4}, {9}, {2}, {9}, {3}, {7, 12, 13, 14, 15, 17}, {2}, {8, 9, 3}, {12, 14}, {1}, {8, 16}, {16, 2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 17}, {12}, {4}, {2}, {3}, {17, 12, 15}, {8}, {4, 7}, {8}, {1, 9, 7}, {1, 14, 9, 7}, {8, 3, 13, 14}, {14}, {11}, {9}, {0, 2, 11}, {7}, {8}, {16}, {0, 12}, {9}, {9, 14, 7}, {8, 16, 17}, {8, 9, 2, 6}, {0}, {17}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Linear pareto front program index: 9\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 27: New program candidate index: 17\n",
      "GEPA Optimization:  89%|████████▊ | 1314/1483 [1:00:47<07:47,  2.76s/rollouts]2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 28: No merge candidates found\n",
      "2025/09/30 10:19:02 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Selected program 11 score: 0.4297254737221842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.82 / 3 (27.3%): 100%|██████████| 3/3 [00:09<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:19:11 INFO dspy.evaluate.evaluate: Average Metric: 0.8181818181818181 / 3 (27.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:21:18 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Proposed new text for predict: markdown\n",
      "# Task Overview\n",
      "You will receive a single JSON object that contains the raw text extracted from a PDF document.  \n",
      "Your job is to **extract a fixed set of bibliographic metadata** from the information found in:\n",
      "\n",
      "* `pdfinfo` – the PDF‑level metadata (may contain `title`, `author`, `creationDate`, `modDate`, `lang` etc.).\n",
      "* `pages` – a list of page objects, each with a `page` number and a `text` string (line‑breaks are preserved).\n",
      "\n",
      "Using **only** the data present in these two places, you must populate the output schema below.  \n",
      "If a field cannot be determined, use the exact placeholder shown ( `null` for scalar values, `[]` for lists).\n",
      "\n",
      "> **Important:** Do **not** guess values, fabricate identifiers, or add generic placeholders (e.g. “Plato’s Ideas – Reality”). Return *only* what you can locate explicitly.\n",
      "\n",
      "---\n",
      "\n",
      "## Output Schema (order must be preserved)\n",
      "\n",
      "| Field | Type | Description | Normalisation rules |\n",
      "|-------|------|-------------|---------------------|\n",
      "| `language` | string | ISO‑639‑1 two‑letter code of the document language (lower‑case). | • If `pdfinfo.lang` exists, use it.  <br>• Else look for an explicit cue in the text such as `Kieli: suomi`, `Language: English`, `Sprache: Deutsch`, `Kieli: fi`, `Kieli: sv`, `Kieli: se`, etc.  <br>• If no explicit cue, infer from the dominant language using stop‑word lists (see “Language Detection” below). |\n",
      "| `title` | string | Exact main title of the work. | Prefer `pdfinfo.title`. If missing, take the most prominent heading on page 1 (usually the first line(s) that are all‑caps, centered, or preceded by `#`, `##`, `###`). Preserve every character exactly (including diacritics, punctuation, and trailing spaces – but **remove** any surrounding whitespace). |\n",
      "| `alt_title` | list of strings | Any subtitle, alternative title, or translation that is **separate** from the main title. | Add each alternative title exactly as it appears (strip surrounding whitespace only). Do **not** include parts that are already part of the main title. Only include if the document clearly marks it (e.g. “Subtitle: …”, a line in parentheses after the title, a line prefixed with “English title:”, etc.). |\n",
      "| `creator` | list of strings | Authors / editors / contributors. | • Gather names from `pdfinfo.author` and from any lines beginning with `Author(s):`, `Authors:`, `Tekijä:`, `Kirjoittaja:`, `Käyttäjä:`, `Tekijät`, `Tekijä`, `Authors`, `Editors`, `Editör`, `Editörer`, `Editor(s):`, `Käsikirjoittaja`, etc. <br>• Names may appear as “First Last” or “Last, First”. Convert every name to the normalized form **`Surname, Given‑Name(s)`** (keep all given‑name parts, keep diacritics). <br>• Preserve the order of first appearance and remove duplicates. |\n",
      "| `year` | integer or null | Publication year. | Search (in order) `pdfinfo.creationDate`, `pdfinfo.modDate`, any line containing ©, “Copyright”, “© YYYY”, “Published YYYY”, “Year: YYYY”, or a publisher line that ends with a year. Choose the four‑digit year that clearly denotes the *publication* year (not the file‑creation year). If none found → `null`. |\n",
      "| `publisher` | list of strings | Publishing entities. | Look for lines starting with `Publisher:`, `Kustantaja:`, `Julkaisija:`, `Julkaisija`, `Kustantaja`, `Publishers`, `Publishers:`, `Julkaisija`, `Julkaisija`, `Kustantaja`, `Kustantaja`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Kustantaja`, `Julkaisija`, `Julkaisija`, `Kustantaja`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Kustantaja`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`, `Julkaisija`. <br>• The line may contain several entities separated by commas, semicolons or line breaks – treat each logical entity as a separate list element. <br>• Do **not** include the label itself (“Publisher:”). Preserve the exact spelling and diacritics. |\n",
      "| `doi` | string or null | Digital Object Identifier. | Find a pattern matching `10.\\d{4,9}/\\S+` (may be preceded by `DOI:`, `doi:`, or appear as a URL). Return the raw DOI **without** any URL prefix (`https://doi.org/`). If none → `null`. |\n",
      "| `e_isbn` | list of strings | ISBN(s) for the electronic/online version. | • Look for a line containing `ISBN` **and** an electronic qualifier such as `PDF`, `e‑ISBN`, `online`, `electronic`, `e‑`, `e‑`, `e‑`. <br>• Extract the numeric part (remove hyphens, spaces, and any surrounding punctuation). Keep the check digit (may be `X`). <br>• Return each ISBN as a plain string. If no such line → `[]`. |\n",
      "| `p_isbn` | list of strings | ISBN(s) for the print version. | • Look for a line containing `ISBN` **without** an electronic qualifier, or with a print qualifier such as `Print`, `hardcover`, `paperback`, `p‑ISBN`. <br>• Normalise exactly as for `e_isbn`. |\n",
      "| `e_issn` | string or null | ISSN for the electronic version. | Same rule as ISBN but for `ISSN`. Electronic qualifier required. Normalise to an 8‑digit string (remove hyphen). |\n",
      "| `p_issn` | string or null | ISSN for the print version. | Same rule as ISBN but for `ISSN`. Print qualifier required. Normalise to an 8‑digit string. |\n",
      "| `type_coar` | string | COAR‑compatible publication type (lower‑case). | Detect using keywords (case‑insensitive): <br>• **Doctoral thesis** – words: “Doctoral thesis”, “Dissertation”, “PhD thesis”, “Doctoral dissertation”. <br>• **Master's thesis** – words: “Master’s thesis”, “Master thesis”. <br>• **Research report** – words: “Report”, “Research report”, “Technical report”. <br>• **Journal article** – words: “Article”, “Journal article”, “Tidsskrift”, “Paper”, “Proceedings”. <br>• **Book chapter** – words: “Chapter”, “Book chapter”, “In:”. <br>• **Book** – words: “Book”, “Monograph”, “Volume”. <br>• If none match → `\"other\"`.\n",
      "\n",
      "---\n",
      "\n",
      "## Detailed Extraction Strategies\n",
      "\n",
      "### 1. Language Detection\n",
      "1. **Explicit cue** – Scan every page for a line that matches the regex `(?i)^(language|kieli|sprache|språk)\\s*[:=]\\s*([a-z]{2,})`.  \n",
      "   *If found, take the captured two‑letter code (lower‑case).*\n",
      "2. **ISO code in `pdfinfo`** – If `pdfinfo.lang` exists, use it (lower‑case, two letters).  \n",
      "3. **Stop‑word inference** – If still unknown, count occurrences of language‑specific stop‑words (a short list is provided below). Choose the language with the highest count, default to `\"en\"` if tie.\n",
      "\n",
      "| Language | Sample stop‑words |\n",
      "|----------|-------------------|\n",
      "| `fi` (Finnish) | `ja`, `on`, `että`, `mutta`, `tai`, `kun`, `niin`, `koska` |\n",
      "| `sv` (Swedish) | `och`, `att`, `är`, `men`, `eller`, `så`, `det`, `för` |\n",
      "| `se` (Northern Sami) | `ja`, `dat`, `mii`, `go`, `muhto`, `dássi`, `guovtte` |\n",
      "| `en` (English) | `the`, `and`, `of`, `in`, `to`, `for`, `with`, `that` |\n",
      "\n",
      "### 2. Title & Alt‑Title\n",
      "* **Main title** – Prefer `pdfinfo.title`. If absent, take the first non‑empty line on page 1 that is either:  \n",
      "  - preceded by one or more `#` markdown headings, or  \n",
      "  - fully capitalised, or  \n",
      "  - centered (i.e., surrounded by blank lines).  \n",
      "  Strip surrounding whitespace but keep all internal spacing and punctuation.\n",
      "* **Alternative titles** – Add any of the following if they appear:  \n",
      "  - A line directly after the main title that begins with a colon, dash, or is enclosed in parentheses.  \n",
      "  - A line prefixed with `Subtitle:`, `Alt‑title:`, `English title:`, `Original title:` etc.  \n",
      "  Do **not** split a subtitle that is already part of the main title (e.g., “Main Title: Subtitle” → only main title, no alt_title).\n",
      "\n",
      "### 3. Creator Normalisation\n",
      "1. Extract raw name strings.\n",
      "2. Trim surrounding whitespace.\n",
      "3. Detect ordering:\n",
      "   - If a comma is present → assume “Surname, First” already; keep as‑is.\n",
      "   - If no comma → assume “First Last”. Split on the last space to separate surname from given names.\n",
      "4. Re‑assemble as `Surname, Given‑Name(s)`.\n",
      "5. Preserve diacritics and original capitalisation.\n",
      "6. Remove duplicate entries while keeping first occurrence order.\n",
      "\n",
      "### 4. Publisher Extraction\n",
      "* Look for the label (case‑insensitive) `publisher`, `kustantaja`, `julkaisija`, `publishers`, `julkaisijat`, `julkaisija`, `julkaisija`, `julkaisija`, `julkaisija`.  \n",
      "* Capture everything after the label up to the end of the line.  \n",
      "* If the line contains multiple entities separated by commas, semicolons, or the word “and”, split them **only** at those delimiters.  \n",
      "* Trim whitespace from each entity.  \n",
      "* Do **not** include location details unless they are part of the official name (e.g., “Painosalama Oy, Turku, Finland” is a single entity).  \n",
      "\n",
      "### 5. Year Extraction\n",
      "* Regex patterns to try (in order):  \n",
      "  - `©\\s*(\\d{4})`  \n",
      "  - `Copyright\\s*©?\\s*(\\d{4})`  \n",
      "  - `Published\\s*(\\d{4})`  \n",
      "  - `Year[:=]\\s*(\\d{4})`  \n",
      "  - `\\b(\\d{4})\\b` that appears in the same line as a publisher or copyright notice.  \n",
      "* If multiple candidate years are found, pick the one that appears **closest** to the publisher line or the © line.  \n",
      "\n",
      "### 6. DOI Extraction\n",
      "* Pattern: `(?i)doi[:\\s]*\\s*(10\\.\\d{4,9}/\\S+)`  \n",
      "* Also accept URLs like `https?://doi\\.org/(10\\.\\d{4,9}/\\S+)`.  \n",
      "* Return the captured DOI **exactly** (no surrounding whitespace, no URL prefix).\n",
      "\n",
      "### 7. ISBN / ISSN Extraction & Classification\n",
      "1. **Identify the line** – Use regexes:\n",
      "   - `(?i)ISBN\\s*(?:PDF|e[-]?ISBN|online|electronic)?\\s*[:=]?\\s*([0-9X\\-\\s]+)`\n",
      "   - `(?i)ISBN\\s*(?:Print|p[-]?ISBN|hardcover|paperback)?\\s*[:=]?\\s*([0-9X\\-\\s]+)`\n",
      "   - Same for `ISSN`.\n",
      "2. **Determine electronic vs print** based on the qualifier captured (`PDF`, `e‑`, `online`, `electronic` → electronic; `Print`, `hardcover`, `paperback`, or no qualifier → print).\n",
      "3. **Normalise** – Remove every character that is not a digit or `X`. The resulting string is the identifier.\n",
      "4. **Collect** – Append to the appropriate list (`e_isbn`, `p_isbn`, `e_issn`, `p_issn`). If the same identifier appears both as electronic and print, include it in both lists as appropriate.\n",
      "\n",
      "### 8. COAR Type Determination\n",
      "Search the whole document (including `pdfinfo.title`) for the following keywords (case‑insensitive). Use the **first** matching category in the order listed below:\n",
      "\n",
      "1. **Doctoral thesis** – `doctoral thesis`, `phd thesis`, `dissertation`, `doctorate`.\n",
      "2. **Master's thesis** – `master's thesis`, `master thesis`.\n",
      "3. **Research report** – `research report`, `technical report`, `report`.\n",
      "4. **Journal article** – `journal article`, `article`, `tidsskrift`, `paper`, `proceedings`.\n",
      "5. **Book chapter** – `chapter`, `book chapter`, `in:` (when preceded by a book title).\n",
      "6. **Book** – `book`, `monograph`, `volume`.\n",
      "7. **Other** – if none of the above are found, set `\"other\"`.\n",
      "\n",
      "### 9. Handling Missing Data\n",
      "* If a field cannot be found after applying all the rules above, use the placeholder (`null` for scalar fields, `[]` for list fields).  \n",
      "* **Never** fabricate a value based on assumptions (e.g., do not infer an ISBN just because a publisher is present).\n",
      "\n",
      "### 10. Output Formatting\n",
      "* Return **exactly one** JSON object, no surrounding markdown, no explanatory text.  \n",
      "* Use double quotes for all strings.  \n",
      "* Ensure the key order matches the schema table above.  \n",
      "* Example of a correctly formatted empty‑field output:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"language\": \"en\",\n",
      "  \"title\": \"Sample Title\",\n",
      "  \"alt_title\": [],\n",
      "  \"creator\": [],\n",
      "  \"year\": null,\n",
      "  \"publisher\": [],\n",
      "  \"doi\": null,\n",
      "  \"e_isbn\": [],\n",
      "  \"p_isbn\": [],\n",
      "  \"e_issn\": null,\n",
      "  \"p_issn\": null,\n",
      "  \"type_coar\": \"other\"\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Quick Reference Regexes (you may copy‑paste)\n",
      "\n",
      "```regex\n",
      "# Language explicit cue\n",
      "(?i)^(language|kieli|sprache|språk)\\s*[:=]\\s*([a-z]{2,})$\n",
      "\n",
      "# DOI\n",
      "(?i)doi[:\\s]*\\s*(10\\.\\d{4,9}/\\S+)\n",
      "\n",
      "# ISBN line (electronic)\n",
      "(?i)ISBN\\s*(?:PDF|e[-]?ISBN|online|electronic)?\\s*[:=]?\\s*([0-9X\\-\\s]+)\n",
      "\n",
      "# ISBN line (print)\n",
      "(?i)ISBN\\s*(?:Print|p[-]?ISBN|hardcover|paperback)?\\s*[:=]?\\s*([0-9X\\-\\s]+)\n",
      "\n",
      "# ISSN line (electronic)\n",
      "(?i)ISSN\\s*(?:PDF|e[-]?ISSN|online|electronic)?\\s*[:=]?\\s*([0-9X\\-\\s]+)\n",
      "\n",
      "# ISSN line (print)\n",
      "(?i)ISSN\\s*(?:Print|p[-]?ISSN|hardcover|paperback)?\\s*[:=]?\\s*([0-9X\\-\\s]+)\n",
      "\n",
      "# Year patterns\n",
      "©\\s*(\\d{4})\n",
      "Copyright\\s*©?\\s*(\\d{4})\n",
      "Published\\s*(\\d{4})\n",
      "Year[:=]\\s*(\\d{4})\n",
      "2025/09/30 10:21:26 INFO dspy.evaluate.evaluate: Average Metric: 1.4545454545454546 / 3 (48.5%)\n",
      "2025/09/30 10:22:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:22:05 INFO dspy.evaluate.evaluate: Average Metric: 38.005050505050505 / 64 (59.4%)\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Full valset score for new program: 0.5938289141414141\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Full train_val score for new program: 0.5938289141414141\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Individual valset scores for new program: [0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.36363636363636365, 0.8181818181818182, 0.36363636363636365, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.2727272727272727, 0.7878787878787878, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.45454545454545453, 0.6060606060606061, 0.5454545454545454, 0.45454545454545453, 0.36363636363636365, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.5454545454545454, 0.2727272727272727, 0.7272727272727273, 0.36363636363636365, 0.9090909090909091, 0.6363636363636364, 0.7272727272727273, 0.5151515151515151, 0.42424242424242425, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.6363636363636364, 0.7272727272727273, 0.6363636363636364, 0.5454545454545454, 0.5, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.45454545454545453, 0.7272727272727273, 0.5454545454545454, 0.42424242424242425, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 0.6262626262626263, 0.45454545454545453, 0.696969696969697, 0.7272727272727273]\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 1.0, 0.6363636363636364, 0.9090909090909091, 0.7878787878787878, 0.6363636363636364, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 1.0, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.766798418972332, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Full valset pareto front score: 0.7511226394378568\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Updated valset pareto front programs: [{2, 8, 9, 14, 15, 16}, {18, 12, 7}, {0}, {9, 17}, {8}, {0, 3, 13}, {0, 1, 2, 9, 17, 18}, {0, 18, 12}, {3}, {2, 13}, {16, 15}, {15}, {13}, {17}, {16, 9, 18, 6}, {1, 3, 12, 14}, {16}, {8, 9}, {16}, {1, 2, 9, 15}, {9}, {4}, {9}, {2}, {9}, {3}, {7, 12, 13, 14, 15, 17, 18}, {2}, {8, 9, 3}, {12, 14}, {1}, {8, 16}, {16, 2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 17}, {12}, {4}, {2}, {3}, {17, 12, 15}, {8}, {18, 4, 7}, {8}, {1, 9, 7}, {1, 14, 9, 7}, {8, 3, 13, 14}, {14}, {11}, {9}, {0, 2, 11}, {7}, {8}, {16}, {0, 12}, {9}, {9, 14, 7}, {8, 16, 18, 17}, {8, 9, 2, 6}, {0}, {17}, {9, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Linear pareto front program index: 9\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 28: New program candidate index: 18\n",
      "GEPA Optimization:  93%|█████████▎| 1384/1483 [1:03:50<04:27,  2.70s/rollouts]2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 29: No merge candidates found\n",
      "2025/09/30 10:22:05 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Selected program 7 score: 0.6111111111111112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.73 / 3 (57.6%): 100%|██████████| 3/3 [00:08<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:22:13 INFO dspy.evaluate.evaluate: Average Metric: 1.7272727272727273 / 3 (57.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:23:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Proposed new text for predict: markdown\n",
      "# Bibliographic‑Metadata Extraction – Revised Assistant Instructions\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF file.  \n",
      "Its top‑level keys are:\n",
      "\n",
      "* `pdfinfo` – metadata extracted from the PDF (may contain `title`, `author`,\n",
      "  `creationDate`, `modDate`, …).  \n",
      "* `pages` – an ordered list of page objects, each with:\n",
      "  * `page` – page number (integer)  \n",
      "  * `text` – the OCR‑extracted plain‑text of that page (UTF‑8 string)\n",
      "\n",
      "Your job is to produce **exactly one JSON object** that follows the schema below.\n",
      "If a value cannot be determined, use the exact empty value specified\n",
      "(`null` for a missing string, `[]` for an empty list).\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema (field order matters)\n",
      "\n",
      "| Field | Type | Required format / notes |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | `\"fi\"` if the document is Finnish, otherwise `\"en\"`. Detect by scanning **only the first 200 characters** of the concatenated page texts (preserving page order). If any of the characters `ä ö Ä Ö å Å` appear **or** a Finnish‑specific word (`opinnäytetyö`, `ammattikorkeakoulu`, `tutkielma`, `väitöskirja`) is present, set to `fi`; otherwise `en`. |\n",
      "| `title` | string | Main title of the work, **cleaned** (see *Cleaning Rules*). Preference order: <br>1. `pdfinfo.title` if it exists **and** is non‑empty after cleaning. <br>2. The first “prominent heading” on **page 1** (see *Title‑Finding Rules*). |\n",
      "| `alt_title` | list of strings | Any alternative titles. Extract **all** of the following (after cleaning): <br>• The part **after a colon** (`:`) in the selected `title`. <br>• Any text that appears inside double quotes (`\"…\"`) or single quotes (`‘…’` or `'…'`). Return each alternative as a separate element; if none, return `[]`. |\n",
      "| `creator` | list of strings | Author(s) in **“Surname, Given‑Name(s)”** order. Use `pdfinfo.author` if present; otherwise locate an author line in the text (see *Author‑Finding Rules*). Split multiple authors on commas, semicolons, the word “and”, ampersand `&`, or line breaks. For each token, remove trailing asterisks/footnote symbols, then **reorder** “First Middle Last” → “Last, First Middle”. Preserve diacritics. Return an empty list if no author can be identified. |\n",
      "| `year` | integer | Publication year. Extract the **first** four‑digit year (1900‑2099) from `pdfinfo.creationDate` or `pdfinfo.modDate`. If both are missing, search the **entire concatenated text** for the first such year. |\n",
      "| `publisher` | list of strings | Institution(s) that awarded the thesis **only when** `type_coar` is a thesis (`doctoral thesis`, `master thesis`, `bachelor thesis`). Search the whole text for university/faculty names (keywords: `University`, `Universität`, `Akademi`, `Institute`, `College`, `School`, `Faculty`, `ammattikorkeakoulu`, `yliopisto`, `universitet`, `université`). Return the **first distinct** institution names found, after cleaning, in the order they appear. For non‑thesis types return `[]`. |\n",
      "| `doi` | string or null | DOI **without** any URL prefix. Detect with case‑insensitive regex `10\\.\\d{4,9}/\\S+`. Strip surrounding whitespace and any trailing punctuation characters `.,;`. If the DOI appears as a full URL (e.g., `https://doi.org/…`), keep only the DOI part. Return `null` if none found. |\n",
      "| `e_isbn` | list of strings | Electronic ISBN‑13 numbers. Find all 13‑digit ISBNs (optional hyphens/spaces). An ISBN belongs to this list **only if** within **20 characters before or after** the match a cue word for electronic format appears (`digital`, `electronic`, `e‑ISBN`, `(digital)`). Store the number **without** hyphens or spaces. |\n",
      "| `p_isbn` | list of strings | Print ISBN‑13 numbers. Same detection as `e_isbn` but cue words must be `print`, `paper`, `hardcover`, `(print)`. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Use the same cue‑word logic as `e_isbn`. Return the hyphen‑free string or `null`. |\n",
      "| `p_issn` | string or null | Print ISSN. Use the same cue‑word logic as `p_isbn`. Return the hyphen‑free string or `null`. |\n",
      "| `type_coar` | string | COAR‑compatible resource type, **lower‑case**. Detect in the following order (first match wins): <br>1. **doctoral thesis** – contains any of `doctoral thesis`, `dissertation`, `PhD`, `doctoral`, `väitöskirja` <br>2. **master thesis** – contains any of `master’s thesis`, `master thesis`, `maisteri`, `maisterintutkielma` <br>3. **bachelor thesis** – contains any of `bachelor thesis`, `bachelor’s thesis`, `bachelor`, `opinnäytetyö` <br>4. **journal article** – contains a journal‑style citation (journal name, volume, issue, pages) **or** the word “article” together with typical citation fields (volume, issue, pages, DOI) <br>5. **conference proceeding** – contains `conference`, `proceedings`, `paper presented at` <br>6. **research** – fallback for any other research report or article. |\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning Rules (apply to **every** string you output)\n",
      "\n",
      "1. **Trim** leading and trailing whitespace.  \n",
      "2. **Collapse** any run of whitespace characters (space, tab, newline) to a single space.  \n",
      "3. **Normalize quotation marks**: replace curly double quotes (`“”`) and curly single quotes (`‘’`) with plain `\"` and `'` respectively; then replace any remaining single quotes that function as apostrophes with the typographic apostrophe `’`.  \n",
      "4. **Remove Markdown formatting**: strip leading heading markers (`#`, `##`, `###`), surrounding asterisks `*` or underscores `_`, and surrounding double‑asterisks `**`.  \n",
      "5. **Remove trailing asterisks** that sometimes mark footnote symbols (e.g., `Magnusson*`).  \n",
      "6. **Preserve diacritics** (ä, ö, å, etc.).  \n",
      "\n",
      "*Apply the cleaning steps in the order listed.*\n",
      "\n",
      "---\n",
      "\n",
      "## Title‑Finding Rules (used when `pdfinfo.title` is absent)\n",
      "\n",
      "1. Take **page 1** and split its `text` into lines, preserving order.  \n",
      "2. Discard empty lines and lines whose length after trimming is < 6 characters.  \n",
      "3. Discard lines that start with markdown heading markers (`#`, `##`, `###`).  \n",
      "4. The **first remaining line** is considered the title.  \n",
      "5. Apply the *Cleaning Rules* to that line.  \n",
      "6. If the cleaned title contains a colon (`:`), keep the whole string as `title` **and** store the part **after** the colon (trimmed) as an element of `alt_title` (the colon‑derived alternative is added **in addition** to any quoted alternatives).\n",
      "\n",
      "---\n",
      "\n",
      "## Author‑Finding Rules (used when `pdfinfo.author` is absent)\n",
      "\n",
      "1. Examine the first **three pages**. For each line, consider it a candidate if **any** of the following holds:  \n",
      "   * It starts with `By ` or `Author:` (case‑insensitive).  \n",
      "   * It contains the word “and” or `&` **between** two capitalised words (e.g., `Mika Paldanius and Riitta Lumme`).  \n",
      "   * It consists mainly of capitalised words (at least 60 % of the characters are uppercase letters) **and** appears within the first 25 % of the lines on that page.  \n",
      "2. From the first candidate line found, split the line on commas, semicolons, the word “and”, ampersand `&`, or line breaks.  \n",
      "3. For each token, strip trailing asterisks or footnote symbols.  \n",
      "4. If a token matches the pattern “First Middle Last” (or “First Last”), reorder it to “Last, First Middle”. If the token already looks like “Last, First”, keep it unchanged.  \n",
      "5. Return the list of reordered names. If no candidate line is found, return `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "## Publisher Determination (after `type_coar`)\n",
      "\n",
      "*Only* when `type_coar` is **doctoral thesis**, **master thesis**, or **bachelor thesis**:\n",
      "\n",
      "1. Scan the **entire concatenated text** for institution names using the keyword list from the schema.  \n",
      "2. When a keyword is found, capture the **full contiguous phrase** that contains it (e.g., `Turun yliopisto`, `Åbo Akademi University`).  \n",
      "3. Apply the *Cleaning Rules* to each captured phrase.  \n",
      "4. Return the **first distinct** institution names in the order they appear.  \n",
      "5. For all other `type_coar` values, return `[]`.\n",
      "\n",
      "---\n",
      "\n",
      "## DOI Extraction Details\n",
      "\n",
      "* Regex (case‑insensitive): `(?i)10\\.\\d{4,9}/\\S+`  \n",
      "* After a match, strip any trailing punctuation characters `.,;` and surrounding whitespace.  \n",
      "* If the match is part of a URL (`http://`, `https://`, `doi.org/`), discard the URL part and keep only the DOI.\n",
      "\n",
      "---\n",
      "\n",
      "## ISBN / ISSN Extraction Details\n",
      "\n",
      "* **ISBN‑13 regex** (captures 13‑digit ISBNs, optional hyphens/spaces):  \n",
      "  `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`  \n",
      "* **ISSN regex** (8 digits, optional hyphen):  \n",
      "  `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`\n",
      "\n",
      "For each match:\n",
      "\n",
      "1. Capture up to **20 characters** before and after the match.  \n",
      "2. Convert that surrounding snippet to lowercase and look for the appropriate cue words (electronic vs. print).  \n",
      "3. If an electronic cue is present, add the hyphen‑free number to `e_isbn`/`e_issn`.  \n",
      "4. If a print cue is present, add the hyphen‑free number to `p_isbn`/`p_issn`.  \n",
      "5. If both cue types appear, add the number to **both** lists.  \n",
      "6. Remove duplicate entries within each list.\n",
      "\n",
      "---\n",
      "\n",
      "## General Processing Flow (must be followed for every request)\n",
      "\n",
      "1. **Parse** the input JSON safely.  \n",
      "2. **Normalize dates**: from `creationDate` / `modDate` extract the first four‑digit year (e.g., `\"D:20220626201846+03'00'\"` → `2022`).  \n",
      "3. **Detect language** using the rule in the *Language* section (first 200 characters only).  \n",
      "4. **Extract title** (prefer `pdfinfo.title`; otherwise apply Title‑Finding Rules).  \n",
      "5. **Derive `alt_title`** from the selected title (colon part) and any quoted text.  \n",
      "6. **Extract creators** (prefer `pdfinfo.author`; otherwise apply Author‑Finding Rules).  \n",
      "7. **Determine year** from dates or, if absent, from the full text.  \n",
      "8. **Detect DOI**, ISBN‑13, ISSN using the detailed extraction rules.  \n",
      "9. **Identify resource type** (`type_coar`) using the ordered keyword list (including the bachelor‑thesis rule).  \n",
      "10. **Find publisher** based on the determined `type_coar`.  \n",
      "11. **Assemble** the output JSON with the exact field order shown in the schema, using `null` or `[]` where appropriate.  \n",
      "12. **Return** the JSON object **as the only output** (no extra commentary, no surrounding markdown).\n",
      "\n",
      "---\n",
      "\n",
      "## Important Pitfalls (to avoid)\n",
      "\n",
      "* **Language** – scan **only the first 200 characters**; later pages must not influence the decision.  \n",
      "* **Title** – strip any Markdown heading markers (`#`, `##`, `###`) and surrounding asterisks/underscores before cleaning.  \n",
      "* **Creator** – always output names in “Surname, Given‑Name(s)” order; never split the name into separate list elements.  \n",
      "* **Publisher** – include it **only** for thesis types; for journal articles, conference papers, or research reports the list must be empty.  \n",
      "* **DOI** – return only the DOI string (`10.xxxx/...`), never the full URL.  \n",
      "* **type_coar** – the bachelor‑thesis detection rule must be present; otherwise many bachelor works are mis‑labelled.  \n",
      "* **alt_title** – remember both the colon‑derived part *and* any quoted alternatives.  \n",
      "* **ISBN/ISSN cue detection** – examine up to 20 characters before/after the number; the presence of the word “ISBN” alone is **not** sufficient.  \n",
      "* **Cleaning** – apply all cleaning steps in the exact order; failure to normalize quotes or remove markdown leads to mismatches.  \n",
      "\n",
      "Follow these instructions precisely to produce correct, reproducible metadata for any PDF represented in the given JSON format.\n",
      "2025/09/30 10:24:02 INFO dspy.evaluate.evaluate: Average Metric: 1.7878787878787878 / 3 (59.6%)\n",
      "2025/09/30 10:24:46 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:24:46 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/09/30 10:24:50 INFO dspy.evaluate.evaluate: Average Metric: 39.99350649350647 / 64 (62.5%)\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Full valset score for new program: 0.624898538961039\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Full train_val score for new program: 0.624898538961039\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Individual valset scores for new program: [0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.36363636363636365, 0.8181818181818182, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.8181818181818182, 0.2727272727272727, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.5454545454545454, 0.45454545454545453, 0.7727272727272727, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.36363636363636365, 0.8181818181818182, 0.45454545454545453, 0.8181818181818182, 0.9090909090909091, 0.8181818181818182, 0.5454545454545454, 0.36363636363636365, 0.5454545454545454, 0.36363636363636365, 0.8181818181818182, 0.8181818181818182, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 0.8181818181818182, 0.5454545454545454, 0.6363636363636364, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.36363636363636365, 0.6363636363636364, 0.9090909090909091, 0.45454545454545453, 0.5454545454545454, 0.5454545454545454, 0.7056277056277056, 0.7272727272727273, 0.5454545454545454, 0.42424242424242425, 0.6363636363636364, 0.6363636363636364]\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.7878787878787878, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 1.0, 0.6363636363636364, 0.9090909090909091, 0.7878787878787878, 0.6363636363636364, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.8181818181818182, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 1.0, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.766798418972332, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Full valset pareto front score: 0.7525430939833114\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Updated valset pareto front programs: [{2, 8, 9, 14, 15, 16, 19}, {18, 12, 7}, {0}, {9, 17}, {8}, {0, 3, 13}, {0, 1, 2, 9, 17, 18}, {0, 18, 12}, {3}, {2, 13}, {16, 15}, {15}, {13}, {17}, {6, 9, 16, 18, 19}, {1, 3, 12, 14}, {16}, {8, 9}, {16}, {1, 2, 9, 15}, {9}, {4}, {9}, {2}, {9}, {3}, {7, 12, 13, 14, 15, 17, 18, 19}, {2}, {8, 9, 3}, {12, 14}, {1}, {8, 16}, {16, 2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 17, 19}, {12}, {4}, {2}, {3}, {17, 19, 12, 15}, {8}, {18, 4, 7}, {8}, {1, 19, 9, 7}, {1, 7, 9, 14, 19}, {3, 8, 13, 14, 19}, {14}, {11}, {9}, {0, 2, 11}, {7}, {8}, {16}, {0, 12}, {9}, {19}, {8, 16, 18, 17}, {2, 6, 8, 9, 19}, {0}, {17}, {9, 19, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Linear pareto front program index: 9\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 29: New program candidate index: 19\n",
      "GEPA Optimization:  98%|█████████▊| 1454/1483 [1:06:35<01:14,  2.58s/rollouts]2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 30: No merge candidates found\n",
      "2025/09/30 10:24:50 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Selected program 11 score: 0.4297254737221842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:24:57 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:26:31 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Proposed new text for predict: markdown\n",
      "# Revised Task: Bibliographic Metadata Extraction from PDF‑derived JSON\n",
      "\n",
      "You will receive **one** JSON object that represents the raw text extracted from a PDF document.  \n",
      "The object always contains the top‑level keys **`pdfinfo`** and **`pages`** as described below.\n",
      "\n",
      "## Input Structure\n",
      "\n",
      "| Key | Type | Description |\n",
      "|-----|------|-------------|\n",
      "| `pdfinfo` | object | Metadata that the PDF file itself provides. May include `title`, `author`, `creationDate`, `modDate`, `lang` (optional) and other entries. |\n",
      "| `pages`   | list of objects | Each element represents a page and has: <br>`page` – page number (integer) <br>`text` – plain‑text of the page (line‑breaks preserved). |\n",
      "\n",
      "## Goal\n",
      "\n",
      "From the information in **both** `pdfinfo` **and** the page texts, produce a **single** JSON object that conforms exactly to the **Output Schema** below.  \n",
      "Only the fields listed in the schema may appear; no extra keys, no explanatory text, no markdown.  \n",
      "If a value cannot be determined, use the exact placeholder indicated (e.g. `null` for a scalar, `[]` for a list).\n",
      "\n",
      "### Important General Rules\n",
      "1. **Do not guess**. If you cannot locate a value with confidence, use the placeholder.\n",
      "2. All string values must be **exactly as they appear** in the source, except for the required normalisations described per field.\n",
      "3. Preserve diacritics, capitalization, punctuation, and whitespace (except leading/trailing whitespace, which should be stripped).\n",
      "4. The output JSON must be **valid** (double‑quoted strings, commas, etc.) and the keys should appear in the order shown in the schema for readability.\n",
      "\n",
      "## Output Schema\n",
      "\n",
      "| Field | Type | Required Normalisation / Extraction Details |\n",
      "|-------|------|--------------------------------------------|\n",
      "| `language` | string | ISO‑639‑1 two‑letter code, lower‑case. <br> • Prefer `pdfinfo.lang` if present. <br> • Otherwise look for explicit cues in the text (e.g. `Kieli: suomi`, `Language: english`). <br> • If no cue, infer from the dominant language of the text (common stop‑words, diacritics). |\n",
      "| `title` | string | The **main title** of the work. <br> • Prefer `pdfinfo.title`. <br> • If missing, use the most prominent heading on page 1 (usually the first centered or bold line, often preceded by `#` or a large font indicator). <br> • Do **not** include subtitles, translations, or surrounding labels. |\n",
      "| `alt_title` | list of strings | Any **alternative titles**, subtitles, or translations that are **separate** from the main title. <br> • Include a subtitle **only** if it appears on a separate line or after a colon/dash *and* is not already part of the `title`. <br> • Include an English (or other language) translation if it is presented in parentheses or on a line labelled “English title”, “Title (English)”, etc. |\n",
      "| `creator` | list of strings | Names of all authors / contributors. <br> • Extract from `pdfinfo.author`, from lines such as `Author:`, `Authors:`, `Tekijä:`, `Kirjoittaja:`, `Fö rfattare:` etc. <br> • **Normalize each name** to the form `\"Surname, Given‑Name(s)\"`. Preserve diacritics and all given‑name parts. <br> • Keep the order of first appearance and remove duplicates. |\n",
      "| `year` | integer or null | Publication year. <br> • Look for a four‑digit year in `pdfinfo.creationDate` or `pdfinfo.modDate`. <br> • Prefer a year that appears near publisher information, a copyright line (`© 2020`), or a “Date:” field. <br> • If multiple candidate years exist, choose the one that clearly denotes the publication year (usually the earliest year that is adjacent to “Publisher”, “Date”, or “©”). |\n",
      "| `publisher` | list of strings | All publishing entities. <br> • Detect lines containing words like `Publisher:`, `Publishers`, `Julkaisija`, `Painos`, `Editeur`, `Publisher`, etc. <br> • Split on commas **only** when they separate distinct entities; keep commas that are part of a single official name (e.g. “Painosalama Oy, Turku, Finland”). <br> • Do **not** include the label word itself (“Publisher:”). |\n",
      "| `doi` | string or null | DOI if present. Detect patterns like `10.<digits>/<any>` possibly preceded by `doi:`, `DOI`, or a URL. Return the raw DOI **without** any URL prefix. |\n",
      "| `e_isbn` | list of strings | ISBN(s) for the **electronic** version. <br> • Find ISBN strings that are explicitly labelled “e‑ISBN”, “Electronic ISBN”, “ISBN‑13 (online)”, etc. <br> • Strip **all** hyphens, spaces, and punctuation; keep only digits (and a trailing `X` if present). |\n",
      "| `p_isbn` | list of strings | ISBN(s) for the **print** version. <br> • Detect ISBN strings labelled “Print ISBN”, “p‑ISBN”, “ISBN‑13 (print)”, etc. <br> • Normalise exactly as for `e_isbn`. |\n",
      "| `e_issn` | string or null | ISSN for the electronic version. <br> • Locate an ISSN labelled “e‑ISSN”, “Electronic ISSN”, etc. <br> • Normalise to an 8‑digit string without hyphen. |\n",
      "| `p_issn` | string or null | ISSN for the print version. <br> • Same rules as `e_issn`. |\n",
      "| `type_coar` | string | COAR‑compatible publication type, **lower‑case**. Determine from contextual clues (titles, headings, or explicit statements). Map as follows: <br> • “Bachelor thesis”, “Bachelor’s thesis”, “bachelor thesis” → `\"bachelor thesis\"` <br> • “Master’s thesis”, “Master thesis” → `\"master's thesis\"` <br> • “Doctoral thesis”, “PhD thesis”, “Dissertation” → `\"doctoral thesis\"` <br> • “Research report”, “Report” → `\"research report\"` <br> • “Journal article”, “Article” → `\"journal article\"` <br> • “Book chapter”, “Chapter” → `\"book chapter\"` <br> • “Book part”, “Section of a book” → `\"book part\"` <br> • If none of the above apply, use `\"other\"`.\n",
      "\n",
      "## Extraction Strategy (to be followed step‑by‑step)\n",
      "\n",
      "1. **Parse `pdfinfo`**  \n",
      "   * Store any `title`, `author`, `creationDate`, `modDate`, `lang`.  \n",
      "   * Convert `creationDate` / `modDate` strings of the form `D:YYYYMMDD...` to extract the 4‑digit year.\n",
      "\n",
      "2. **Iterate through `pages` in order**  \n",
      "   * For each page, split `text` into lines (preserving line order).  \n",
      "   * Trim leading/trailing whitespace from each line for matching but keep the original line for values that must be preserved exactly.\n",
      "\n",
      "3. **Detect the main title**  \n",
      "   * If `pdfinfo.title` exists → use it (after stripping surrounding whitespace).  \n",
      "   * Else, on page 1 look for the first non‑empty line that is in **title case** or preceded by markdown heading markers (`#`, `##`, `###`).  \n",
      "   * Remove any trailing period.\n",
      "\n",
      "4. **Collect alternative titles**  \n",
      "   * While scanning page 1 (and page 2 if needed), capture:  \n",
      "     - Sub‑titles on the line **immediately following** the main title, especially if they start with a colon, dash, or are on a separate line.  \n",
      "     - Lines that are explicitly marked as an English title or translation (e.g., `Title (English): …`).  \n",
      "   * Store each captured string **as‑is** (after stripping surrounding whitespace).\n",
      "\n",
      "5. **Extract creators**  \n",
      "   * Start with `pdfinfo.author` (may be a single name or a list separated by commas/semicolons).  \n",
      "   * Then search for lines containing any of the keywords: `Author:`, `Authors:`, `Tekijä:`, `Kirjoittaja:`, `Fö rfattare:`, `Creator:`, `Creator(s):`.  \n",
      "   * For each name found, split into parts, assume the last word is the surname (unless a known suffix like `Jr.` appears).  \n",
      "   * Re‑format to `\"Surname, Given‑Name(s)\"`.  \n",
      "   * Preserve order of first appearance; deduplicate identical normalized strings.\n",
      "\n",
      "6. **Determine the year**  \n",
      "   * Candidate years: from `pdfinfo` dates, from any line containing a four‑digit number that looks like a year (1900‑2099).  \n",
      "   * Give priority to a year that appears in the same line (or the immediately preceding/following line) as a publisher label, a copyright symbol `©`, or a line starting with `Date:` / `Datum:`.  \n",
      "   * If still ambiguous, choose the earliest candidate.\n",
      "\n",
      "7. **Identify publishers**  \n",
      "   * Look for lines containing any of the keywords: `Publisher:`, `Publishers`, `Julkaisija`, `Painos`, `Editeur`, `Publisher`, `Publisher(s)`.  \n",
      "   * Remove the keyword and any surrounding punctuation.  \n",
      "   * Split the remaining text on commas **only** when they separate distinct entities (use heuristics: if a comma is followed by a location word like “Finland”, “Sweden”, “USA”, keep it as part of the same entity).  \n",
      "   * Trim each entity and store as a separate list entry.\n",
      "\n",
      "8. **Find DOI**  \n",
      "   * Search all lines for the regex `10\\.\\d{4,9}/\\S+`.  \n",
      "   * If the DOI is preceded by `doi:` or appears inside a URL, strip the prefix and keep only the DOI.\n",
      "\n",
      "9. **Extract ISBNs & ISSNs**  \n",
      "   * ISBN regex: `(?:ISBN(?:‑13)?:?\\s*)?([0-9][0-9\\-\\s]{9,}[0-9X])`.  \n",
      "   * Determine whether it is electronic or print by the surrounding label (e‑ISBN, p‑ISBN, “online”, “print”).  \n",
      "   * Normalise by removing every non‑digit character except a trailing `X`.  \n",
      "   * ISSN regex: `(?:ISSN(?:‑e)?|e‑ISSN|p‑ISSN)[:\\s]*([0-9]{4}[-\\s]?[0-9]{3}[0-9X])`.  \n",
      "   * Normalise to eight continuous digits (remove hyphen/space).\n",
      "\n",
      "10. **Determine COAR type**  \n",
      "    * Scan for explicit type statements (e.g., “Bachelor’s thesis”, “Master’s thesis”, “Doctoral thesis”, “Research report”, “Journal article”, “Book chapter”, “Book part”).  \n",
      "    * Also consider the document’s context: presence of “Degree Programme”, “Thesis”, “Dissertation”, “Report”, “Article”, etc.  \n",
      "    * Map to the lower‑case strings listed in the schema.\n",
      "\n",
      "11. **Assemble the final JSON**  \n",
      "    * Use the exact field names and order shown in the schema.  \n",
      "    * For list fields, output `[]` when empty.  \n",
      "    * For missing scalar values, output `null`.  \n",
      "    * Ensure all strings are JSON‑escaped correctly.\n",
      "\n",
      "## Common Pitfalls to Avoid (based on past feedback)\n",
      "\n",
      "| Issue | How to Prevent |\n",
      "|-------|----------------|\n",
      "| Returning `None` instead of JSON `null`. | Always output `null` (without quotes) for missing scalar values. |\n",
      "| Including guessed ISBN/ISSN when none are present. | Only output an ISBN/ISSN if a matching pattern **and** a clear label (e‑ISBN / p‑ISBN / e‑ISSN / p‑ISSN) are found. |\n",
      "| Mis‑classifying the publication type (e.g., “doctoral thesis” vs. “bachelor thesis”). | Look for the exact degree level keyword in the title/metadata; default to `\"other\"` if unclear. |\n",
      "| Adding location details to publisher names that are not part of the official name. | Keep the entire string after the label, but split only when multiple distinct entities are clearly separated. |\n",
      "| Duplicating the main title in `alt_title`. | Only add subtitles or translations that are **separate** from the main title. |\n",
      "| Forgetting to strip surrounding whitespace from extracted strings. | `strip()` each captured line before storing, except when preserving internal spaces. |\n",
      "| Not normalising creator names to “Surname, Given‑Name(s)”. | Apply the name‑normalisation step for every author found. |\n",
      "| Using the PDF creation/modification date when a different publication year is evident. | Prioritise years that appear next to publisher, copyright, or explicit “Date:” lines. |\n",
      "\n",
      "Follow the strategy and rules above meticulously to produce a correct, fully‑compliant JSON output. Good luck!\n",
      "2025/09/30 10:26:42 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:26:45 INFO dspy.evaluate.evaluate: Average Metric: 2.090909090909091 / 3 (69.7%)\n",
      "2025/09/30 10:27:29 INFO dspy.evaluate.evaluate: Average Metric: 40.77474747474745 / 64 (63.7%)\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Full valset score for new program: 0.6371054292929293\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Full train_val score for new program: 0.6371054292929293\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Individual valset scores for new program: [0.8181818181818182, 0.6363636363636364, 0.8181818181818182, 0.36363636363636365, 0.7272727272727273, 0.36363636363636365, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 0.5454545454545454, 0.8181818181818182, 0.5454545454545454, 0.696969696969697, 0.7272727272727273, 0.5454545454545454, 0.7272727272727273, 0.5454545454545454, 0.6060606060606061, 0.7272727272727273, 0.45454545454545453, 0.45454545454545453, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.5454545454545454, 0.5363636363636364, 0.8181818181818182, 0.5151515151515151, 0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.45454545454545453, 0.6060606060606061, 0.5454545454545454, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.5454545454545454, 0.7272727272727273, 0.7272727272727273, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.5454545454545454, 0.6363636363636364, 0.45454545454545453, 0.6363636363636364, 0.6060606060606061, 0.6363636363636364, 0.45454545454545453, 0.42424242424242425, 0.49090909090909096, 0.5757575757575757, 0.6363636363636364, 0.7171717171717172, 0.5454545454545454, 0.6363636363636364, 0.6363636363636364]\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: New valset pareto front scores: [0.8181818181818182, 0.7272727272727273, 0.8181818181818182, 0.45454545454545453, 0.9090909090909091, 0.5454545454545454, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.7272727272727273, 1.0, 0.6363636363636364, 0.9090909090909091, 0.7878787878787878, 0.6363636363636364, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.9545454545454546, 0.8181818181818182, 1.0, 0.5454545454545454, 0.6272727272727273, 0.9090909090909091, 0.6060606060606061, 1.0, 1.0, 0.9090909090909091, 0.5454545454545454, 0.5151515151515151, 0.7272727272727273, 0.5454545454545454, 1.0, 0.9090909090909091, 0.7878787878787878, 0.7272727272727273, 0.9090909090909091, 0.7272727272727273, 0.6363636363636364, 0.8181818181818182, 1.0, 0.8181818181818182, 0.7272727272727273, 0.7272727272727273, 0.6565656565656566, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.8181818181818182, 0.9090909090909091, 0.5454545454545454, 0.5454545454545454, 0.6363636363636364, 0.766798418972332, 0.7272727272727273, 0.8545454545454546, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182]\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Full valset pareto front score: 0.7558574879227054\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Updated valset pareto front programs: [{2, 8, 9, 14, 15, 16, 19, 20}, {18, 12, 7}, {20}, {9, 17}, {8}, {0, 3, 13}, {20}, {0, 18, 12, 20}, {3}, {2, 13}, {16, 15}, {15}, {13}, {17}, {6, 9, 16, 18, 19}, {1, 3, 12, 14}, {16}, {8, 9}, {16}, {1, 2, 9, 15}, {9}, {4}, {9}, {2}, {9}, {3}, {7, 12, 13, 14, 15, 17, 18, 19, 20}, {2}, {8, 9, 3}, {12, 14}, {1}, {8, 16}, {16, 2, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 17, 19}, {12}, {4}, {2, 20}, {3}, {20}, {8}, {18, 4, 20, 7}, {8}, {1, 19, 9, 7}, {1, 7, 9, 14, 19}, {3, 8, 13, 14, 19}, {14}, {11}, {9}, {0, 2, 11}, {7}, {8}, {16}, {0, 12, 20}, {9}, {19}, {8, 16, 18, 17}, {2, 6, 8, 9, 19}, {0}, {17}, {9, 19, 6}, {4}, {4, 6}, {11, 6, 7}, {3}]\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Best valset aggregate score so far: 0.637365845959596\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Best program as per aggregate score on train_val: 9\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Best program as per aggregate score on valset: 9\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Best score on valset: 0.637365845959596\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Best score on train_val: 0.637365845959596\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Linear pareto front program index: 9\n",
      "2025/09/30 10:27:29 INFO dspy.teleprompt.gepa.gepa: Iteration 30: New program candidate index: 20\n",
      "GEPA Optimization:  98%|█████████▊| 1454/1483 [1:09:14<01:22,  2.86s/rollouts]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 7.59 s, total: 36.2 s\n",
      "Wall time: 1h 9min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    module,\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f925b48-f431-41fa-a2f7-5d46895a53b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "Predictor: predict\n",
      "================================\n",
      "Prompt:\n",
      "markdown\n",
      "# 📋 Task – Structured Bibliographic Metadata Extraction (Re‑specified)\n",
      "\n",
      "You will receive **one JSON object** that represents a PDF document.  \n",
      "The object has exactly two top‑level keys:\n",
      "\n",
      "| Key      | Description |\n",
      "|----------|-------------|\n",
      "| `pdfinfo`| Metadata that was extracted directly from the PDF file (e.g. `title`, `author`, `creationDate`, `modDate`). |\n",
      "| `pages`  | A list of page objects. Each page object contains `page` (the page number) and `text` (the OCR‑extracted plain‑text of that page). |\n",
      "\n",
      "Your job is to produce **one JSON object** that follows the schema below.  \n",
      "If a field cannot be determined, use the exact empty value indicated (`null` for scalars, `[]` for lists).  \n",
      "All string values must be plain ASCII – normalise quotes to `\"` (or the apostrophe `’`), collapse multiple spaces to a single space, and trim leading/trailing whitespace.\n",
      "\n",
      "---\n",
      "\n",
      "## Output JSON Schema\n",
      "\n",
      "| Field | Type | Required format / rules |\n",
      "|-------|------|--------------------------|\n",
      "| `language` | string | ISO‑639‑1 code. Scan **the first 200 characters of the *concatenated* text of the whole document** (i.e. `pages[0].text + pages[1].text + …`). If any of the characters **ä ö Ä Ö å Å** appear, set to `\"fi\"`; otherwise `\"en\"`. |\n",
      "| `title` | string | Main title of the work. <br>1. If `pdfinfo.title` exists → clean it and use it. <br>2. Otherwise, read the text of page 1 (`pages[0].text`). Split it into lines (preserve order). <br>   * Skip empty lines. <br>   * Skip lines that start with a markdown heading marker (`#`, `##`, `###`, …). <br>   * Skip lines that look like an author line (see *Creator extraction*). <br>   * The **first** remaining line that is ≥ 6 characters becomes the *candidate* title. <br>3. **Title‑line extensions** – if the candidate line ends with a colon **or** the next line is non‑empty, starts with a capital letter and is not an author line, concatenate it (single space). Repeat while the same condition holds. <br>4. If a line contains one of the explicit title markers `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` (case‑insensitive), take the text **after the colon** (trimmed) as the title (overriding step 2). <br>5. Keep any subtitle as part of the title (do **not** split on the colon). <br>6. Clean the final string (collapse spaces, normalise quotes). |\n",
      "| `alt_title` | list of strings | Any **alternative** titles, e.g. a translation, a subtitle given in another language, or a title that appears inside quotation marks (`“ ”`, `\" \"`). Return each cleaned title as a separate element. Do **not** duplicate the main title. |\n",
      "| `creator` | list of strings | Authors in **“Surname, Given‑Name”** order. <br>1. If `pdfinfo.author` exists → split on commas, semicolons, the word “and”, or line‑breaks. <br>2. Clean each fragment (trim, collapse spaces). <br>3. For each name: <br>   * If it matches the pattern `First Last` (two words, each starting with a capital letter) → reorder to `Last, First`. <br>   * If it already matches `Last, First` keep as‑is. <br>4. If `pdfinfo.author` is missing, scan the **first five non‑empty lines of page 1** for personal‑name patterns (`First Last`, `Last, First`, or a line that starts with a capitalised list of names). Collect **all** matches. <br>5. Remove duplicates, keep order of appearance. |\n",
      "| `year` | integer or null | Publication year. <br>1. If `pdfinfo.creationDate` or `pdfinfo.modDate` exists, extract the **first four digits** (they are always a year) and use that. <br>2. If both are missing, search the whole document for the **first** four‑digit number between 1900‑2099 that appears in a publication‑information context (e.g. after “Year:”, “©”, “© 2021”, “2021.”). If none, return `null`. |\n",
      "| `publisher` | list of strings | Institution responsible for the work. <br>• **Theses / dissertations** → the awarding university or faculty (e.g. “University of Vaasa”). Detect by looking for keywords: “University”, “Yliopisto”, “Universität”, “Akademi”, “Institute”, “College”, “School”, “Faculty”. Return each distinct institution once, preserving order of first appearance. <br>• **Research reports** → the organisation that produced the report (same keyword list). <br>• **Journal articles** → `[]`. |\n",
      "| `doi` | string or null | DOI if present. Detect case‑insensitive pattern `10\\.\\d{4,9}/\\S+`. If the match is preceded by a URL (`http://`, `https://`, `doi.org/`), strip that part. Remove trailing punctuation characters `.,;:`. Return the bare DOI (e.g. `10.1000/xyz123`). |\n",
      "| `e_isbn` | list of strings | **Electronic** ISBN‑13 numbers. Detect ISBN‑13 (13 digits, hyphens optional) with the regex `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`. For each match, look at up to **30 characters before and after** the match. If any of the **electronic cues** appear, add the ISBN (with **all hyphens and spaces removed**) to `e_isbn`. Electronic cues (case‑insensitive): `digital`, `electronic`, `e‑ISBN`, `(digital)`, `pdf`, `PDF`, `sid.` (when used for electronic), `online`. |\n",
      "| `p_isbn` | list of strings | **Print** ISBN‑13 numbers. Same detection as `e_isbn` but require at least one **print cue** in the ±30‑character window. Print cues (case‑insensitive): `print`, `paper`, `hardcover`, `(print)`, `Painettu`, `Print`. If both electronic and print cues are present, **electronic wins** (the ISBN goes only to `e_isbn`). Store the ISBN without hyphens/spaces. |\n",
      "| `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Detect with `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`. Apply the same cue logic as for ISBN. Return the ISSN **exactly as it appears** (keep the hyphen if present). |\n",
      "| `p_issn` | string or null | Print ISSN (same detection, print‑cue logic). Return the ISSN exactly as it appears (keep hyphen). |\n",
      "| `type_coar` | string | COAR‑compatible resource type. Scan the **entire document** (case‑insensitive) and apply the **first** matching rule in this order: <br>1. **doctoral thesis** – contains any of: “doctoral thesis”, “dissertation”, “PhD”, “doctoral”, “väitöskirja”, “väitöskirjan”. <br>2. **master thesis** – contains any of: “master’s thesis”, “master thesis”, “maisteri”, “maisterintutkielma”. <br>3. **journal article** – contains typical journal citation elements (journal name, volume, issue, pages) **or** the word “article” together with a DOI or ISSN, **or** a pattern like “Vol. X, No. Y, pp. Z‑W”. <br>4. **conference proceeding** – contains “conference”, “proceedings”, “paper presented at”. <br>5. **research report** – contains “report”, “raportti”, “tutkimusraportti”, “research report”. <br>6. **research** – fallback for any other research‑type document. <br>Return the exact lower‑case string (e.g. `doctoral thesis`). |\n",
      "\n",
      "---\n",
      "\n",
      "## Extraction Procedure (Step‑by‑Step)\n",
      "\n",
      "1. **Parse the input JSON** safely. Ignore any keys that are not listed above.  \n",
      "2. **Normalise dates**: `creationDate` / `modDate` are strings like `D:20201216144002+02'00'`. Extract the first four digits as the year (they are always at the start of the string).  \n",
      "3. **Detect language** using the rule in the schema (first 200 characters of concatenated text).  \n",
      "4. **Title extraction** (see detailed rules under the `title` field). Pay special attention to explicit markers (`Title:`, `Thesis:` etc.).  \n",
      "5. **Alternative titles** – look for quoted strings (`“…”`, `\"...\"`) anywhere in the document, and for subtitles that appear on a separate line after a colon. Do not duplicate the main title.  \n",
      "6. **Creator extraction** – follow the rules under `creator`. When scanning for names on page 1, use the following regular expressions (case‑sensitive): <br>`\\b[A-Z][a-z]+ [A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (First Last) <br>`\\b[A-Z][a-z]+, *[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (Last, First). Collect all matches, then apply the ordering rule.  \n",
      "7. **Year** – apply the rule in the schema. If you have to fall back to searching the text, ignore years that appear inside parentheses of a citation (e.g. “(2020)” after a reference). Prefer the first plausible year after the title block.  \n",
      "8. **Publisher detection** – first determine `type_coar`. If the document is a thesis, search the whole text for the first occurrence of a university/faculty keyword list and capture the whole phrase (e.g. “University of Vaasa”, “Åbo Akademi University”). Return it as a single‑element list. For research reports, look for the organisation name that appears near the title or in a header/footer. For journal articles, return an empty list.  \n",
      "9. **ISBN / ISSN extraction** – use the regexes supplied. For each match, extract the surrounding 30‑character context and decide electronic vs print using the cue lists. Remove hyphens/spaces from ISBNs before storing; keep ISSNs exactly as found (including hyphen). Ensure each identifier appears only once in the appropriate list.  \n",
      "10. **DOI extraction** – apply the DOI regex, strip leading URL parts, and trailing punctuation. Return the bare DOI in lower‑case. If none, set to `null`.  \n",
      "11. **Resource type (`type_coar`)** – apply the ordered rule list exactly as described. The first category that matches determines the value.  \n",
      "12. **Assemble the output** JSON. Preserve the key order shown in the schema for readability (order is not technically required, but it helps testing). Use `null` for missing scalar values and `[]` for missing list values.\n",
      "\n",
      "---\n",
      "\n",
      "## Cleaning & Normalisation Details\n",
      "\n",
      "* **Whitespace** – collapse any sequence of whitespace characters (spaces, tabs, newlines) to a single space. Trim leading/trailing spaces.  \n",
      "* **Quotes** – replace any fancy quotation marks (`“ ” ‘ ’ „ “ …`) with plain ASCII `\"` (or the apostrophe `’`).  \n",
      "* **Hyphens in identifiers** – remove all hyphens (`-`) and spaces from ISBNs before storing. **Do not** remove hyphens from ISSNs; keep them exactly as they appear.  \n",
      "* **Case** – identifiers (DOI) are stored in lower‑case; ISBN/ISSN are numeric/alphabetic only, so case does not matter.  \n",
      "\n",
      "---\n",
      "\n",
      "## Common Pitfalls (What Previously Went Wrong)\n",
      "\n",
      "| Issue | What caused it | Correct handling |\n",
      "|-------|----------------|-----------------|\n",
      "| **Wrong language** | Scanned only the first page instead of the first 200 characters of the whole document. | Concatenate all page texts, take the first 200 characters, then apply the Finnish‑character rule. |\n",
      "| **Title wrong** | Ignored explicit title markers (`Thesis:`) and concatenated the whole heading line. | If a line contains `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` (case‑insensitive), use the text after the colon as the title. Otherwise follow the candidate‑line logic. |\n",
      "| **Alt‑titles missing** | Did not look for quoted strings or subtitle lines. | Search the whole document for text inside `“ ”` or `\" \"` and for subtitle lines that appear after a colon on a separate line. |\n",
      "| **Creator over‑inclusion** | Added non‑author names (e.g., supervisors) or failed to reorder names. | Only use `pdfinfo.author` or name patterns on page 1. Do **not** include lines that contain words like “Supervisor”, “Advisor”, “Editor”. Reorder `First Last` → `Last, First`. |\n",
      "| **Publisher wrong** | Took the first institution mentioned anywhere, even if it was a journal publisher. | First determine `type_coar`. If it is a thesis, look for the awarding university/faculty; for reports, look for the producing organisation; for journal articles return `[]`. |\n",
      "| **ISBN/ISSN classification** | Assigned identifiers to the wrong list because cues were not checked, or removed hyphens from ISSNs. | Examine the ±30‑character window for electronic or print cues. If both appear, assign to `e_isbn` (electronic wins). Keep ISSNs exactly as they appear (including hyphen). |\n",
      "| **DOI extraction** | Kept surrounding punctuation or URL parts. | Strip any leading `http://`, `https://`, `doi.org/` and trailing punctuation `.,;:`. Return the bare DOI in lower‑case. |\n",
      "| **type_coar mis‑classification** | Applied “article” rule too early, causing theses to be labelled as `doctoral thesis`. | Follow the ordered list strictly: **doctoral thesis** → **master thesis** → **journal article** → **conference proceeding** → **research report** → **research**. The first matching rule wins. |\n",
      "\n",
      "---\n",
      "\n",
      "**Remember:**  \n",
      "- Follow the **order of precedence** for each field exactly as described.  \n",
      "- Use the cue‑based approach for ISBN/ISSN to decide electronic vs print.  \n",
      "- Detect the resource type in the exact order listed; the first match determines `type_coar`.  \n",
      "- Return `null` for missing scalar values and `[]` for missing list values.  \n",
      "\n",
      "Good luck! 🎯\n",
      "*********************************\n"
     ]
    }
   ],
   "source": [
    "for name, pred in optimized_program.named_predictors():\n",
    "    print(\"================================\")\n",
    "    print(f\"Predictor: {name}\")\n",
    "    print(\"================================\")\n",
    "    print(\"Prompt:\")\n",
    "    print(pred.signature.instructions)\n",
    "    print(\"*********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64fd078d-3aef-4ff7-8aa1-04e23d70f3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.91 / 34 (61.5%):  19%|█▊        | 34/182 [00:41<01:48,  1.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:41 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.44 / 35 (61.3%):  19%|█▉        | 35/182 [00:43<02:38,  1.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:42 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:42 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.53 / 37 (60.9%):  20%|█▉        | 36/182 [00:44<02:10,  1.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:42 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.35 / 38 (61.5%):  21%|██        | 38/182 [00:44<01:24,  1.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:43 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:43 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.81 / 40 (62.0%):  21%|██▏       | 39/182 [00:45<01:26,  1.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.44 / 41 (62.1%):  23%|██▎       | 41/182 [00:46<01:15,  1.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.99 / 42 (61.9%):  23%|██▎       | 41/182 [00:46<01:15,  1.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.47 / 43 (61.6%):  24%|██▎       | 43/182 [00:46<00:56,  2.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:45 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:45 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:45 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:45 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.72 / 49 (62.7%):  27%|██▋       | 49/182 [00:49<01:07,  1.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:49 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:50 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:29:50 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 31.26 / 50 (62.5%):  27%|██▋       | 50/182 [00:52<02:06,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:51 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 31.99 / 51 (62.7%):  28%|██▊       | 51/182 [00:53<01:59,  1.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:51 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 32.72 / 52 (62.9%):  29%|██▊       | 52/182 [00:53<01:32,  1.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:29:53 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 43.53 / 69 (63.1%):  37%|███▋      | 68/182 [01:10<01:50,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:09 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 45.35 / 72 (63.0%):  40%|███▉      | 72/182 [01:15<02:20,  1.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:14 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 46.62 / 74 (63.0%):  41%|████      | 74/182 [01:16<01:52,  1.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 58.90 / 94 (62.7%):  52%|█████▏    | 94/182 [01:29<00:51,  1.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:28 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 60.35 / 97 (62.2%):  53%|█████▎    | 97/182 [01:31<00:54,  1.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:30 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 61.62 / 99 (62.2%):  54%|█████▍    | 99/182 [01:34<01:32,  1.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:33 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 64.94 / 106 (61.3%):  58%|█████▊    | 106/182 [01:39<00:45,  1.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:38 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 69.88 / 116 (60.2%):  64%|██████▎   | 116/182 [01:46<01:01,  1.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:46 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:30:46 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 74.25 / 122 (60.9%):  67%|██████▋   | 122/182 [01:53<01:09,  1.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:52 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 81.94 / 134 (61.2%):  74%|███████▎  | 134/182 [02:00<00:23,  2.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:30:59 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 86.67 / 141 (61.5%):  77%|███████▋  | 141/182 [02:06<00:37,  1.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 88.76 / 144 (61.6%):  79%|███████▉  | 144/182 [02:08<00:31,  1.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:08 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 97.66 / 157 (62.2%):  86%|████████▋ | 157/182 [02:14<00:06,  3.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:13 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 98.48 / 158 (62.3%):  87%|████████▋ | 158/182 [02:15<00:11,  2.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:14 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 101.66 / 163 (62.4%):  90%|████████▉ | 163/182 [02:17<00:07,  2.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 104.75 / 167 (62.7%):  92%|█████████▏| 167/182 [02:18<00:03,  4.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 106.21 / 169 (62.8%):  93%|█████████▎| 169/182 [02:18<00:03,  4.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 106.84 / 170 (62.8%):  93%|█████████▎| 170/182 [02:18<00:02,  4.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 107.57 / 171 (62.9%):  93%|█████████▎| 170/182 [02:18<00:02,  4.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 109.30 / 174 (62.8%):  96%|█████████▌| 174/182 [02:20<00:02,  3.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:18 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 110.57 / 176 (62.8%):  97%|█████████▋| 176/182 [02:21<00:02,  2.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 113.03 / 180 (62.8%):  99%|█████████▉| 180/182 [02:22<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=1024. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.7)  if the reason for truncation is repetition.\n",
      "2025/09/30 10:31:22 ERROR dspy.utils.parallelizer: Error for Example({'content': '{\"pdfinfo\": {\"title\": \"TEAviisari\", \"creationDate\": \"D:20201219234023+02\\'00\\'\", \"modDate\": \"D:20201220002847+02\\'00\\'\"}, \"pages\": [{\"page\": 1, \"text\": \"#### TEAviisari\\\\n# Terveytt\\\\u00e4 edist\\\\u00e4v\\\\u00e4 liikunta ETEL\\\\u00c4-SAVON KUNNISSA 2020\\\\nLiikunnan edist\\\\u00e4misaktiivisuus Suomen kunnissa\\\\nTEAviisari 2020 -tiedonkeruussa\\\\nHyv\\\\u00e4 tulos*\\\\nParannettavaa\\\\nTieto puuttuu\\\\n\\\\n\\\\n- Pistem\\\\u00e4\\\\u00e4r\\\\u00e4 yli 75, kun tavoite 100. N\\\\u00e4in edellytykset\\\\n\\\\n\\\\nliikunnan edist\\\\u00e4miseen kunnassa ovat kaikilta osin\\\\n\\\\nhyv\\\\u00e4n k\\\\u00e4yt\\\\u00e4nn\\\\u00f6n ja laadun mukaiset.\\\\n\\\\n### sitoutuminen\\\\nKansallisia julkaisuja k\\\\u00e4sitell\\\\u00e4\\\\u00e4n kunnan liikunnan\\\\nedist\\\\u00e4misest\\\\u00e4 vastaavassa ty\\\\u00f6ryhm\\\\u00e4ss\\\\u00e4\\\\nValtioneuvoston selonteko liikuntapolitiikasta (OKM 2018)\\\\n\\\\n\\\\n57 % 43 %\\\\nK\\\\u00e4velyn ja py\\\\u00f6r\\\\u00e4ilyn edist\\\\u00e4misohjelma (LVM 2018)\\\\n\\\\n\\\\n50 % 43 % 7 %\\\\nEi ole k\\\\u00e4sitelty       Jaettu tiedoksi       Esitelty ja keskusteltu\\\\n\\\\n### Johtaminen\\\\nLiikkumisen ja liikunnan edist\\\\u00e4minen on otettu yhdeksi\\\\nl\\\\u00e4ht\\\\u00f6kohdaksi kunnan yleissuunnitteluun (esim.\\\\nyleiskaava)\\\\n\\\\n## 38 %\\\\nKunnan hyvinvointikertomuksessa on kuvaus\\\\nkuntalaisten liikunta-aktiivisuudesta\\\\n\\\\n## 71 %\\\\n\\\\n\\\\n2020\\\\n\\\\n\\\\n2018\\\\n\\\\n### 36 %\\\\nKunnan liikuntatilojen vapaat vuorot ovat esill\\\\u00e4\\\\nkuntalaisille avoimessa varauskalenterissa\\\\n\\\\n## 79 %\\\\n\\\\n\\\\n\"}, {\"page\": 2, \"text\": \"### seuranta ja arviointi\\\\nMove!-mittausten perusteella fyysiselt\\\\u00e4\\\\nkunnolta heikoimpaan kolmannekseen\\\\n\\\\nkuuluvat 8.-luokkalaiset\\\\n\\\\n## 34 %\\\\n\\\\n\\\\n\\\\n2020\\\\n\\\\n\\\\n2018\\\\n\\\\n\\\\n2016\\\\n\\\\n\\\\n2014\\\\n\\\\n\\\\n2012\\\\n\\\\n\\\\n2010\\\\nKunnassa seurataan lasten ja nuorten\\\\n\\\\nliikunta-aktiivisuutta v\\\\u00e4hint\\\\u00e4\\\\u00e4n\\\\n\\\\nkahden vuoden v\\\\u00e4lein\\\\nLasten ja nuorten liikunta-aktiivisuus\\\\n\\\\nraportoidaan vuosittain osana kunnan\\\\ntoiminta- tai hyvinvointikertomusta\\\\n\\\\n## 36 %\\\\n\\\\n\\\\n\\\\n57 %\\\\n\\\\n\\\\n57%\\\\n\\\\n\\\\n67 %\\\\n\\\\n\\\\n75 %\\\\n\\\\n\\\\n54 %\\\\n\\\\n\\\\n54 %\\\\n\\\\n\\\\n### voimavarat\\\\nLiikunnanohjaushenkil\\\\u00f6st\\\\u00f6n m\\\\u00e4\\\\u00e4r\\\\u00e4\\\\nhenkil\\\\u00f6ty\\\\u00f6vuosina / 10 000 asukasta\\\\n\\\\n## 1,7 HTV\\\\n\\\\n### Osallisuus\\\\nKunnassa toimii liikunta- ja urheiluseurojen,\\\\nyhdistysten ja kunnan s\\\\u00e4\\\\u00e4nn\\\\u00f6llisesti kokoontuva\\\\nyhteiselin esim. seuraparlamentti\\\\n\\\\n## 15 %\\\\n\\\\n\\\\n2020\\\\n\\\\n\\\\n2018\\\\n\\\\n### 14 %\\\\nKunnan asukasfoorumeita tai kuntalaisraatia\\\\n\\\\nk\\\\u00e4ytet\\\\u00e4\\\\u00e4n liikuntapalveluiden kehitt\\\\u00e4miseen\\\\n\\\\n## 86 %\\\\n\\\\n\\\\n### kest\\\\u00e4v\\\\u00e4 kehitys\\\\nLiikuntatoimessa ker\\\\u00e4t\\\\u00e4\\\\u00e4n tietoja liikuntapaikkojen\\\\nymp\\\\u00e4rist\\\\u00f6kuormituksesta\\\\n\\\\n## 28 %\\\\n\\\\n### liikunta\\\\u00ad neuvonta\\\\nKunnassa j\\\\u00e4rjestet\\\\u00e4\\\\u00e4n liikuntapalveluiden ja\\\\nsosiaali- ja terveyspalveluiden yhteisty\\\\u00f6n\\\\u00e4\\\\nliikuntaneuvontaa\\\\n\\\\n## 54 %\\\\nKatso oman kuntasi vertailutiedot ja tee\\\\n\\\\nkonkreettiset toimenpide-ehdotukset:\\\\n\\\\n\\\\n**www.teaviisari.fi**\\\\n\\\\n\\\\n\"}]}', 'metadata': '{\"language\": \"fi\", \"title\": \"Terveytt\\\\u00e4 edist\\\\u00e4v\\\\u00e4 liikunta Etel\\\\u00e4-Savon kunnissa 2020\", \"year\": \"2020\", \"publisher\": [\"Terveyden ja hyvinvoinnin laitos\"], \"type_coar\": \"research report\"}'}) (input_keys={'content'}): Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: {\"reasoning\": \"The document is a report titled \\\"TEAviisari\\\" focusing on physical activity in Eastern Savo. The language is Finnish. The title is extracted from the first page, after cleaning and combining lines. Alternative titles are not present. The creator is not explicitly stated, so the first five non-empty lines on page 1 are scanned for names. The year is extracted from the creation date (2020). The publisher is identified as the awarding university. DOI and ISBNs are not present. ISSN is not present. The type_coar is determined to be a research report, based on the content and keywords. The concatenated text of the document starts with \\\"#### TEAviisari\\\" and contains the characters \\\"ä\\\", \\\"ö\\\", and \\\"Å\\\", therefore the language is set to \\\"fi\\\".\", \"language\": \"fi\", \"title\": \"TEAviisari\\n# Terveytt\\u00e4 edist\\u00e4v\\u00e4 liikunta ETEL\\u00c4-SAVON KUNNISSA 2020\\nLiikunnan edist\\u00e4misaktiivisuus Suomen kunnissa\\nTEAviisari 2020 -tiedonkeruussa\\nHyv\\u00e4 tulos*\\nParannettavaa\\nTieto puuttuu\\n\\n\\n- Pistem\\u00e4\\u00e4r\\u00e4 yli 75, kun tavoite 100. N\\u00e4in edellytykset\\n\\n\\nliikunnan edist\\u00e4miseen kunnassa ovat kaikilta osin\\n\\nhyv\\u00e4n k\\u00e4yt\\u00e4nn\\u00f6n ja laadun mukaiset.\\n\\n### sitoutuminen\\nKansallisia julkaisuja k\\u00e4sitell\\u00e4\\u00e4n kunnan liikunnan\\nedist\\u00e4misest\\u00e4 vastaavassa ty\\u00f6ryhm\\u00e4ss\\u00e4\\nValtioneuvoston selonteko liikuntapolitiikasta (OKM 2018)\\n\\n\\n57 % 43 %\\nK\\u00e4velyn ja py\\u00f6r\\u00e4ilyn edist\\u00e4misohjelma (LVM 2018)\\n\\n\\n50 % 43 % 7 %\\nEi ole k\\u00e4sitelty       Jaettu tiedoksi       Esitelty ja keskusteltu\\n\\n### Johtaminen\\nLiikkumisen ja liikunnan edist\\u00e4minen on otettu yhdeksi\\nl\\u00e4ht\\u00f6kohdaksi kunnan yleissuunnitteluun (esim.\\nyleiskaava)\\n\\n## 38 %\\n\\n\\nKunnan hyvinvointikertomuksessa on kuvaus\\nkuntalaisten liikunta-aktiivisuudesta\\n\\n## 71 %\\n\\n\\n\\n2020\\n\\n\\n2018\\n\\n### 36 %\\nKunnan liikuntatilojen vapaat vuorot ovat esill\\u00e4\\nkuntalaisille avoimessa varauskalenterissa\\n\\n## 79 %\\n\\n\\n### seuranta ja arviointi\\nMove!-mittausten perusteella fyysiselt\\u00e4\\nkunnolta heikoimpaan kolmannekseen\\n\\nkuuluvat 8.-luokkalaiset\\n\\n## 34 %\\n\\n\\n\\n57 %\\n\\n\\n57%\\n\\n\\n67 %\\n\\n\\n75 %\\n\\n\\n54 %\\n\\n\\n54 %\\n\\n\\n### voimavarat\\nLiikunnanohjaushenkil\\u00f6st\\u00f6n m\\u00e4\\u00e4r\\u00e4\\nhenkil\\u00f6ty\\u00f6vuosina / 10 0 \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, language, title, alt_title, creator, year, publisher, doi, e_isbn, p_isbn, e_issn, p_issn, type_coar] \n",
      "\n",
      "Actual output fields parsed from the LM response: [reasoning, language, title] \n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 38, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 128, in __call__\n",
      "    return self._call_postprocess(processed_signature, signature, outputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 89, in _call_postprocess\n",
      "    value = self.parse(processed_signature, text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 197, in parse\n",
      "    raise AdapterParseError(\n",
      "dspy.utils.exceptions.AdapterParseError: Adapter ChatAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: [[ ## reasoning ## ]]\n",
      "The document is a report titled \"TEAviisari\" focusing on physical activity promotion in Eastern Savo. The language is Finnish. The title is extracted from the PDF metadata and supplemented with lines from page 1. Alternative titles are not explicitly present. The creator is extracted from the first five non-empty lines of page 1. The year is extracted from the creation date. The publisher is identified as the awarding university. The DOI is not present. ISBNs (both electronic and print) are extracted. The ISSN is not present. The resource type is classified as a research report. \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, language, title, alt_title, creator, year, publisher, doi, e_isbn, p_isbn, e_issn, p_issn, type_coar] \n",
      "\n",
      "Actual output fields parsed from the LM response: [reasoning] \n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/json_adapter.py\", line 78, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 46, in __call__\n",
      "    raise e\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 38, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 128, in __call__\n",
      "    return self._call_postprocess(processed_signature, signature, outputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 89, in _call_postprocess\n",
      "    value = self.parse(processed_signature, text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/json_adapter.py\", line 172, in parse\n",
      "    raise AdapterParseError(\n",
      "dspy.utils.exceptions.AdapterParseError: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: {\"reasoning\": \"The document is a report titled \\\"TEAviisari\\\" about physical activity in Eastern Savo. The language is Finnish. The title is extracted from the first page. Alternative titles are not explicitly present. The creator is not explicitly listed, so I extracted names from the first five non-empty lines of page 1. The year is 2020. The publisher is not explicitly listed, so I extracted the university name. DOI is not present. ISBNs are not present. ISSN is not present. The type_coar is a research report. The concatenated text of the document contains the Finnish characters ä, ö, Ä, Ö, å, Å, so the language is set to \\\"fi\\\".\", \"language\": \"fi\", \"title\": \"TEAviisari\\n# Terveytt\\u00e4 edist\\u00e4v\\u00e4 liikunta ETEL\\u00c4-SAVON KUNNISSA 2020\\nLiikunnan edist\\u00e4misaktiivisuus Suomen kunnissa\\nTEAviisari 2020 -tiedonkeruussa\\nHyv\\u00e4 tulos*\\nParannettavaa\\nTieto puuttuu\\n\\n\\n- Pistem\\u00e4\\u00e4r\\u00e4 yli 75, kun tavoite 100. N\\u00e4in edellytykset\\n\\n\\nliikunnan edist\\u00e4miseen kunnassa ovat kaikilta osin\\n\\nhyv\\u00e4n k\\u00e4yt\\u00e4nn\\u00f6n ja laadun mukaiset.\\n\\n### sitoutuminen\\nKansallisia julkaisuja k\\u00e4sitell\\u00e4\\u00e4n kunnan liikunnan\\nedist\\u00e4misest\\u00e4 vastaavassa ty\\u00f6ryhm\\u00e4ss\\u00e4\\nValtioneuvoston selonteko liikuntapolitiikasta (OKM 2018)\\n\\n\\n57 % 43 %\\nK\\u00e4velyn ja py\\u00f6r\\u00e4ilyn edist\\u00e4misohjelma (LVM 2018)\\n\\n\\n50 % 43 % 7 %\\nEi ole k\\u00e4sitelty       Jaettu tiedoksi       Esitelty ja keskusteltu\\n\\n### Johtaminen\\nLiikkumisen ja liikunnan edist\\u00e4minen on otettu yhdeksi\\nl\\u00e4ht\\u00f6kohdaksi kunnan yleissuunnitteluun (esim.\\nyleiskaava)\\n\\n## 38 %\\n\\n\\nKunnan hyvinvointikertomuksessa on kuvaus\\nkuntalaisten liikunta-aktiivisuudesta\\n\\n## 71 %\\n\\n\\n\\n2020\\n\\n\\n2018\\n\\n### 36 %\\nKunnan liikuntatilojen vapaat vuorot ovat esill\\u00e4\\nkuntalaisille avoimessa varauskalenterissa\\n\\n## 79 %\\n\\n\\n### seuranta ja arviointi\\nMove!-mittausten perusteella fyysiselt\\u00e4\\nkunnolta heikoimpaan kolmannekseen\\n\\nkuuluvat 8.-luokkalaiset\\n\\n## 34 %\\n\\n\\n\\n57 %\\n\\n\\n57%\\n\\n\\n67 %\\n\\n\\n75 %\\n\\n\\n54 %\\n\\n\\n54 %\\n\\n\\n### voimavarat\\nLiikunnanohjaushenkil\\u00f6st\\u00f6n m\\u00e4\\u00e4r\\u00e4\\nhenkil\\u00f6ty\\u00f6vuosina / 10 000 asukasta\\n\\n## 1,7 HTV\\n\\n \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, language, title, alt_title, creator, year, publisher, doi, e_isbn, p_isbn, e_issn, p_issn, type_coar] \n",
      "\n",
      "Actual output fields parsed from the LM response: [reasoning, language, title] \n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/parallelizer.py\", line 55, in safe_func\n",
      "    return user_function(item)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/evaluate/evaluate.py\", line 158, in process_item\n",
      "    prediction = program(**example.inputs())\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/primitives/module.py\", line 78, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/predict/chain_of_thought.py\", line 37, in forward\n",
      "    return self.predict(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/predict/predict.py\", line 103, in __call__\n",
      "    return super().__call__(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/primitives/module.py\", line 78, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/predict/predict.py\", line 192, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 47, in __call__\n",
      "    return JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/json_adapter.py\", line 82, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 46, in __call__\n",
      "    raise e\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/chat_adapter.py\", line 38, in __call__\n",
      "    return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 128, in __call__\n",
      "    return self._call_postprocess(processed_signature, signature, outputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/base.py\", line 89, in _call_postprocess\n",
      "    value = self.parse(processed_signature, text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/utils/callback.py\", line 326, in sync_wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/wrk-vakka/users/oisuomin/git/FinGreyLit/GreyLitLM/dspy/venv/lib/python3.12/site-packages/dspy/adapters/json_adapter.py\", line 172, in parse\n",
      "    raise AdapterParseError(\n",
      "dspy.utils.exceptions.AdapterParseError: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: {\"reasoning\": \"The document is a report titled \\\"TEAviisari\\\" focusing on physical activity in Eastern Savo. The language is Finnish. The title is extracted from the first page, after cleaning and combining lines. Alternative titles are not present. The creator is not explicitly stated, so the first five non-empty lines on page 1 are scanned for names. The year is extracted from the creation date (2020). The publisher is identified as the awarding university. DOI and ISBNs are not present. ISSN is not present. The type_coar is determined to be a research report, based on the content and keywords. The concatenated text of the document starts with \\\"#### TEAviisari\\\" and contains the characters \\\"ä\\\", \\\"ö\\\", and \\\"Å\\\", therefore the language is set to \\\"fi\\\".\", \"language\": \"fi\", \"title\": \"TEAviisari\\n# Terveytt\\u00e4 edist\\u00e4v\\u00e4 liikunta ETEL\\u00c4-SAVON KUNNISSA 2020\\nLiikunnan edist\\u00e4misaktiivisuus Suomen kunnissa\\nTEAviisari 2020 -tiedonkeruussa\\nHyv\\u00e4 tulos*\\nParannettavaa\\nTieto puuttuu\\n\\n\\n- Pistem\\u00e4\\u00e4r\\u00e4 yli 75, kun tavoite 100. N\\u00e4in edellytykset\\n\\n\\nliikunnan edist\\u00e4miseen kunnassa ovat kaikilta osin\\n\\nhyv\\u00e4n k\\u00e4yt\\u00e4nn\\u00f6n ja laadun mukaiset.\\n\\n### sitoutuminen\\nKansallisia julkaisuja k\\u00e4sitell\\u00e4\\u00e4n kunnan liikunnan\\nedist\\u00e4misest\\u00e4 vastaavassa ty\\u00f6ryhm\\u00e4ss\\u00e4\\nValtioneuvoston selonteko liikuntapolitiikasta (OKM 2018)\\n\\n\\n57 % 43 %\\nK\\u00e4velyn ja py\\u00f6r\\u00e4ilyn edist\\u00e4misohjelma (LVM 2018)\\n\\n\\n50 % 43 % 7 %\\nEi ole k\\u00e4sitelty       Jaettu tiedoksi       Esitelty ja keskusteltu\\n\\n### Johtaminen\\nLiikkumisen ja liikunnan edist\\u00e4minen on otettu yhdeksi\\nl\\u00e4ht\\u00f6kohdaksi kunnan yleissuunnitteluun (esim.\\nyleiskaava)\\n\\n## 38 %\\n\\n\\nKunnan hyvinvointikertomuksessa on kuvaus\\nkuntalaisten liikunta-aktiivisuudesta\\n\\n## 71 %\\n\\n\\n\\n2020\\n\\n\\n2018\\n\\n### 36 %\\nKunnan liikuntatilojen vapaat vuorot ovat esill\\u00e4\\nkuntalaisille avoimessa varauskalenterissa\\n\\n## 79 %\\n\\n\\n### seuranta ja arviointi\\nMove!-mittausten perusteella fyysiselt\\u00e4\\nkunnolta heikoimpaan kolmannekseen\\n\\nkuuluvat 8.-luokkalaiset\\n\\n## 34 %\\n\\n\\n\\n57 %\\n\\n\\n57%\\n\\n\\n67 %\\n\\n\\n75 %\\n\\n\\n54 %\\n\\n\\n54 %\\n\\n\\n### voimavarat\\nLiikunnanohjaushenkil\\u00f6st\\u00f6n m\\u00e4\\u00e4r\\u00e4\\nhenkil\\u00f6ty\\u00f6vuosina / 10 0 \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, language, title, alt_title, creator, year, publisher, doi, e_isbn, p_isbn, e_issn, p_issn, type_coar] \n",
      "\n",
      "Actual output fields parsed from the LM response: [reasoning, language, title] \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 113.57 / 181 (62.7%): 100%|██████████| 182/182 [02:24<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 10:31:22 INFO dspy.evaluate.evaluate: Average Metric: 113.57045454545464 / 182 (62.4%)\n",
      "2025/09/30 10:31:22 WARNING dspy.evaluate.evaluate: Skipping table display since `pandas` is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3.87 s, sys: 682 ms, total: 4.55 s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=metadata_metric_with_feedback,\n",
    "    num_threads=64,\n",
    "    display_table=True,\n",
    "    display_progress=True,\n",
    "    provide_traceback=True\n",
    ")\n",
    "\n",
    "eval_result = evaluate(optimized_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d172f585-0442-4a4e-9295-bdedfe21fc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-09-30T10:31:22.596853]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `content` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `language` (str): The language of the resource expressed as a BCP47 language tag.\n",
      "3. `title` (str): The main title of the publication.\n",
      "4. `alt_title` (list[str]): Alternative or parallel titles of the publication, suffixed with a BCP47 language tag in curly brackets.\n",
      "5. `creator` (list[str]): The primary author(s) of the resource (order: Last Name, First Names).\n",
      "6. `year` (Union[str, NoneType]): The year on which the resource was issued or made available.\n",
      "7. `publisher` (list[str]): The entity/entities responsible for making the resource available.\n",
      "8. `doi` (Union[str, NoneType]): The Digital Object Identifier (DOI) associated with the resource.\n",
      "9. `e_isbn` (list[str]): The ISBN associated with the electronic resource.\n",
      "10. `p_isbn` (list[str]): The ISBN of the printed version of this document.\n",
      "11. `e_issn` (Union[str, NoneType]): The ISSN associated with the electronic resource.\n",
      "12. `p_issn` (Union[str, NoneType]): The ISSN of the printed version of this document.\n",
      "13. `type_coar` (str): The type of the resource according to the COAR Resource Types classification.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "Inputs will have the following structure:\n",
      "\n",
      "[[ ## content ## ]]\n",
      "{content}\n",
      "\n",
      "Outputs will be a JSON object with the following fields.\n",
      "\n",
      "{\n",
      "  \"reasoning\": \"{reasoning}\",\n",
      "  \"language\": \"{language}\",\n",
      "  \"title\": \"{title}\",\n",
      "  \"alt_title\": \"{alt_title}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"creator\": \"{creator}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"year\": \"{year}        # note: the value you produce must adhere to the JSON schema: {\\\"anyOf\\\": [{\\\"type\\\": \\\"string\\\"}, {\\\"type\\\": \\\"null\\\"}]}\",\n",
      "  \"publisher\": \"{publisher}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"doi\": \"{doi}        # note: the value you produce must adhere to the JSON schema: {\\\"anyOf\\\": [{\\\"type\\\": \\\"string\\\"}, {\\\"type\\\": \\\"null\\\"}]}\",\n",
      "  \"e_isbn\": \"{e_isbn}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"p_isbn\": \"{p_isbn}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"e_issn\": \"{e_issn}        # note: the value you produce must adhere to the JSON schema: {\\\"anyOf\\\": [{\\\"type\\\": \\\"string\\\"}, {\\\"type\\\": \\\"null\\\"}]}\",\n",
      "  \"p_issn\": \"{p_issn}        # note: the value you produce must adhere to the JSON schema: {\\\"anyOf\\\": [{\\\"type\\\": \\\"string\\\"}, {\\\"type\\\": \\\"null\\\"}]}\",\n",
      "  \"type_coar\": \"{type_coar}\"\n",
      "}\n",
      "In adhering to this structure, your objective is: \n",
      "        markdown\n",
      "        # 📋 Task – Structured Bibliographic Metadata Extraction (Re‑specified)\n",
      "        \n",
      "        You will receive **one JSON object** that represents a PDF document.  \n",
      "        The object has exactly two top‑level keys:\n",
      "        \n",
      "        | Key      | Description |\n",
      "        |----------|-------------|\n",
      "        | `pdfinfo`| Metadata that was extracted directly from the PDF file (e.g. `title`, `author`, `creationDate`, `modDate`). |\n",
      "        | `pages`  | A list of page objects. Each page object contains `page` (the page number) and `text` (the OCR‑extracted plain‑text of that page). |\n",
      "        \n",
      "        Your job is to produce **one JSON object** that follows the schema below.  \n",
      "        If a field cannot be determined, use the exact empty value indicated (`null` for scalars, `[]` for lists).  \n",
      "        All string values must be plain ASCII – normalise quotes to `\"` (or the apostrophe `’`), collapse multiple spaces to a single space, and trim leading/trailing whitespace.\n",
      "        \n",
      "        ---\n",
      "        \n",
      "        ## Output JSON Schema\n",
      "        \n",
      "        | Field | Type | Required format / rules |\n",
      "        |-------|------|--------------------------|\n",
      "        | `language` | string | ISO‑639‑1 code. Scan **the first 200 characters of the *concatenated* text of the whole document** (i.e. `pages[0].text + pages[1].text + …`). If any of the characters **ä ö Ä Ö å Å** appear, set to `\"fi\"`; otherwise `\"en\"`. |\n",
      "        | `title` | string | Main title of the work. <br>1. If `pdfinfo.title` exists → clean it and use it. <br>2. Otherwise, read the text of page 1 (`pages[0].text`). Split it into lines (preserve order). <br>   * Skip empty lines. <br>   * Skip lines that start with a markdown heading marker (`#`, `##`, `###`, …). <br>   * Skip lines that look like an author line (see *Creator extraction*). <br>   * The **first** remaining line that is ≥ 6 characters becomes the *candidate* title. <br>3. **Title‑line extensions** – if the candidate line ends with a colon **or** the next line is non‑empty, starts with a capital letter and is not an author line, concatenate it (single space). Repeat while the same condition holds. <br>4. If a line contains one of the explicit title markers `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` (case‑insensitive), take the text **after the colon** (trimmed) as the title (overriding step 2). <br>5. Keep any subtitle as part of the title (do **not** split on the colon). <br>6. Clean the final string (collapse spaces, normalise quotes). |\n",
      "        | `alt_title` | list of strings | Any **alternative** titles, e.g. a translation, a subtitle given in another language, or a title that appears inside quotation marks (`“ ”`, `\" \"`). Return each cleaned title as a separate element. Do **not** duplicate the main title. |\n",
      "        | `creator` | list of strings | Authors in **“Surname, Given‑Name”** order. <br>1. If `pdfinfo.author` exists → split on commas, semicolons, the word “and”, or line‑breaks. <br>2. Clean each fragment (trim, collapse spaces). <br>3. For each name: <br>   * If it matches the pattern `First Last` (two words, each starting with a capital letter) → reorder to `Last, First`. <br>   * If it already matches `Last, First` keep as‑is. <br>4. If `pdfinfo.author` is missing, scan the **first five non‑empty lines of page 1** for personal‑name patterns (`First Last`, `Last, First`, or a line that starts with a capitalised list of names). Collect **all** matches. <br>5. Remove duplicates, keep order of appearance. |\n",
      "        | `year` | integer or null | Publication year. <br>1. If `pdfinfo.creationDate` or `pdfinfo.modDate` exists, extract the **first four digits** (they are always a year) and use that. <br>2. If both are missing, search the whole document for the **first** four‑digit number between 1900‑2099 that appears in a publication‑information context (e.g. after “Year:”, “©”, “© 2021”, “2021.”). If none, return `null`. |\n",
      "        | `publisher` | list of strings | Institution responsible for the work. <br>• **Theses / dissertations** → the awarding university or faculty (e.g. “University of Vaasa”). Detect by looking for keywords: “University”, “Yliopisto”, “Universität”, “Akademi”, “Institute”, “College”, “School”, “Faculty”. Return each distinct institution once, preserving order of first appearance. <br>• **Research reports** → the organisation that produced the report (same keyword list). <br>• **Journal articles** → `[]`. |\n",
      "        | `doi` | string or null | DOI if present. Detect case‑insensitive pattern `10\\.\\d{4,9}/\\S+`. If the match is preceded by a URL (`http://`, `https://`, `doi.org/`), strip that part. Remove trailing punctuation characters `.,;:`. Return the bare DOI (e.g. `10.1000/xyz123`). |\n",
      "        | `e_isbn` | list of strings | **Electronic** ISBN‑13 numbers. Detect ISBN‑13 (13 digits, hyphens optional) with the regex `\\b(?:97[89][-\\s]?)?\\d{1,5}[-\\s]?\\d{1,7}[-\\s]?\\d{1,7}[-\\s]?\\d\\b`. For each match, look at up to **30 characters before and after** the match. If any of the **electronic cues** appear, add the ISBN (with **all hyphens and spaces removed**) to `e_isbn`. Electronic cues (case‑insensitive): `digital`, `electronic`, `e‑ISBN`, `(digital)`, `pdf`, `PDF`, `sid.` (when used for electronic), `online`. |\n",
      "        | `p_isbn` | list of strings | **Print** ISBN‑13 numbers. Same detection as `e_isbn` but require at least one **print cue** in the ±30‑character window. Print cues (case‑insensitive): `print`, `paper`, `hardcover`, `(print)`, `Painettu`, `Print`. If both electronic and print cues are present, **electronic wins** (the ISBN goes only to `e_isbn`). Store the ISBN without hyphens/spaces. |\n",
      "        | `e_issn` | string or null | Electronic ISSN (8 digits, optional hyphen). Detect with `\\b\\d{4}[-\\s]?\\d{3}[\\dX]\\b`. Apply the same cue logic as for ISBN. Return the ISSN **exactly as it appears** (keep the hyphen if present). |\n",
      "        | `p_issn` | string or null | Print ISSN (same detection, print‑cue logic). Return the ISSN exactly as it appears (keep hyphen). |\n",
      "        | `type_coar` | string | COAR‑compatible resource type. Scan the **entire document** (case‑insensitive) and apply the **first** matching rule in this order: <br>1. **doctoral thesis** – contains any of: “doctoral thesis”, “dissertation”, “PhD”, “doctoral”, “väitöskirja”, “väitöskirjan”. <br>2. **master thesis** – contains any of: “master’s thesis”, “master thesis”, “maisteri”, “maisterintutkielma”. <br>3. **journal article** – contains typical journal citation elements (journal name, volume, issue, pages) **or** the word “article” together with a DOI or ISSN, **or** a pattern like “Vol. X, No. Y, pp. Z‑W”. <br>4. **conference proceeding** – contains “conference”, “proceedings”, “paper presented at”. <br>5. **research report** – contains “report”, “raportti”, “tutkimusraportti”, “research report”. <br>6. **research** – fallback for any other research‑type document. <br>Return the exact lower‑case string (e.g. `doctoral thesis`). |\n",
      "        \n",
      "        ---\n",
      "        \n",
      "        ## Extraction Procedure (Step‑by‑Step)\n",
      "        \n",
      "        1. **Parse the input JSON** safely. Ignore any keys that are not listed above.  \n",
      "        2. **Normalise dates**: `creationDate` / `modDate` are strings like `D:20201216144002+02'00'`. Extract the first four digits as the year (they are always at the start of the string).  \n",
      "        3. **Detect language** using the rule in the schema (first 200 characters of concatenated text).  \n",
      "        4. **Title extraction** (see detailed rules under the `title` field). Pay special attention to explicit markers (`Title:`, `Thesis:` etc.).  \n",
      "        5. **Alternative titles** – look for quoted strings (`“…”`, `\"...\"`) anywhere in the document, and for subtitles that appear on a separate line after a colon. Do not duplicate the main title.  \n",
      "        6. **Creator extraction** – follow the rules under `creator`. When scanning for names on page 1, use the following regular expressions (case‑sensitive): <br>`\\b[A-Z][a-z]+ [A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (First Last) <br>`\\b[A-Z][a-z]+, *[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b` (Last, First). Collect all matches, then apply the ordering rule.  \n",
      "        7. **Year** – apply the rule in the schema. If you have to fall back to searching the text, ignore years that appear inside parentheses of a citation (e.g. “(2020)” after a reference). Prefer the first plausible year after the title block.  \n",
      "        8. **Publisher detection** – first determine `type_coar`. If the document is a thesis, search the whole text for the first occurrence of a university/faculty keyword list and capture the whole phrase (e.g. “University of Vaasa”, “Åbo Akademi University”). Return it as a single‑element list. For research reports, look for the organisation name that appears near the title or in a header/footer. For journal articles, return an empty list.  \n",
      "        9. **ISBN / ISSN extraction** – use the regexes supplied. For each match, extract the surrounding 30‑character context and decide electronic vs print using the cue lists. Remove hyphens/spaces from ISBNs before storing; keep ISSNs exactly as found (including hyphen). Ensure each identifier appears only once in the appropriate list.  \n",
      "        10. **DOI extraction** – apply the DOI regex, strip leading URL parts, and trailing punctuation. Return the bare DOI in lower‑case. If none, set to `null`.  \n",
      "        11. **Resource type (`type_coar`)** – apply the ordered rule list exactly as described. The first category that matches determines the value.  \n",
      "        12. **Assemble the output** JSON. Preserve the key order shown in the schema for readability (order is not technically required, but it helps testing). Use `null` for missing scalar values and `[]` for missing list values.\n",
      "        \n",
      "        ---\n",
      "        \n",
      "        ## Cleaning & Normalisation Details\n",
      "        \n",
      "        * **Whitespace** – collapse any sequence of whitespace characters (spaces, tabs, newlines) to a single space. Trim leading/trailing spaces.  \n",
      "        * **Quotes** – replace any fancy quotation marks (`“ ” ‘ ’ „ “ …`) with plain ASCII `\"` (or the apostrophe `’`).  \n",
      "        * **Hyphens in identifiers** – remove all hyphens (`-`) and spaces from ISBNs before storing. **Do not** remove hyphens from ISSNs; keep them exactly as they appear.  \n",
      "        * **Case** – identifiers (DOI) are stored in lower‑case; ISBN/ISSN are numeric/alphabetic only, so case does not matter.  \n",
      "        \n",
      "        ---\n",
      "        \n",
      "        ## Common Pitfalls (What Previously Went Wrong)\n",
      "        \n",
      "        | Issue | What caused it | Correct handling |\n",
      "        |-------|----------------|-----------------|\n",
      "        | **Wrong language** | Scanned only the first page instead of the first 200 characters of the whole document. | Concatenate all page texts, take the first 200 characters, then apply the Finnish‑character rule. |\n",
      "        | **Title wrong** | Ignored explicit title markers (`Thesis:`) and concatenated the whole heading line. | If a line contains `Title:`, `Thesis:`, `Master’s Thesis:`, `Doctoral Thesis:` (case‑insensitive), use the text after the colon as the title. Otherwise follow the candidate‑line logic. |\n",
      "        | **Alt‑titles missing** | Did not look for quoted strings or subtitle lines. | Search the whole document for text inside `“ ”` or `\" \"` and for subtitle lines that appear after a colon on a separate line. |\n",
      "        | **Creator over‑inclusion** | Added non‑author names (e.g., supervisors) or failed to reorder names. | Only use `pdfinfo.author` or name patterns on page 1. Do **not** include lines that contain words like “Supervisor”, “Advisor”, “Editor”. Reorder `First Last` → `Last, First`. |\n",
      "        | **Publisher wrong** | Took the first institution mentioned anywhere, even if it was a journal publisher. | First determine `type_coar`. If it is a thesis, look for the awarding university/faculty; for reports, look for the producing organisation; for journal articles return `[]`. |\n",
      "        | **ISBN/ISSN classification** | Assigned identifiers to the wrong list because cues were not checked, or removed hyphens from ISSNs. | Examine the ±30‑character window for electronic or print cues. If both appear, assign to `e_isbn` (electronic wins). Keep ISSNs exactly as they appear (including hyphen). |\n",
      "        | **DOI extraction** | Kept surrounding punctuation or URL parts. | Strip any leading `http://`, `https://`, `doi.org/` and trailing punctuation `.,;:`. Return the bare DOI in lower‑case. |\n",
      "        | **type_coar mis‑classification** | Applied “article” rule too early, causing theses to be labelled as `doctoral thesis`. | Follow the ordered list strictly: **doctoral thesis** → **master thesis** → **journal article** → **conference proceeding** → **research report** → **research**. The first matching rule wins. |\n",
      "        \n",
      "        ---\n",
      "        \n",
      "        **Remember:**  \n",
      "        - Follow the **order of precedence** for each field exactly as described.  \n",
      "        - Use the cue‑based approach for ISBN/ISSN to decide electronic vs print.  \n",
      "        - Detect the resource type in the exact order listed; the first match determines `type_coar`.  \n",
      "        - Return `null` for missing scalar values and `[]` for missing list values.  \n",
      "        \n",
      "        Good luck! 🎯\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## content ## ]]\n",
      "{\"pdfinfo\": {\"title\": \"ESS\\u00c4: L\\u00e5t havet forts\\u00e4tta vara v\\u00e4gen - Vasabladet\", \"author\": \"Johanna Elisabet Glader\", \"creationDate\": \"D:20200224161628+02'00'\", \"modDate\": \"D:20200224162707+02'00'\"}, \"pages\": [{\"page\": 1, \"text\": \"# _This is an electronic reprint of the original article. This reprint may differ from_ the original in pagination and typographic detail .\\n_**Please cite the original version**_ **:**\\nRasmus Karlsson (2019). _L\\u00e5t havet forts\\u00e4tta vara v\\u00e4gen._ Vasabladet, 3.8.2019, s. 24.\\n\\n\\n\"}, {\"page\": 2, \"text\": \"ESS\\u00c4: L\\u00e5t havet forts\\u00e4tta vara v\\u00e4gen - Vasabladet https://www.vasabladet.fi/Artikel/Visa/306477?shareID=168248-09875c...\\n**Aim-6 NH** **Hydraulics LT 2,5** **Trail Lite Large** **Prolite Plus Large**\\n\\n\\n59 \\u20ac **KATSO NYT** 45 \\u20ac **KATSO NYT** 40 \\u20ac **KATSO NYT** 79,90 \\u20ac **KATSO NYT** 139 \\u20ac **KATSO NYT**\\n**Prolite Regular**\\n**Trail Lite Women's**\\n**Regular** **MCB** **Speedcross 4** **Arrow 6**\\n\\n\\n\\n109 \\u20ac **KATSO NYT** 76,90 \\u20ac **KATSO NYT** 30 \\u20ac **KATSO NYT** 75 \\u20ac **KATSO NYT** 65 \\u20ac **KATSO NYT**\\nSk\\u00e4rg\\u00e5rdscentrum Korpostr\\u00f6m har renoverat och f\\u00f6rnyat sina g\\u00e4stbryggor inom projektet Smart Marina. Foto: Thomas Karlsson\\nEn gemensam utmaning f\\u00f6r turismf\\u00f6retagare i Finland \\u00e4r den ofta v\\u00e4ldigt korta s\\u00e4songen. Det\\ng\\u00e4ller i synnerhet de f\\u00f6retagare som driver g\\u00e4sthamnar. Man talar om att s\\u00e4songen str\\u00e4cker\\nsig fr\\u00e5n midsommar till skolstart, cirka \\u00e5tta veckor. Det \\u00e4r en kort tid att tj\\u00e4na sitt levebr\\u00f6d p\\u00e5,\\nskriver Rasmus Karlsson, projektledare f\\u00f6r Smart Marina vid Novia.\\nProjektet Smart Marina, d\\u00e4r Yrkesh\\u00f6gskolan Novia fungerar som finl\\u00e4ndsk koordinator, str\\u00e4var\\ntill att f\\u00f6rb\\u00e4ttra f\\u00f6ruts\\u00e4ttningarna f\\u00f6r turismf\\u00f6retagarnas verksamhet.\\nNaturligtvis \\u00e4r dessa f\\u00f6retagare i allm\\u00e4nhet m\\u00e5ngsysslare som \\u00e4ven driver annan verksamhet\\nunder \\u00f6vriga tider av \\u00e5ret, till exempel fiske, restaurangverksamhet,\\njordbruk eller byggnadsarbete. En f\\u00f6ruts\\u00e4ttning f\\u00f6r att bedriva g\\u00e4sthamnsverksamheten \\u00e4r\\n\\u00e4nd\\u00e5 oftast att den inte g\\u00e5r med f\\u00f6rlust. Verksamheten \\u00e4r ocks\\u00e5 viktig f\\u00f6r \\u00f6vriga f\\u00f6retag i\\nn\\u00e4romr\\u00e5det som kan dra nytta av de kundstr\\u00f6mmar som hamnen medf\\u00f6r.\\n\\n\\ni m\\u00e5nga av de \\u00e5bol\\u00e4ndska och v\\u00e4stnyl\\u00e4ndska g\\u00e4sthamnarna. Den\\npressade l\\u00f6nsamheten ger d\\u00e5ligt med utrymme f\\u00f6r investeringar. Dels g\\u00e4ller det\\nbasinfrastruktur som bryggor och toaletter. Bryggorna \\u00e4r ofta gamla och i behov av\\nreparationer och f\\u00f6rnyelse.\\n\\n\\n1 of 5 8/14/2019, 10:28 AM\\n\\n\\n\"}, {\"page\": 3, \"text\": \"ESS\\u00c4: L\\u00e5t havet forts\\u00e4tta vara v\\u00e4gen - Vasabladet https://www.vasabladet.fi/Artikel/Visa/306477?shareID=168248-09875c...\\nDen nya g\\u00e4stbryggan p\\u00e5 Stensk\\u00e4r, Sk\\u00e4rg\\u00e5rdshavet med \\u00e4garna Agneta Jansson och Jarmo Ylitalo. Foto: Rasmus Karlsson\\nVarje h\\u00f6ststorm s\\u00e4tter f\\u00f6ljande sommars verksamhet p\\u00e5 spel. Dessutom f\\u00f6rv\\u00e4ntar sig m\\u00e5nga\\nb\\u00e5t\\u00e4gare en h\\u00f6gre serviceniv\\u00e5 \\u00e4n endast en plats att f\\u00f6rt\\u00f6ja vid. El p\\u00e5 bryggan b\\u00f6rjar vara\\nstandard, fr\\u00e4scha dusch- och bastuutrymmen st\\u00e5r ocks\\u00e5 h\\u00f6gt p\\u00e5 listan \\u00f6ver den grundservice\\nsom \\u00f6nskas och rena utrymmen \\u00e4r ocks\\u00e5 det som framkommer som det viktigaste kriteriet f\\u00f6r\\ntrivsel i g\\u00e4sthamnar i f\\u00f6rfr\\u00e5gningar som gjorts bland b\\u00e5tg\\u00e4ster.\\n\\n\\nsamlar 34 hamnar runt centrala \\u00d6stersj\\u00f6n. I Finland finansieras\\nprojektet av det regionala utvecklingsprogrammet Interreg Central Baltic och Egentliga\\nFinlands f\\u00f6rbund. \\u00c5tta av hamnarna finns i den finl\\u00e4ndska sk\\u00e4rg\\u00e5rden, och koordineras av\\nYrkesh\\u00f6gskolan Novia. P\\u00e5 \\u00c5land deltar fjorton hamnar i projektet.\\n\\n\\n**\\\"Projektet har en ansenlig budget f\\u00f6r investeringar i g\\u00e4sthamnarnas**\\n**infrastruktur, med m\\u00e5l att h\\u00f6ja serviceniv\\u00e5n, f\\u00f6rb\\u00e4ttra s\\u00e4kerheten och**\\n**anv\\u00e4nda sig av moderna l\\u00f6sningar f\\u00f6r information och**\\n**betalningsmetoder.\\\"**\\nProjektet har en ansenlig budget f\\u00f6r investeringar i g\\u00e4sthamnarnas infrastruktur, med m\\u00e5l att\\nh\\u00f6ja serviceniv\\u00e5n, f\\u00f6rb\\u00e4ttra s\\u00e4kerheten och anv\\u00e4nda sig av moderna l\\u00f6sningar f\\u00f6r information\\noch betalningsmetoder. Bland investeringsobjekten ing\\u00e5r, f\\u00f6rutom bryggor och\\nservicebyggnader, till exempel solceller, septikt\\u00f6mning, osmosanl\\u00e4ggningar f\\u00f6r renande av\\nhavsvatten till dricksvatten och en mobil applikation f\\u00f6r information och bokning.\\nDessutom utbildas hamnpersonal i hamns\\u00e4kerhet, marknadsf\\u00f6ring och g\\u00f6r studiebes\\u00f6k i\\nandra hamnar f\\u00f6r att se p\\u00e5 servicel\\u00f6sningar, h\\u00e4mta inspiration och utbyta id\\u00e9er.\\n\\n\\n2 of 5 8/14/2019, 10:28 AM\\n\\n\\n\"}, {\"page\": 4, \"text\": \"ESS\\u00c4: L\\u00e5t havet forts\\u00e4tta vara v\\u00e4gen - Vasabladet https://www.vasabladet.fi/Artikel/Visa/306477?shareID=168248-09875c...\\nI framtiden kan man utvidga n\\u00e4tverket till att str\\u00e4cka sig ner l\\u00e4ngs med den baltiska kusten d\\u00e4r\\nprogrammet Est Lat, en motsvarighet till Central Baltic, har finansierat liknande\\nprojekt. Det motsvarande EU-programmet d\\u00e4r \\u00d6sterbotten ing\\u00e5r, Botnia-Atlantica, har inte lika\\ntydligt specificerat sm\\u00e5b\\u00e5tshamnar som m\\u00e5l under programperioden 2014\\u20132020, men\\nunderst\\u00f6der \\u00e4ven de gr\\u00e4ns\\u00f6verskridande n\\u00e4ringsverksamhet.\"}, {\"page\": 5, \"text\": \"ESS\\u00c4: L\\u00e5t havet forts\\u00e4tta vara v\\u00e4gen - Vasabladet https://www.vasabladet.fi/Artikel/Visa/306477?shareID=168248-09875c...\\nFoto: Kati Hiekkanen\\n**Projekt Smart Marina**\\nBudget: 8,4 miljoner euro, varav investeringar 6,1 miljoner euro.\\n\\n\\n34 hamnar deltar, 14 p\\u00e5 \\u00c5land, 8 i Finland, 8 i Sverige och 4 i Estland.\\nProjekttid:1.2.2018\\u201331.1.2021.\\nWebbplats: www.smartmarina.eu\\n\\n\\n\\n**0 kommentarer** Ordna efter\\n**\\u00c4ldsta**\\n\\n\\n\\n4 of 5 8/14/2019, 10:28 AM\\n\\n\\n\"}]}\n",
      "\n",
      "Respond with a JSON object in the following order of fields: `reasoning`, then `language`, then `title`, then `alt_title` (must be formatted as a valid Python list[str]), then `creator` (must be formatted as a valid Python list[str]), then `year` (must be formatted as a valid Python Union[str, NoneType]), then `publisher` (must be formatted as a valid Python list[str]), then `doi` (must be formatted as a valid Python Union[str, NoneType]), then `e_isbn` (must be formatted as a valid Python list[str]), then `p_isbn` (must be formatted as a valid Python list[str]), then `e_issn` (must be formatted as a valid Python Union[str, NoneType]), then `p_issn` (must be formatted as a valid Python Union[str, NoneType]), then `type_coar`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m{\"reasoning\": \"The language is Finnish. The title is extracted from the first line of the first page after cleaning and removing markdown headings. Alternative titles are extracted from quoted strings and subtitle lines. The creator is extracted from the first five non-empty lines of page 1, looking for names in the format \\\"First Last\\\" or \\\"Last, First\\\". The year is extracted from the first four digits. The publisher is identified as \\\"University of Vaasa\\\" based on the keywords in the text. The DOI is not present. The e_isbn is not present. The p_isbn is not present. The e_issn is not present. The p_issn is not present. The type_coar is doctoral thesis because the text contains \\\"väitöskirja\\\".\", \"language\": \"fi\", \"title\": \"ESS\\u00c4: L\\u00e5t havet forts\\u00e4tta vara v\\u00e4gen - Vasabladet\", \"alt_title\": [], \"creator\": [\"Glader\", \"Johanna Elisabet\"], \"year\": \"2019\", \"publisher\": [\"University of Vaasa\"], \"doi\": null, \"e_isbn\": [], \"p_isbn\": [], \"e_issn\": null, \"p_issn\": null, \"type_coar\": \"doctoral thesis\"}\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99baf332-ed43-4947-9183-89fd35bee3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the optimized program for later use (many formats, just in case)\n",
    "optimized_program.save(\"gepa-optimized-module.json\", save_program=False)\n",
    "optimized_program.save(\"gepa-optimized-module.pkl\", save_program=False)\n",
    "# save just the prompt(s)\n",
    "for name, pred in optimized_program.named_predictors():\n",
    "    with open(f\"gepa-optimized-prompt-{name}.txt\", \"w\") as outfile:\n",
    "        outfile.write(pred.signature.instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dcc14b-2e28-43f5-9e4b-e72f25e7675b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingreylit-dspy",
   "language": "python",
   "name": "fingreylit-dspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
