{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Gemma-3-12B-it model using Axolotl framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/appl/easybuild/opt/CUDA/12.6.0\n"
     ]
    }
   ],
   "source": [
    "!printenv CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit\"\n",
    "#SLICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 640 train records\n",
      "Wrote 182 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'assistant', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"axolotl-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"axolotl-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"axolotl-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: axolotl-train.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "test_datasets:\n",
    "  - path: axolotl-eval.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "chat_template: gemma3\n",
    "eot_tokens:\n",
    "  - <end_of_turn>\n",
    "\n",
    "peft_use_dora: true\n",
    "adapter: lora\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 4\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-10-06 11:51:21,860] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:2907303] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-06 11:51:21,861] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:2907303] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-06 11:51:22,205] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:2907303] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-06 11:51:24,063] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:2907303] [RANK:0] Unable to find prepared dataset in last_run_prepared/a1f0f4050f5ca00863d87ef6150ebdb9\u001b[39m\n",
      "[2025-10-06 11:51:24,063] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:2907303] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-06 11:51:24,063] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:2907303] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 640 examples [00:00, 18904.84 examples/s]\n",
      "[2025-10-06 11:51:24,621] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:2907303] [RANK:0] Loading dataset: axolotl-train.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-06 11:51:24,691] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:2907303] [RANK:0] Using chat template:\n",
      "---\n",
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|█| 640/640 [00:12<00:00, 53.11 examples/s\n",
      "[2025-10-06 11:51:37,467] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:2907303] [RANK:0] min_input_len: 388\u001b[39m\n",
      "[2025-10-06 11:51:37,467] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:2907303] [RANK:0] max_input_len: 8994\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 640/640 [00:00<00:00, 1894.27 exa\n",
      "\u001b[33m[2025-10-06 11:51:39,124] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:201] [PID:2907303] [RANK:0] Dropped 5 long samples from dataset\u001b[39m\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 635/635 [00:00<00\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 635/635 [00:00<00\n",
      "Saving the dataset (1/1 shards): 100%|█| 635/635 [00:00<00:00, 13219.75 examples\n",
      "[2025-10-06 11:51:43,298] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:2907303] [RANK:0] Unable to find prepared dataset in last_run_prepared/96cf84ff701e3a0ea3029ba6c0a1b4a7\u001b[39m\n",
      "[2025-10-06 11:51:43,298] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:2907303] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-06 11:51:43,298] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:2907303] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 9900.98 examples/s]\n",
      "[2025-10-06 11:51:43,694] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:2907303] [RANK:0] Loading dataset: axolotl-eval.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-06 11:51:43,760] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:2907303] [RANK:0] Using chat template:\n",
      "---\n",
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:11<00:00,  2.85 examples/s]\n",
      "[2025-10-06 11:51:55,806] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:2907303] [RANK:0] min_input_len: 526\u001b[39m\n",
      "[2025-10-06 11:51:55,806] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:2907303] [RANK:0] max_input_len: 3316\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:00<00:00, 167.79 exampl\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Saving the dataset (1/1 shards): 100%|█| 32/32 [00:00<00:00, 2401.46 examples/s]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-06 11:53:11,511] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:2907303] [RANK:0] gather_len_batches: [200]\u001b[39m\n",
      "[2025-10-06 11:53:11,512] [INFO] [axolotl.utils.data.sft._prepare_standard_dataset:123] [PID:2907303] [RANK:0] Maximum number of steps set at 200\u001b[39m\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:30<00:00,  6.15s/it]\n",
      "[2025-10-06 11:53:54,495] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:2907303] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-06 11:53:54,841] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:2907303] [RANK:0] found linear modules: ['down_proj', 'fc1', 'fc2', 'gate_proj', 'k_proj', 'o_proj', 'out_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-10-06 11:53:54,841] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:2907303] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 76,673,904 || all params: 12,263,998,944 || trainable%: 0.6252\n",
      "[2025-10-06 11:57:57,297] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:2907303] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n",
      "[2025-10-06 11:58:01,534] [WARNING] [accelerate.utils.other.check_os_kernel:441] [PID:2907303] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[2025-10-06 11:58:16,698] [INFO] [axolotl.train.save_initial_configs:403] [PID:2907303] [RANK:0] Pre-saving adapter config to ./out-gemma-3-12b-it...\u001b[39m\n",
      "[2025-10-06 11:58:16,700] [INFO] [axolotl.train.save_initial_configs:407] [PID:2907303] [RANK:0] Pre-saving tokenizer to ./out-gemma-3-12b-it...\u001b[39m\n",
      "[2025-10-06 11:58:17,099] [INFO] [axolotl.train.save_initial_configs:410] [PID:2907303] [RANK:0] Pre-saving model config to ./out-gemma-3-12b-it...\u001b[39m\n",
      "[2025-10-06 11:58:17,109] [INFO] [axolotl.train.save_initial_configs:414] [PID:2907303] [RANK:0] Pre-saving processor to ./out-gemma-3-12b-it...\u001b[39m\n",
      "[2025-10-06 11:58:19,472] [INFO] [axolotl.train.execute_training:225] [PID:2907303] [RANK:0] Starting trainer...\u001b[39m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-06 11:59:24,251] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:2907303] [RANK:0] gather_len_batches: [200]\u001b[39m\n",
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:15,  1.10s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:20,  1.61s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:06<00:22,  1.88s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.06s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.14s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:13<00:19,  2.21s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:17,  2.25s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.29s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:20<00:13,  2.30s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.38s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.36s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.35s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.25307035446167, 'eval_runtime': 38.3255, 'eval_samples_per_second': 0.835, 'eval_steps_per_second': 0.417, 'epoch': 0}\n",
      "  0%|                                                   | 0/200 [00:38<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.34s/it]\u001b[A\n",
      "{'loss': 8.9769, 'grad_norm': 35.46774673461914, 'learning_rate': 0.0, 'epoch': 0.02}0m\u001b[0m\n",
      "  0%|▏                                        | 1/200 [01:32<5:06:05, 92.29s/it][2025-10-06 12:01:36,450] [INFO] [axolotl.utils.callbacks.log_gpu_memory_usage:107] [PID:2907303] [RANK:0] cuda memory usage while training: 23.239GB (+35.236GB cache, +1.326GB misc)\u001b[39m\n",
      "{'loss': 7.467, 'grad_norm': 29.30667495727539, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 6.5783, 'grad_norm': 27.107826232910156, 'learning_rate': 4e-05, 'epoch': 0.06}\n",
      "{'loss': 7.2071, 'grad_norm': 28.453346252441406, 'learning_rate': 6e-05, 'epoch': 0.08}\n",
      "{'loss': 3.724, 'grad_norm': 14.152173042297363, 'learning_rate': 8e-05, 'epoch': 0.1}\n",
      "{'loss': 2.3183, 'grad_norm': 6.76303768157959, 'learning_rate': 0.0001, 'epoch': 0.12}\n",
      "{'loss': 2.1521, 'grad_norm': 6.296454429626465, 'learning_rate': 0.00012, 'epoch': 0.14}\n",
      "{'loss': 1.3862, 'grad_norm': 4.5816216468811035, 'learning_rate': 0.00014, 'epoch': 0.16}\n",
      "{'loss': 1.1437, 'grad_norm': 5.2618727684021, 'learning_rate': 0.00016, 'epoch': 0.18}\n",
      "{'loss': 0.7875, 'grad_norm': 4.026547431945801, 'learning_rate': 0.00018, 'epoch': 0.2}\n",
      "{'loss': 0.5165, 'grad_norm': 4.1609320640563965, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 0.5438, 'grad_norm': 3.400615930557251, 'learning_rate': 0.0001999863304992469, 'epoch': 0.24}\n",
      "{'loss': 0.4147, 'grad_norm': 2.705183267593384, 'learning_rate': 0.00019994532573409262, 'epoch': 0.26}\n",
      "{'loss': 0.3819, 'grad_norm': 2.2992777824401855, 'learning_rate': 0.00019987699691483048, 'epoch': 0.28}\n",
      "{'loss': 0.3031, 'grad_norm': 3.0171327590942383, 'learning_rate': 0.00019978136272187747, 'epoch': 0.3}\n",
      "{'loss': 0.5627, 'grad_norm': 2.3636393547058105, 'learning_rate': 0.000199658449300667, 'epoch': 0.32}\n",
      "{'loss': 0.3352, 'grad_norm': 2.4636034965515137, 'learning_rate': 0.00019950829025450114, 'epoch': 0.34}\n",
      "{'loss': 0.3205, 'grad_norm': 1.9678289890289307, 'learning_rate': 0.00019933092663536382, 'epoch': 0.36}\n",
      "{'loss': 0.2483, 'grad_norm': 1.7441695928573608, 'learning_rate': 0.00019912640693269752, 'epoch': 0.38}\n",
      "{'loss': 0.2126, 'grad_norm': 1.8473049402236938, 'learning_rate': 0.00019889478706014687, 'epoch': 0.4}\n",
      "{'loss': 0.2438, 'grad_norm': 1.907650351524353, 'learning_rate': 0.00019863613034027224, 'epoch': 0.42}\n",
      "{'loss': 0.2488, 'grad_norm': 2.0537643432617188, 'learning_rate': 0.00019835050748723824, 'epoch': 0.44}\n",
      "{'loss': 0.697, 'grad_norm': 2.1502699851989746, 'learning_rate': 0.00019803799658748094, 'epoch': 0.46}\n",
      "{'loss': 0.3292, 'grad_norm': 1.7692095041275024, 'learning_rate': 0.00019769868307835994, 'epoch': 0.48}\n",
      "{'loss': 0.1748, 'grad_norm': 1.1686283349990845, 'learning_rate': 0.0001973326597248006, 'epoch': 0.5}\n",
      " 12%|█████                                   | 25/200 [17:22<1:55:35, 39.63s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.17s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:07<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.08s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.16s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:19,  2.22s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.26s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.30s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.30s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.34s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.33s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.07936982810497284, 'eval_runtime': 37.5862, 'eval_samples_per_second': 0.851, 'eval_steps_per_second': 0.426, 'epoch': 0.5}\n",
      " 12%|█████                                   | 25/200 [17:59<1:55:35, 39.63s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'loss': 0.3648, 'grad_norm': 2.002760887145996, 'learning_rate': 0.00019694002659393305, 'epoch': 0.52}\n",
      "{'loss': 0.344, 'grad_norm': 1.6656560897827148, 'learning_rate': 0.00019652089102773488, 'epoch': 0.54}\n",
      "{'loss': 0.5825, 'grad_norm': 1.5654418468475342, 'learning_rate': 0.00019607536761368484, 'epoch': 0.56}\n",
      "{'loss': 0.2186, 'grad_norm': 1.3034168481826782, 'learning_rate': 0.00019560357815343577, 'epoch': 0.58}\n",
      "{'loss': 0.2566, 'grad_norm': 1.771070122718811, 'learning_rate': 0.00019510565162951537, 'epoch': 0.6}\n",
      "{'loss': 0.3224, 'grad_norm': 1.1553674936294556, 'learning_rate': 0.00019458172417006347, 'epoch': 0.62}\n",
      "{'loss': 0.2055, 'grad_norm': 2.0887136459350586, 'learning_rate': 0.00019403193901161613, 'epoch': 0.64}\n",
      "{'loss': 0.3668, 'grad_norm': 0.9864580035209656, 'learning_rate': 0.0001934564464599461, 'epoch': 0.66}\n",
      "{'loss': 0.2902, 'grad_norm': 1.8022651672363281, 'learning_rate': 0.00019285540384897073, 'epoch': 0.68}\n",
      "{'loss': 0.4517, 'grad_norm': 1.3311656713485718, 'learning_rate': 0.00019222897549773848, 'epoch': 0.7}\n",
      "{'loss': 0.3595, 'grad_norm': 1.6008422374725342, 'learning_rate': 0.00019157733266550575, 'epoch': 0.72}\n",
      "{'loss': 0.374, 'grad_norm': 1.32474946975708, 'learning_rate': 0.00019090065350491626, 'epoch': 0.74}\n",
      "{'loss': 0.3899, 'grad_norm': 2.432384490966797, 'learning_rate': 0.00019019912301329592, 'epoch': 0.76}\n",
      "{'loss': 0.3991, 'grad_norm': 1.6085022687911987, 'learning_rate': 0.00018947293298207635, 'epoch': 0.78}\n",
      "{'loss': 0.2818, 'grad_norm': 1.220136046409607, 'learning_rate': 0.0001887222819443612, 'epoch': 0.8}\n",
      "{'loss': 0.2127, 'grad_norm': 1.2581653594970703, 'learning_rate': 0.0001879473751206489, 'epoch': 0.82}\n",
      "{'loss': 0.2678, 'grad_norm': 1.6854360103607178, 'learning_rate': 0.00018714842436272773, 'epoch': 0.84}\n",
      "{'loss': 0.5058, 'grad_norm': 1.466457724571228, 'learning_rate': 0.00018632564809575742, 'epoch': 0.86}\n",
      "{'loss': 0.2267, 'grad_norm': 1.1286612749099731, 'learning_rate': 0.0001854792712585539, 'epoch': 0.88}\n",
      "{'loss': 0.1846, 'grad_norm': 0.9295051693916321, 'learning_rate': 0.00018460952524209355, 'epoch': 0.9}\n",
      "{'loss': 0.7345, 'grad_norm': 1.8906368017196655, 'learning_rate': 0.00018371664782625287, 'epoch': 0.92}\n",
      "{'loss': 0.1558, 'grad_norm': 0.911230206489563, 'learning_rate': 0.00018280088311480201, 'epoch': 0.94}\n",
      "{'loss': 0.3094, 'grad_norm': 1.4101735353469849, 'learning_rate': 0.00018186248146866927, 'epoch': 0.96}\n",
      "{'loss': 0.4225, 'grad_norm': 1.3890918493270874, 'learning_rate': 0.00018090169943749476, 'epoch': 0.98}\n",
      "{'loss': 0.4117, 'grad_norm': 1.4725831747055054, 'learning_rate': 0.0001799187996894925, 'epoch': 1.0}\n",
      " 25%|██████████                              | 50/200 [34:29<1:38:57, 39.58s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.17s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:07<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.08s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.16s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:19,  2.22s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.26s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.30s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.30s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.34s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.33s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.061764683574438095, 'eval_runtime': 37.5711, 'eval_samples_per_second': 0.852, 'eval_steps_per_second': 0.426, 'epoch': 1.0}\n",
      " 25%|██████████                              | 50/200 [35:06<1:38:57, 39.58s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'loss': 0.2407, 'grad_norm': 0.8724678158760071, 'learning_rate': 0.00017891405093963938, 'epoch': 1.02}\n",
      "{'loss': 0.1218, 'grad_norm': 0.9914234280586243, 'learning_rate': 0.00017788772787621126, 'epoch': 1.04}\n",
      "{'loss': 0.1918, 'grad_norm': 2.2392046451568604, 'learning_rate': 0.00017684011108568592, 'epoch': 1.06}\n",
      "{'loss': 0.1738, 'grad_norm': 0.6507205367088318, 'learning_rate': 0.0001757714869760335, 'epoch': 1.08}\n",
      "{'loss': 0.1618, 'grad_norm': 1.1327239274978638, 'learning_rate': 0.0001746821476984154, 'epoch': 1.1}\n",
      "{'loss': 0.0881, 'grad_norm': 0.8122654557228088, 'learning_rate': 0.00017357239106731317, 'epoch': 1.12}\n",
      "{'loss': 0.2386, 'grad_norm': 1.6741632223129272, 'learning_rate': 0.00017244252047910892, 'epoch': 1.14}\n",
      "{'loss': 0.1374, 'grad_norm': 0.7927038073539734, 'learning_rate': 0.00017129284482913972, 'epoch': 1.16}\n",
      "{'loss': 0.0549, 'grad_norm': 0.5206702351570129, 'learning_rate': 0.00017012367842724887, 'epoch': 1.18}\n",
      "{'loss': 0.2225, 'grad_norm': 1.210288643836975, 'learning_rate': 0.0001689353409118566, 'epoch': 1.2}\n",
      "{'loss': 0.2157, 'grad_norm': 1.0633223056793213, 'learning_rate': 0.00016772815716257412, 'epoch': 1.22}\n",
      "{'loss': 0.1193, 'grad_norm': 5.439505577087402, 'learning_rate': 0.0001665024572113848, 'epoch': 1.24}\n",
      "{'loss': 0.2251, 'grad_norm': 0.8083823919296265, 'learning_rate': 0.00016525857615241687, 'epoch': 1.26}\n",
      "{'loss': 0.1528, 'grad_norm': 1.1716482639312744, 'learning_rate': 0.00016399685405033167, 'epoch': 1.28}\n",
      "{'loss': 0.113, 'grad_norm': 0.7949053645133972, 'learning_rate': 0.0001627176358473537, 'epoch': 1.3}\n",
      "{'loss': 0.1829, 'grad_norm': 1.0936264991760254, 'learning_rate': 0.0001614212712689668, 'epoch': 1.32}\n",
      "{'loss': 0.1621, 'grad_norm': 1.050760269165039, 'learning_rate': 0.00016010811472830252, 'epoch': 1.34}\n",
      "{'loss': 0.1689, 'grad_norm': 0.6672958135604858, 'learning_rate': 0.00015877852522924732, 'epoch': 1.36}\n",
      "{'loss': 0.1153, 'grad_norm': 0.8482832312583923, 'learning_rate': 0.00015743286626829437, 'epoch': 1.38}\n",
      "{'loss': 0.1169, 'grad_norm': 0.8793713450431824, 'learning_rate': 0.0001560715057351673, 'epoch': 1.4}\n",
      "{'loss': 0.1815, 'grad_norm': 1.0877233743667603, 'learning_rate': 0.00015469481581224272, 'epoch': 1.42}\n",
      "{'loss': 0.2159, 'grad_norm': 9.65846061706543, 'learning_rate': 0.0001533031728727994, 'epoch': 1.44}\n",
      "{'loss': 0.1284, 'grad_norm': 0.9298238158226013, 'learning_rate': 0.00015189695737812152, 'epoch': 1.46}\n",
      "{'loss': 0.1848, 'grad_norm': 1.0203856229782104, 'learning_rate': 0.0001504765537734844, 'epoch': 1.48}\n",
      "{'loss': 0.1991, 'grad_norm': 1.1052898168563843, 'learning_rate': 0.00014904235038305083, 'epoch': 1.5}\n",
      " 38%|███████████████                         | 75/200 [52:16<1:22:29, 39.60s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.17s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:07<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.08s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.16s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:19,  2.22s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.25s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.30s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.30s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.34s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.33s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.07154253870248795, 'eval_runtime': 37.6006, 'eval_samples_per_second': 0.851, 'eval_steps_per_second': 0.426, 'epoch': 1.5}\n",
      " 38%|███████████████                         | 75/200 [52:54<1:22:29, 39.60s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'loss': 0.2892, 'grad_norm': 1.5431745052337646, 'learning_rate': 0.00014759473930370736, 'epoch': 1.52}\n",
      "{'loss': 0.2081, 'grad_norm': 1.0070595741271973, 'learning_rate': 0.0001461341162978688, 'epoch': 1.54}\n",
      "{'loss': 0.1713, 'grad_norm': 1.2642396688461304, 'learning_rate': 0.00014466088068528068, 'epoch': 1.56}\n",
      "{'loss': 0.1758, 'grad_norm': 1.0344091653823853, 'learning_rate': 0.00014317543523384928, 'epoch': 1.58}\n",
      "{'loss': 0.0765, 'grad_norm': 0.5410005450248718, 'learning_rate': 0.00014167818604952906, 'epoch': 1.6}\n",
      "{'loss': 0.1613, 'grad_norm': 1.0725617408752441, 'learning_rate': 0.00014016954246529696, 'epoch': 1.62}\n",
      "{'loss': 0.0966, 'grad_norm': 0.7324310541152954, 'learning_rate': 0.00013864991692924523, 'epoch': 1.64}\n",
      "{'loss': 0.0931, 'grad_norm': 0.9923219680786133, 'learning_rate': 0.00013711972489182208, 'epoch': 1.66}\n",
      "{'loss': 0.2696, 'grad_norm': 0.7926069498062134, 'learning_rate': 0.00013557938469225167, 'epoch': 1.68}\n",
      "{'loss': 0.2638, 'grad_norm': 1.1011937856674194, 'learning_rate': 0.00013402931744416433, 'epoch': 1.7}\n",
      "{'loss': 0.1397, 'grad_norm': 0.8772648572921753, 'learning_rate': 0.00013246994692046836, 'epoch': 1.72}\n",
      "{'loss': 0.0722, 'grad_norm': 0.5400723814964294, 'learning_rate': 0.00013090169943749476, 'epoch': 1.74}\n",
      "{'loss': 0.2842, 'grad_norm': 2.8433122634887695, 'learning_rate': 0.0001293250037384465, 'epoch': 1.76}\n",
      "{'loss': 0.1888, 'grad_norm': 1.041156530380249, 'learning_rate': 0.00012774029087618446, 'epoch': 1.78}\n",
      "{'loss': 0.1374, 'grad_norm': 3.684229612350464, 'learning_rate': 0.00012614799409538198, 'epoch': 1.8}\n",
      "{'loss': 0.1528, 'grad_norm': 0.8940197825431824, 'learning_rate': 0.00012454854871407994, 'epoch': 1.82}\n",
      "{'loss': 0.1908, 'grad_norm': 0.9586880803108215, 'learning_rate': 0.00012294239200467516, 'epoch': 1.84}\n",
      "{'loss': 0.1321, 'grad_norm': 0.8699865341186523, 'learning_rate': 0.0001213299630743747, 'epoch': 1.86}\n",
      "{'loss': 0.0481, 'grad_norm': 0.792743444442749, 'learning_rate': 0.00011971170274514802, 'epoch': 1.88}\n",
      "{'loss': 0.1869, 'grad_norm': 1.043825626373291, 'learning_rate': 0.000118088053433211, 'epoch': 1.9}\n",
      "{'loss': 0.1944, 'grad_norm': 1.715797781944275, 'learning_rate': 0.00011645945902807341, 'epoch': 1.92}\n",
      "{'loss': 0.1393, 'grad_norm': 0.8290280699729919, 'learning_rate': 0.0001148263647711842, 'epoch': 1.94}\n",
      "{'loss': 0.0803, 'grad_norm': 1.1963515281677246, 'learning_rate': 0.00011318921713420691, 'epoch': 1.96}\n",
      "{'loss': 0.1261, 'grad_norm': 0.8865647912025452, 'learning_rate': 0.00011154846369695863, 'epoch': 1.98}\n",
      "{'loss': 0.0837, 'grad_norm': 0.6956930160522461, 'learning_rate': 0.0001099045530250463, 'epoch': 2.0}\n",
      " 50%|██████████████████▌                  | 100/200 [1:09:23<1:05:55, 39.56s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.17s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:07<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.08s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.16s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:19,  2.22s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.25s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.30s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.30s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.34s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.33s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.049564018845558167, 'eval_runtime': 37.564, 'eval_samples_per_second': 0.852, 'eval_steps_per_second': 0.426, 'epoch': 2.0}\n",
      " 50%|██████████████████▌                  | 100/200 [1:10:01<1:05:55, 39.56s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'loss': 0.08, 'grad_norm': 0.6722405552864075, 'learning_rate': 0.00010825793454723325, 'epoch': 2.02}\n",
      "{'loss': 0.0629, 'grad_norm': 0.41515377163887024, 'learning_rate': 0.00010660905843256994, 'epoch': 2.04}\n",
      "{'loss': 0.0948, 'grad_norm': 0.5676549077033997, 'learning_rate': 0.00010495837546732224, 'epoch': 2.06}\n",
      "{'loss': 0.0892, 'grad_norm': 0.5657507181167603, 'learning_rate': 0.00010330633693173082, 'epoch': 2.08}\n",
      "{'loss': 0.0637, 'grad_norm': 0.4872317910194397, 'learning_rate': 0.00010165339447663587, 'epoch': 2.1}\n",
      "{'loss': 0.132, 'grad_norm': 1.087523341178894, 'learning_rate': 0.0001, 'epoch': 2.12}\n",
      "{'loss': 0.1401, 'grad_norm': 0.8876938819885254, 'learning_rate': 9.834660552336415e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0659, 'grad_norm': 0.6015400290489197, 'learning_rate': 9.669366306826919e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0894, 'grad_norm': 0.4330505430698395, 'learning_rate': 9.504162453267777e-05, 'epoch': 2.18}\n",
      "{'loss': 0.144, 'grad_norm': 0.74090576171875, 'learning_rate': 9.339094156743007e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0823, 'grad_norm': 0.5381562113761902, 'learning_rate': 9.174206545276677e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0277, 'grad_norm': 0.3997431695461273, 'learning_rate': 9.009544697495374e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0887, 'grad_norm': 0.7808639407157898, 'learning_rate': 8.845153630304139e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0985, 'grad_norm': 0.6711230278015137, 'learning_rate': 8.681078286579311e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0786, 'grad_norm': 5.473174571990967, 'learning_rate': 8.517363522881579e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0279, 'grad_norm': 0.48493874073028564, 'learning_rate': 8.35405409719266e-05, 'epoch': 2.32}\n",
      "{'loss': 0.1274, 'grad_norm': 1.0865697860717773, 'learning_rate': 8.191194656678904e-05, 'epoch': 2.34}\n",
      "{'loss': 0.1453, 'grad_norm': 0.8146023154258728, 'learning_rate': 8.028829725485199e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0462, 'grad_norm': 0.8282454013824463, 'learning_rate': 7.867003692562534e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0386, 'grad_norm': 0.4658718407154083, 'learning_rate': 7.705760799532485e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0477, 'grad_norm': 0.5444771647453308, 'learning_rate': 7.54514512859201e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0512, 'grad_norm': 0.4544529914855957, 'learning_rate': 7.385200590461803e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0326, 'grad_norm': 0.6160447001457214, 'learning_rate': 7.225970912381556e-05, 'epoch': 2.46}\n",
      "{'loss': 0.0658, 'grad_norm': 1.2059787511825562, 'learning_rate': 7.067499626155354e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0907, 'grad_norm': 1.2881804704666138, 'learning_rate': 6.909830056250527e-05, 'epoch': 2.5}\n",
      " 62%|████████████████████████▍              | 125/200 [1:27:16<49:31, 39.61s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.17s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:06<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.08s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.16s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:19,  2.22s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.25s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.30s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.30s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.34s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.33s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06361024081707001, 'eval_runtime': 37.5544, 'eval_samples_per_second': 0.852, 'eval_steps_per_second': 0.426, 'epoch': 2.5}\n",
      " 62%|████████████████████████▍              | 125/200 [1:27:53<49:31, 39.61s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'loss': 0.0654, 'grad_norm': 0.6004621982574463, 'learning_rate': 6.753005307953167e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0673, 'grad_norm': 0.6229197978973389, 'learning_rate': 6.59706825558357e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0586, 'grad_norm': 0.5444331765174866, 'learning_rate': 6.442061530774834e-05, 'epoch': 2.56}\n",
      "{'loss': 0.081, 'grad_norm': 0.9123799800872803, 'learning_rate': 6.28802751081779e-05, 'epoch': 2.58}\n",
      "{'loss': 0.1428, 'grad_norm': 0.8908362984657288, 'learning_rate': 6.135008307075481e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0706, 'grad_norm': 1.1660776138305664, 'learning_rate': 5.983045753470308e-05, 'epoch': 2.62}\n",
      "{'loss': 0.0401, 'grad_norm': 0.5998367667198181, 'learning_rate': 5.832181395047098e-05, 'epoch': 2.64}\n",
      "{'loss': 0.1137, 'grad_norm': 0.6807586550712585, 'learning_rate': 5.6824564766150726e-05, 'epoch': 2.66}\n",
      "{'loss': 0.0134, 'grad_norm': 0.30294689536094666, 'learning_rate': 5.533911931471936e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0762, 'grad_norm': 0.7860315442085266, 'learning_rate': 5.386588370213124e-05, 'epoch': 2.7}\n",
      "{'loss': 0.0579, 'grad_norm': 0.5871977210044861, 'learning_rate': 5.240526069629265e-05, 'epoch': 2.72}\n",
      "{'loss': 0.107, 'grad_norm': 3.171689510345459, 'learning_rate': 5.095764961694922e-05, 'epoch': 2.74}\n",
      "{'loss': 0.1292, 'grad_norm': 1.5104105472564697, 'learning_rate': 4.952344622651566e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0489, 'grad_norm': 0.5808483362197876, 'learning_rate': 4.810304262187852e-05, 'epoch': 2.78}\n",
      "{'loss': 0.0368, 'grad_norm': 0.4111695885658264, 'learning_rate': 4.669682712720065e-05, 'epoch': 2.8}\n",
      "{'loss': 0.1843, 'grad_norm': 1.061418056488037, 'learning_rate': 4.530518418775733e-05, 'epoch': 2.82}\n",
      "{'loss': 0.0719, 'grad_norm': 0.75709468126297, 'learning_rate': 4.392849426483274e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1048, 'grad_norm': 0.6394405364990234, 'learning_rate': 4.256713373170564e-05, 'epoch': 2.86}\n",
      "{'loss': 0.0365, 'grad_norm': 1.1685068607330322, 'learning_rate': 4.12214747707527e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0697, 'grad_norm': 0.8933620452880859, 'learning_rate': 3.9891885271697496e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0532, 'grad_norm': 0.46049487590789795, 'learning_rate': 3.857872873103322e-05, 'epoch': 2.92}\n",
      "{'loss': 0.0522, 'grad_norm': 0.5912122130393982, 'learning_rate': 3.7282364152646297e-05, 'epoch': 2.94}\n",
      "{'loss': 0.158, 'grad_norm': 1.4349714517593384, 'learning_rate': 3.600314594966834e-05, 'epoch': 2.96}\n",
      "{'loss': 0.0635, 'grad_norm': 0.4834209382534027, 'learning_rate': 3.4741423847583134e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0454, 'grad_norm': 0.9659250974655151, 'learning_rate': 3.349754278861517e-05, 'epoch': 3.0}\n",
      " 75%|█████████████████████████████▎         | 150/200 [1:44:23<32:57, 39.56s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.17s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:06<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:23,  2.11s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.18s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:20,  2.23s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.26s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.31s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.31s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.34s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.33s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0528721921145916, 'eval_runtime': 37.6381, 'eval_samples_per_second': 0.85, 'eval_steps_per_second': 0.425, 'epoch': 3.0}\n",
      " 75%|█████████████████████████████▎         | 150/200 [1:45:00<32:57, 39.56s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'loss': 0.0652, 'grad_norm': 0.41725072264671326, 'learning_rate': 3.227184283742591e-05, 'epoch': 3.02}\n",
      "{'loss': 0.114, 'grad_norm': 0.7488211393356323, 'learning_rate': 3.106465908814342e-05, 'epoch': 3.04}\n",
      "{'loss': 0.0262, 'grad_norm': 0.5446503758430481, 'learning_rate': 2.9876321572751144e-05, 'epoch': 3.06}\n",
      "{'loss': 0.0292, 'grad_norm': 0.34806227684020996, 'learning_rate': 2.87071551708603e-05, 'epoch': 3.08}\n",
      "{'loss': 0.0401, 'grad_norm': 0.4552004933357239, 'learning_rate': 2.7557479520891104e-05, 'epoch': 3.1}\n",
      "{'loss': 0.0588, 'grad_norm': 0.4015624523162842, 'learning_rate': 2.6427608932686843e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0153, 'grad_norm': 0.32840070128440857, 'learning_rate': 2.5317852301584643e-05, 'epoch': 3.14}\n",
      "{'loss': 0.0624, 'grad_norm': 0.3847925662994385, 'learning_rate': 2.422851302396655e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0489, 'grad_norm': 0.4978523254394531, 'learning_rate': 2.315988891431412e-05, 'epoch': 3.18}\n",
      "{'loss': 0.015, 'grad_norm': 0.2767183482646942, 'learning_rate': 2.2112272123788768e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0233, 'grad_norm': 0.25056299567222595, 'learning_rate': 2.1085949060360654e-05, 'epoch': 3.22}\n",
      "{'loss': 0.0169, 'grad_norm': 0.25332269072532654, 'learning_rate': 2.008120031050753e-05, 'epoch': 3.24}\n",
      "{'loss': 0.051, 'grad_norm': 0.5922134518623352, 'learning_rate': 1.9098300562505266e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0301, 'grad_norm': 0.4236699044704437, 'learning_rate': 1.8137518531330767e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0126, 'grad_norm': 0.2498588114976883, 'learning_rate': 1.7199116885197995e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0466, 'grad_norm': 0.33553841710090637, 'learning_rate': 1.6283352173747145e-05, 'epoch': 3.32}\n",
      "{'loss': 0.0105, 'grad_norm': 0.5390718579292297, 'learning_rate': 1.5390474757906446e-05, 'epoch': 3.34}\n",
      "{'loss': 0.0213, 'grad_norm': 0.26365432143211365, 'learning_rate': 1.4520728741446089e-05, 'epoch': 3.36}\n",
      "{'loss': 0.0431, 'grad_norm': 0.7640235424041748, 'learning_rate': 1.3674351904242611e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0254, 'grad_norm': 0.36155712604522705, 'learning_rate': 1.2851575637272262e-05, 'epoch': 3.4}\n",
      "{'loss': 0.0184, 'grad_norm': 0.8607904314994812, 'learning_rate': 1.2052624879351104e-05, 'epoch': 3.42}\n",
      "{'loss': 0.0633, 'grad_norm': 0.48463407158851624, 'learning_rate': 1.1277718055638819e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0304, 'grad_norm': 0.5303412079811096, 'learning_rate': 1.0527067017923654e-05, 'epoch': 3.46}\n",
      "{'loss': 0.0157, 'grad_norm': 10.98591136932373, 'learning_rate': 9.80087698670411e-06, 'epoch': 3.48}\n",
      "{'loss': 0.0148, 'grad_norm': 0.24193361401557922, 'learning_rate': 9.09934649508375e-06, 'epoch': 3.5}\n",
      " 88%|██████████████████████████████████▏    | 175/200 [2:02:10<16:30, 39.60s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.16s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:06<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.08s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.16s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:19,  2.22s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.25s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.34s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.33s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.33s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.33s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.35s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.34s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.34s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.05171352997422218, 'eval_runtime': 37.7046, 'eval_samples_per_second': 0.849, 'eval_steps_per_second': 0.424, 'epoch': 3.5}\n",
      " 88%|██████████████████████████████████▏    | 175/200 [2:02:48<16:30, 39.60s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'loss': 0.0829, 'grad_norm': 0.47102436423301697, 'learning_rate': 8.422667334494249e-06, 'epoch': 3.52}\n",
      "{'loss': 0.0167, 'grad_norm': 0.3562886416912079, 'learning_rate': 7.771024502261526e-06, 'epoch': 3.54}\n",
      "{'loss': 0.0641, 'grad_norm': 0.3084860146045685, 'learning_rate': 7.144596151029303e-06, 'epoch': 3.56}\n",
      "{'loss': 0.0411, 'grad_norm': 0.321661114692688, 'learning_rate': 6.543553540053926e-06, 'epoch': 3.58}\n",
      "{'loss': 0.0391, 'grad_norm': 0.46967020630836487, 'learning_rate': 5.968060988383883e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0293, 'grad_norm': 0.3952450752258301, 'learning_rate': 5.418275829936537e-06, 'epoch': 3.62}\n",
      "{'loss': 0.0085, 'grad_norm': 0.17491351068019867, 'learning_rate': 4.8943483704846475e-06, 'epoch': 3.64}\n",
      "{'loss': 0.0383, 'grad_norm': 0.3795441687107086, 'learning_rate': 4.3964218465642355e-06, 'epoch': 3.66}\n",
      "{'loss': 0.0122, 'grad_norm': 0.19660091400146484, 'learning_rate': 3.924632386315186e-06, 'epoch': 3.68}\n",
      "{'loss': 0.0353, 'grad_norm': 0.2742617130279541, 'learning_rate': 3.4791089722651436e-06, 'epoch': 3.7}\n",
      "{'loss': 0.0989, 'grad_norm': 0.7160890698432922, 'learning_rate': 3.059973406066963e-06, 'epoch': 3.72}\n",
      "{'loss': 0.0123, 'grad_norm': 0.22935479879379272, 'learning_rate': 2.667340275199426e-06, 'epoch': 3.74}\n",
      "{'loss': 0.0121, 'grad_norm': 0.29286471009254456, 'learning_rate': 2.3013169216400733e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0721, 'grad_norm': 1.6463794708251953, 'learning_rate': 1.9620034125190644e-06, 'epoch': 3.78}\n",
      "{'loss': 0.0705, 'grad_norm': 0.46374133229255676, 'learning_rate': 1.6494925127617634e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0112, 'grad_norm': 0.24157531559467316, 'learning_rate': 1.3638696597277679e-06, 'epoch': 3.82}\n",
      "{'loss': 0.0289, 'grad_norm': 0.4981115758419037, 'learning_rate': 1.1052129398531507e-06, 'epoch': 3.84}\n",
      "{'loss': 0.0226, 'grad_norm': 0.5541749596595764, 'learning_rate': 8.735930673024806e-07, 'epoch': 3.86}\n",
      "{'loss': 0.0286, 'grad_norm': 0.5310027003288269, 'learning_rate': 6.690733646361857e-07, 'epoch': 3.88}\n",
      "{'loss': 0.009, 'grad_norm': 0.19896362721920013, 'learning_rate': 4.917097454988584e-07, 'epoch': 3.9}\n",
      "{'loss': 0.0288, 'grad_norm': 0.281507670879364, 'learning_rate': 3.415506993330153e-07, 'epoch': 3.92}\n",
      "{'loss': 0.028, 'grad_norm': 0.5467099547386169, 'learning_rate': 2.1863727812254653e-07, 'epoch': 3.94}\n",
      "{'loss': 0.0095, 'grad_norm': 0.24794040620326996, 'learning_rate': 1.230030851695263e-07, 'epoch': 3.96}\n",
      "{'loss': 0.0168, 'grad_norm': 0.36240115761756897, 'learning_rate': 5.467426590739511e-08, 'epoch': 3.98}\n",
      "{'loss': 0.0078, 'grad_norm': 0.2361232191324234, 'learning_rate': 1.3669500753099585e-08, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████████| 200/200 [2:19:13<00:00, 38.30s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:16,  1.17s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:21,  1.65s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:07<00:22,  1.91s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:09<00:22,  2.08s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:21,  2.16s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:14<00:19,  2.22s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.26s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:18<00:16,  2.31s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:21<00:13,  2.30s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:23<00:11,  2.31s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:25<00:09,  2.32s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:28<00:07,  2.34s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:30<00:04,  2.33s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:32<00:02,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.05235676467418671, 'eval_runtime': 37.5714, 'eval_samples_per_second': 0.852, 'eval_steps_per_second': 0.426, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████████| 200/200 [2:19:50<00:00, 38.30s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.33s/it]\u001b[A\n",
      "{'train_runtime': 8392.6125, 'train_samples_per_second': 0.191, 'train_steps_per_second': 0.024, 'train_loss': 0.3483530584769323, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████████| 200/200 [2:19:52<00:00, 41.96s/it]\n",
      "[2025-10-06 14:19:17,146] [INFO] [axolotl.train.save_trained_model:244] [PID:2907303] [RANK:0] Training completed! Saving trained model to ./out-gemma-3-12b-it.\u001b[39m\n",
      "[2025-10-06 14:19:17,705] [INFO] [axolotl.train.save_trained_model:341] [PID:2907303] [RANK:0] Model successfully saved to ./out-gemma-3-12b-it\u001b[39m\n",
      "\u001b[0mCPU times: user 42.7 s, sys: 4.83 s, total: 47.6 s\n",
      "Wall time: 2h 28min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the LoRA/DoRA into the base model (for inference & quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-06 14:19:45,490] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:2922235] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-06 14:19:45,490] [WARNING] [axolotl.utils.schemas.config.check_sample_packing_wo_flash:482] [PID:2922235] [RANK:0] sample_packing without flash, sdp, xformers or flex attention does not handle cross sample decontamination.\u001b[39m\n",
      "\u001b[33m[2025-10-06 14:19:45,491] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:2922235] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-06 14:19:45,696] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:2922235] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-06 14:19:45,708] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:318] [PID:2922235] [RANK:0] loading tokenizer... google/gemma-3-12b-it\u001b[39m\n",
      "[2025-10-06 14:19:47,180] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:321] [PID:2922235] [RANK:0] loading model...\u001b[39m\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:31<00:00,  6.30s/it]\n",
      "[2025-10-06 14:20:21,847] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:2922235] [RANK:0] cuda memory usage after model load: 22.701GB (+1.877GB cache, +1.223GB misc)\u001b[39m\n",
      "[2025-10-06 14:20:21,880] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:2922235] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-06 14:20:21,892] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:2922235] [RANK:0] found linear modules: ['down_proj', 'fc1', 'fc2', 'gate_proj', 'k_proj', 'o_proj', 'out_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-10-06 14:20:21,892] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:2922235] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 76,673,904 || all params: 12,263,998,944 || trainable%: 0.6252\n",
      "[2025-10-06 14:20:24,230] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:2922235] [RANK:0] cuda memory usage after adapters: 22.994GB (+5.906GB cache, +1.317GB misc)\u001b[39m\n",
      "[2025-10-06 14:20:24,700] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:327] [PID:2922235] [RANK:0] loading processor...\u001b[39m\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[2025-10-06 14:20:29,337] [INFO] [axolotl.cli.merge_lora.do_merge_lora:31] [PID:2922235] [RANK:0] Running merge of LoRA with base model...\u001b[39m\n",
      "Unloading and merging model: 100%|████████| 1658/1658 [00:00<00:00, 3839.86it/s]\n",
      "[2025-10-06 14:20:29,851] [INFO] [axolotl.cli.merge_lora.do_merge_lora:44] [PID:2922235] [RANK:0] Saving merged model to: out-gemma-3-12b-it/merged...\u001b[39m\n",
      "\u001b[0mCPU times: user 737 ms, sys: 115 ms, total: 851 ms\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/axolotl merge-lora {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 14:22:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-06 14:22:17 [utils.py:328] non-default args: {'max_model_len': 8192, 'disable_log_stats': True, 'model': 'out-gemma-3-12b-it/merged'}\n",
      "INFO 10-06 14:22:31 [__init__.py:742] Resolved architecture: Gemma3ForConditionalGeneration\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO 10-06 14:22:31 [__init__.py:1815] Using max model len 8192\n",
      "INFO 10-06 14:22:33 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:38 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:38 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='out-gemma-3-12b-it/merged', speculative_config=None, tokenizer='out-gemma-3-12b-it/merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=out-gemma-3-12b-it/merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[W1006 14:22:42.316323543 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:42 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m WARNING 10-06 14:22:42 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:47 [gpu_model_runner.py:2338] Starting to load model out-gemma-3-12b-it/merged...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:47 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:47 [cuda.py:379] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:48 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.12s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.11s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.14s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:04<00:01,  1.15s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:05<00:00,  1.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:05<00:00,  1.20s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:54 [default_loader.py:268] Loading weights took 6.05 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:54 [gpu_model_runner.py:2392] Model loading took 23.3141 GiB and 6.666985 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:22:54 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 31 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:23:12 [backends.py:539] Using cache directory: /home/oisuomin/.cache/vllm/torch_compile_cache/2a19c131b8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:23:12 [backends.py:550] Dynamo bytecode transform time: 15.00 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:23:17 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:23:58 [backends.py:215] Compiling a graph for dynamic shape takes 45.30 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:24:05 [monitor.py:34] torch.compile takes 60.30 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:24:06 [gpu_worker.py:298] Available KV cache memory: 44.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:24:06 [kv_cache_utils.py:1028] GPU KV cache size: 121,936 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:24:06 [kv_cache_utils.py:1032] Maximum concurrency for 8,192 tokens per request: 14.86x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:04<00\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:24:12 [gpu_model_runner.py:3118] Graph capturing finished in 6 secs, took 3.01 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:24:12 [gpu_worker.py:391] Free memory on device (78.7/79.18 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 23.31 GiB for weight, 3.27 GiB for peak activation, 0.02 GiB for non-torch memory, and 3.01 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=44564384256` to fit into requested memory, or `--kv-cache-memory=52545194496` to fully utilize gpu memory. Current kv cache memory in use is 47949187584 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2922429)\u001b[0;0m INFO 10-06 14:24:12 [core.py:218] init engine (profile, create kv cache, warmup model) took 77.71 seconds\n",
      "INFO 10-06 14:24:14 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-06 14:24:14 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Adding requests: 100%|███████████████████████| 182/182 [00:00<00:00, 454.55it/s]\n",
      "Processed prompts: 100%|█| 182/182 [01:06<00:00,  2.72it/s, est. speed input: 67\n",
      "ERROR 10-06 14:25:23 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n",
      "| language   | field     |   mean |   size |\n",
      "|------------|-----------|--------|--------|\n",
      "| en         | alt_title | 0.8033 |     61 |\n",
      "| en         | creator   | 0.8615 |     61 |\n",
      "| en         | doi       | 0.9836 |     61 |\n",
      "| en         | e-isbn    | 0.8634 |     61 |\n",
      "| en         | e-issn    | 0.9508 |     61 |\n",
      "| en         | language  | 1.0000 |     61 |\n",
      "| en         | p-isbn    | 0.8525 |     61 |\n",
      "| en         | p-issn    | 0.9180 |     61 |\n",
      "| en         | publisher | 0.7213 |     61 |\n",
      "| en         | title     | 0.9016 |     61 |\n",
      "| en         | type_coar | 0.8525 |     61 |\n",
      "| en         | year      | 0.9016 |     61 |\n",
      "| fi         | alt_title | 0.8630 |     73 |\n",
      "| fi         | creator   | 0.9566 |     73 |\n",
      "| fi         | doi       | 1.0000 |     73 |\n",
      "| fi         | e-isbn    | 1.0000 |     73 |\n",
      "| fi         | e-issn    | 0.9041 |     73 |\n",
      "| fi         | language  | 0.9863 |     73 |\n",
      "| fi         | p-isbn    | 0.9863 |     73 |\n",
      "| fi         | p-issn    | 0.9589 |     73 |\n",
      "| fi         | publisher | 0.9041 |     73 |\n",
      "| fi         | title     | 0.9041 |     73 |\n",
      "| fi         | type_coar | 0.8767 |     73 |\n",
      "| fi         | year      | 0.8630 |     73 |\n",
      "| se         | alt_title | 1.0000 |      3 |\n",
      "| se         | creator   | 0.6667 |      3 |\n",
      "| se         | doi       | 1.0000 |      3 |\n",
      "| se         | e-isbn    | 1.0000 |      3 |\n",
      "| se         | e-issn    | 1.0000 |      3 |\n",
      "| se         | language  | 0.6667 |      3 |\n",
      "| se         | p-isbn    | 1.0000 |      3 |\n",
      "| se         | p-issn    | 1.0000 |      3 |\n",
      "| se         | publisher | 0.6667 |      3 |\n",
      "| se         | title     | 0.3333 |      3 |\n",
      "| se         | type_coar | 0.3333 |      3 |\n",
      "| se         | year      | 0.6667 |      3 |\n",
      "| sv         | alt_title | 0.8444 |     45 |\n",
      "| sv         | creator   | 0.9556 |     45 |\n",
      "| sv         | doi       | 1.0000 |     45 |\n",
      "| sv         | e-isbn    | 0.9259 |     45 |\n",
      "| sv         | e-issn    | 0.9778 |     45 |\n",
      "| sv         | language  | 1.0000 |     45 |\n",
      "| sv         | p-isbn    | 0.8889 |     45 |\n",
      "| sv         | p-issn    | 0.9778 |     45 |\n",
      "| sv         | publisher | 0.8815 |     45 |\n",
      "| sv         | title     | 0.9556 |     45 |\n",
      "| sv         | type_coar | 0.9333 |     45 |\n",
      "| sv         | year      | 0.9111 |     45 |\n",
      "CPU times: user 1.29 s, sys: 167 ms, total: 1.46 s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate using the evaluate-model script, which needs venv with vLLM installed\n",
    "!../dspy/venv/bin/python evaluate-model.py out-{MODEL_SHORT_NAME}/merged axolotl-test.jsonl results-{MODEL_SHORT_NAME}.md\n",
    "!cat results-{MODEL_SHORT_NAME}.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greylitlm-axolotl",
   "language": "python",
   "name": "greylitlm-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
