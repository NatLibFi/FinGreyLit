{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Gemma-3-4B-it model using Axolotl framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/appl/easybuild/opt/CUDA/12.6.0\n"
     ]
    }
   ],
   "source": [
    "!printenv CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit\"\n",
    "#SLICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 640 train records\n",
      "Wrote 182 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'assistant', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"axolotl-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"axolotl-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"axolotl-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: axolotl-train.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "test_datasets:\n",
    "  - path: axolotl-eval.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "chat_template: gemma3\n",
    "eot_tokens:\n",
    "  - <end_of_turn>\n",
    "\n",
    "peft_use_dora: true\n",
    "adapter: lora\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "#lora_target_linear: true\n",
    "lora_target_modules: 'model.language_model.layers.[\\\\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 5\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-10-07 12:12:13,579] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3090169] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-07 12:12:13,579] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3090169] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-07 12:12:17,425] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3090169] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-07 12:12:30,467] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3090169] [RANK:0] Unable to find prepared dataset in last_run_prepared/cb6ba859f59fc79bea35912b9debcad8\u001b[39m\n",
      "[2025-10-07 12:12:30,467] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3090169] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-07 12:12:30,467] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3090169] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 640 examples [00:00, 14351.84 examples/s]\n",
      "[2025-10-07 12:12:31,160] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3090169] [RANK:0] Loading dataset: axolotl-train.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-07 12:12:31,229] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3090169] [RANK:0] Using chat template:\n",
      "---\n",
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|█| 640/640 [00:14<00:00, 43.06 examples/s\n",
      "[2025-10-07 12:12:46,897] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3090169] [RANK:0] min_input_len: 388\u001b[39m\n",
      "[2025-10-07 12:12:46,898] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3090169] [RANK:0] max_input_len: 8994\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 640/640 [00:00<00:00, 1771.32 exa\n",
      "\u001b[33m[2025-10-07 12:12:48,596] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:201] [PID:3090169] [RANK:0] Dropped 5 long samples from dataset\u001b[39m\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 635/635 [00:03<00\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 635/635 [00:04<00\n",
      "Saving the dataset (1/1 shards): 100%|█| 635/635 [00:00<00:00, 11013.77 examples\n",
      "[2025-10-07 12:12:59,160] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3090169] [RANK:0] Unable to find prepared dataset in last_run_prepared/adda697e020e77476dddfe4a21729386\u001b[39m\n",
      "[2025-10-07 12:12:59,160] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3090169] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-07 12:12:59,160] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3090169] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 7128.62 examples/s]\n",
      "[2025-10-07 12:12:59,609] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3090169] [RANK:0] Loading dataset: axolotl-eval.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-07 12:12:59,676] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3090169] [RANK:0] Using chat template:\n",
      "---\n",
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:11<00:00,  2.78 examples/s]\n",
      "[2025-10-07 12:13:12,031] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3090169] [RANK:0] min_input_len: 526\u001b[39m\n",
      "[2025-10-07 12:13:12,031] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3090169] [RANK:0] max_input_len: 3316\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:00<00:00, 155.29 exampl\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 32/32 [00:02<00:0\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:03<00:0\n",
      "Saving the dataset (1/1 shards): 100%|█| 32/32 [00:00<00:00, 1016.12 examples/s]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-07 12:16:00,888] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3090169] [RANK:0] gather_len_batches: [200]\u001b[39m\n",
      "[2025-10-07 12:16:00,892] [INFO] [axolotl.utils.data.sft._prepare_standard_dataset:123] [PID:3090169] [RANK:0] Maximum number of steps set at 250\u001b[39m\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:36<00:00, 18.38s/it]\n",
      "[2025-10-07 12:16:51,073] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3090169] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-07 12:16:51,298] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:3090169] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 60,614,656 || all params: 4,360,694,128 || trainable%: 1.3900\n",
      "[2025-10-07 12:20:42,605] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3090169] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n",
      "[2025-10-07 12:20:57,421] [WARNING] [accelerate.utils.other.check_os_kernel:441] [PID:3090169] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[2025-10-07 12:21:30,263] [INFO] [axolotl.train.save_initial_configs:403] [PID:3090169] [RANK:0] Pre-saving adapter config to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-07 12:21:30,265] [INFO] [axolotl.train.save_initial_configs:407] [PID:3090169] [RANK:0] Pre-saving tokenizer to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-07 12:21:30,789] [INFO] [axolotl.train.save_initial_configs:410] [PID:3090169] [RANK:0] Pre-saving model config to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-07 12:21:30,797] [INFO] [axolotl.train.save_initial_configs:414] [PID:3090169] [RANK:0] Pre-saving processor to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-07 12:21:33,092] [INFO] [axolotl.train.execute_training:225] [PID:3090169] [RANK:0] Starting trainer...\u001b[39m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-07 12:23:12,581] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3090169] [RANK:0] gather_len_batches: [200]\u001b[39m\n",
      "  0%|                                                   | 0/250 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.28it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:05<00:26,  2.02s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:06<00:19,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:15,  1.43s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:08<00:12,  1.27s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:09<00:10,  1.18s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:12<00:14,  1.82s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:18,  2.65s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:17<00:12,  2.15s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:18<00:09,  1.86s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:23<00:10,  2.71s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:24<00:06,  2.20s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:28<00:05,  2.76s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:29<00:02,  2.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.8112740516662598, 'eval_runtime': 39.5489, 'eval_samples_per_second': 0.809, 'eval_steps_per_second': 0.405, 'epoch': 0}\n",
      "  0%|                                                   | 0/250 [00:39<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  1.84s/it]\u001b[A\n",
      "{'loss': 10.77, 'grad_norm': 28.105850219726562, 'learning_rate': 0.0, 'epoch': 0.02}0m\u001b[0m\n",
      "  0%|▏                                       | 1/250 [01:43<7:09:09, 103.41s/it][2025-10-07 12:25:26,791] [INFO] [axolotl.utils.callbacks.log_gpu_memory_usage:107] [PID:3090169] [RANK:0] cuda memory usage while training: 8.381GB (+31.894GB cache, +1.326GB misc)\u001b[39m\n",
      "{'loss': 8.9025, 'grad_norm': 21.629432678222656, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 8.4932, 'grad_norm': 21.806713104248047, 'learning_rate': 4e-05, 'epoch': 0.06}\n",
      "{'loss': 9.7035, 'grad_norm': 21.400407791137695, 'learning_rate': 6e-05, 'epoch': 0.08}\n",
      "{'loss': 6.3048, 'grad_norm': 12.936450958251953, 'learning_rate': 8e-05, 'epoch': 0.1}\n",
      "{'loss': 5.1782, 'grad_norm': 9.827860832214355, 'learning_rate': 0.0001, 'epoch': 0.12}\n",
      "{'loss': 4.4921, 'grad_norm': 7.335166931152344, 'learning_rate': 0.00012, 'epoch': 0.14}\n",
      "{'loss': 3.3313, 'grad_norm': 4.903113842010498, 'learning_rate': 0.00014, 'epoch': 0.16}\n",
      "{'loss': 2.8918, 'grad_norm': 4.03757905960083, 'learning_rate': 0.00016, 'epoch': 0.18}\n",
      "{'loss': 2.4682, 'grad_norm': 7.104467391967773, 'learning_rate': 0.00018, 'epoch': 0.2}\n",
      "{'loss': 1.6481, 'grad_norm': 3.7803585529327393, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.4417, 'grad_norm': 3.553441286087036, 'learning_rate': 0.00019999143275740072, 'epoch': 0.24}\n",
      "{'loss': 0.9319, 'grad_norm': 4.706770420074463, 'learning_rate': 0.00019996573249755572, 'epoch': 0.26}\n",
      "{'loss': 0.9645, 'grad_norm': 2.456547975540161, 'learning_rate': 0.0001999229036240723, 'epoch': 0.28}\n",
      "{'loss': 0.8778, 'grad_norm': 3.011399269104004, 'learning_rate': 0.0001998629534754574, 'epoch': 0.3}\n",
      "{'loss': 0.8803, 'grad_norm': 1.6383343935012817, 'learning_rate': 0.00019978589232386035, 'epoch': 0.32}\n",
      "{'loss': 0.6296, 'grad_norm': 1.7694448232650757, 'learning_rate': 0.0001996917333733128, 'epoch': 0.34}\n",
      "{'loss': 0.7885, 'grad_norm': 2.0397162437438965, 'learning_rate': 0.0001995804927574662, 'epoch': 0.36}\n",
      "{'loss': 0.5735, 'grad_norm': 1.7985608577728271, 'learning_rate': 0.00019945218953682734, 'epoch': 0.38}\n",
      "{'loss': 0.4879, 'grad_norm': 1.4972115755081177, 'learning_rate': 0.00019930684569549264, 'epoch': 0.4}\n",
      "{'loss': 0.4443, 'grad_norm': 1.7854770421981812, 'learning_rate': 0.00019914448613738106, 'epoch': 0.42}\n",
      "{'loss': 0.4466, 'grad_norm': 1.802934169769287, 'learning_rate': 0.00019896513868196704, 'epoch': 0.44}\n",
      "{'loss': 1.0371, 'grad_norm': 2.08164644241333, 'learning_rate': 0.00019876883405951377, 'epoch': 0.46}\n",
      "{'loss': 0.4936, 'grad_norm': 1.7051163911819458, 'learning_rate': 0.00019855560590580778, 'epoch': 0.48}\n",
      "{'loss': 0.3193, 'grad_norm': 1.3745330572128296, 'learning_rate': 0.0001983254907563955, 'epoch': 0.5}\n",
      " 10%|████                                    | 25/250 [09:49<1:12:09, 19.24s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.25it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.51it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:23,  1.92s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:10<00:32,  2.95s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:11<00:23,  2.34s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:15<00:23,  2.59s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:16<00:18,  2.36s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:19<00:18,  2.57s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:31<00:31,  5.20s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:31<00:19,  3.90s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:35<00:15,  3.92s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:41<00:13,  4.50s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:43<00:07,  3.51s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:48<00:04,  4.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.15061482787132263, 'eval_runtime': 53.4954, 'eval_samples_per_second': 0.598, 'eval_steps_per_second': 0.299, 'epoch': 0.5}\n",
      " 10%|████                                    | 25/250 [10:44<1:12:09, 19.24s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:49<00:00,  3.24s/it]\u001b[A\n",
      "{'loss': 0.5084, 'grad_norm': 1.5361604690551758, 'learning_rate': 0.00019807852804032305, 'epoch': 0.52}\n",
      "{'loss': 0.587, 'grad_norm': 2.7471201419830322, 'learning_rate': 0.00019781476007338058, 'epoch': 0.54}\n",
      "{'loss': 0.8562, 'grad_norm': 2.128957748413086, 'learning_rate': 0.00019753423205085127, 'epoch': 0.56}\n",
      "{'loss': 0.3528, 'grad_norm': 1.5056132078170776, 'learning_rate': 0.00019723699203976766, 'epoch': 0.58}\n",
      "{'loss': 0.4514, 'grad_norm': 1.693068504333496, 'learning_rate': 0.00019692309097067546, 'epoch': 0.6}\n",
      "{'loss': 0.4529, 'grad_norm': 1.239743709564209, 'learning_rate': 0.00019659258262890683, 'epoch': 0.62}\n",
      "{'loss': 0.3741, 'grad_norm': 1.3322618007659912, 'learning_rate': 0.00019624552364536473, 'epoch': 0.64}\n",
      "{'loss': 0.5762, 'grad_norm': 1.3357208967208862, 'learning_rate': 0.0001958819734868193, 'epoch': 0.66}\n",
      "{'loss': 0.6666, 'grad_norm': 2.1601457595825195, 'learning_rate': 0.0001955019944457187, 'epoch': 0.68}\n",
      "{'loss': 0.6165, 'grad_norm': 1.1707782745361328, 'learning_rate': 0.00019510565162951537, 'epoch': 0.7}\n",
      "{'loss': 0.4371, 'grad_norm': 1.5289466381072998, 'learning_rate': 0.0001946930129495106, 'epoch': 0.72}\n",
      "{'loss': 0.517, 'grad_norm': 1.2448753118515015, 'learning_rate': 0.00019426414910921787, 'epoch': 0.74}\n",
      "{'loss': 0.4272, 'grad_norm': 1.3326643705368042, 'learning_rate': 0.00019381913359224842, 'epoch': 0.76}\n",
      "{'loss': 0.6115, 'grad_norm': 1.1236752271652222, 'learning_rate': 0.00019335804264972018, 'epoch': 0.78}\n",
      "{'loss': 0.394, 'grad_norm': 1.0502856969833374, 'learning_rate': 0.00019288095528719243, 'epoch': 0.8}\n",
      "{'loss': 0.298, 'grad_norm': 0.7671082019805908, 'learning_rate': 0.0001923879532511287, 'epoch': 0.82}\n",
      "{'loss': 0.4404, 'grad_norm': 1.5406174659729004, 'learning_rate': 0.00019187912101488984, 'epoch': 0.84}\n",
      "{'loss': 0.7275, 'grad_norm': 1.3290989398956299, 'learning_rate': 0.0001913545457642601, 'epoch': 0.86}\n",
      "{'loss': 0.3279, 'grad_norm': 1.1874077320098877, 'learning_rate': 0.00019081431738250814, 'epoch': 0.88}\n",
      "{'loss': 0.2212, 'grad_norm': 1.086330771446228, 'learning_rate': 0.00019025852843498607, 'epoch': 0.9}\n",
      "{'loss': 0.9435, 'grad_norm': 1.817737102508545, 'learning_rate': 0.00018968727415326884, 'epoch': 0.92}\n",
      "{'loss': 0.2292, 'grad_norm': 0.7417959570884705, 'learning_rate': 0.0001891006524188368, 'epoch': 0.94}\n",
      "{'loss': 0.2745, 'grad_norm': 0.9868471622467041, 'learning_rate': 0.0001884987637463042, 'epoch': 0.96}\n",
      "{'loss': 0.5103, 'grad_norm': 1.1259112358093262, 'learning_rate': 0.00018788171126619653, 'epoch': 0.98}\n",
      "{'loss': 0.6577, 'grad_norm': 1.194282054901123, 'learning_rate': 0.00018724960070727972, 'epoch': 1.0}\n",
      " 20%|████████▍                                 | 50/250 [18:40<54:14, 16.27s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:04<00:09,  1.10it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:09,  1.09it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.07it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.06it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.04it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10167529433965683, 'eval_runtime': 15.6714, 'eval_samples_per_second': 2.042, 'eval_steps_per_second': 1.021, 'epoch': 1.0}\n",
      " 20%|████████▍                                 | 50/250 [18:55<54:14, 16.27s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 0.4476, 'grad_norm': 0.8390801548957825, 'learning_rate': 0.00018660254037844388, 'epoch': 1.02}\n",
      "{'loss': 0.194, 'grad_norm': 0.8735445141792297, 'learning_rate': 0.0001859406411501453, 'epoch': 1.04}\n",
      "{'loss': 0.316, 'grad_norm': 1.3753103017807007, 'learning_rate': 0.00018526401643540922, 'epoch': 1.06}\n",
      "{'loss': 0.2308, 'grad_norm': 0.6924192309379578, 'learning_rate': 0.00018457278217039736, 'epoch': 1.08}\n",
      "{'loss': 0.2623, 'grad_norm': 0.8266617655754089, 'learning_rate': 0.00018386705679454242, 'epoch': 1.1}\n",
      "{'loss': 0.2162, 'grad_norm': 1.3636929988861084, 'learning_rate': 0.00018314696123025454, 'epoch': 1.12}\n",
      "{'loss': 0.4238, 'grad_norm': 1.2359362840652466, 'learning_rate': 0.00018241261886220154, 'epoch': 1.14}\n",
      "{'loss': 0.2001, 'grad_norm': 0.665673553943634, 'learning_rate': 0.00018166415551616792, 'epoch': 1.16}\n",
      "{'loss': 0.1302, 'grad_norm': 0.6524714827537537, 'learning_rate': 0.00018090169943749476, 'epoch': 1.18}\n",
      "{'loss': 0.3033, 'grad_norm': 0.8035838603973389, 'learning_rate': 0.00018012538126910608, 'epoch': 1.2}\n",
      "{'loss': 0.3326, 'grad_norm': 0.8027909994125366, 'learning_rate': 0.00017933533402912354, 'epoch': 1.22}\n",
      "{'loss': 0.196, 'grad_norm': 0.8736813068389893, 'learning_rate': 0.00017853169308807448, 'epoch': 1.24}\n",
      "{'loss': 0.2859, 'grad_norm': 0.5840650200843811, 'learning_rate': 0.0001777145961456971, 'epoch': 1.26}\n",
      "{'loss': 0.2369, 'grad_norm': 0.6860517859458923, 'learning_rate': 0.00017688418320734598, 'epoch': 1.28}\n",
      "{'loss': 0.1646, 'grad_norm': 1.2029591798782349, 'learning_rate': 0.0001760405965600031, 'epoch': 1.3}\n",
      "{'loss': 0.3225, 'grad_norm': 0.9282876253128052, 'learning_rate': 0.00017518398074789775, 'epoch': 1.32}\n",
      "{'loss': 0.2315, 'grad_norm': 1.2928636074066162, 'learning_rate': 0.00017431448254773944, 'epoch': 1.34}\n",
      "{'loss': 0.3313, 'grad_norm': 1.2120795249938965, 'learning_rate': 0.00017343225094356855, 'epoch': 1.36}\n",
      "{'loss': 0.2694, 'grad_norm': 2.4225480556488037, 'learning_rate': 0.00017253743710122875, 'epoch': 1.38}\n",
      "{'loss': 0.1724, 'grad_norm': 0.9034881591796875, 'learning_rate': 0.00017163019434246547, 'epoch': 1.4}\n",
      "{'loss': 0.2502, 'grad_norm': 1.4034056663513184, 'learning_rate': 0.00017071067811865476, 'epoch': 1.42}\n",
      "{'loss': 0.3608, 'grad_norm': 1.0977011919021606, 'learning_rate': 0.00016977904598416803, 'epoch': 1.44}\n",
      "{'loss': 0.174, 'grad_norm': 0.6072863936424255, 'learning_rate': 0.0001688354575693754, 'epoch': 1.46}\n",
      "{'loss': 0.3151, 'grad_norm': 0.8650938272476196, 'learning_rate': 0.0001678800745532942, 'epoch': 1.48}\n",
      "{'loss': 0.3262, 'grad_norm': 1.0543535947799683, 'learning_rate': 0.00016691306063588583, 'epoch': 1.5}\n",
      " 30%|████████████▌                             | 75/250 [26:02<45:31, 15.61s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.07it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09724263846874237, 'eval_runtime': 15.5311, 'eval_samples_per_second': 2.06, 'eval_steps_per_second': 1.03, 'epoch': 1.5}\n",
      " 30%|████████████▌                             | 75/250 [26:18<45:31, 15.61s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.05it/s]\u001b[A\n",
      "{'loss': 0.3993, 'grad_norm': 1.017412781715393, 'learning_rate': 0.00016593458151000688, 'epoch': 1.52}\n",
      "{'loss': 0.3472, 'grad_norm': 0.9534249901771545, 'learning_rate': 0.00016494480483301836, 'epoch': 1.54}\n",
      "{'loss': 0.3958, 'grad_norm': 1.607896089553833, 'learning_rate': 0.00016394390019805848, 'epoch': 1.56}\n",
      "{'loss': 0.213, 'grad_norm': 0.9488537311553955, 'learning_rate': 0.00016293203910498376, 'epoch': 1.58}\n",
      "{'loss': 0.136, 'grad_norm': 0.5376736521720886, 'learning_rate': 0.00016190939493098344, 'epoch': 1.6}\n",
      "{'loss': 0.2371, 'grad_norm': 0.7788302898406982, 'learning_rate': 0.00016087614290087208, 'epoch': 1.62}\n",
      "{'loss': 0.1678, 'grad_norm': 0.6726142168045044, 'learning_rate': 0.00015983246005706593, 'epoch': 1.64}\n",
      "{'loss': 0.1957, 'grad_norm': 0.6511305570602417, 'learning_rate': 0.00015877852522924732, 'epoch': 1.66}\n",
      "{'loss': 0.4187, 'grad_norm': 0.9294118881225586, 'learning_rate': 0.0001577145190037234, 'epoch': 1.68}\n",
      "{'loss': 0.395, 'grad_norm': 0.6763216257095337, 'learning_rate': 0.00015664062369248328, 'epoch': 1.7}\n",
      "{'loss': 0.1912, 'grad_norm': 2.1997742652893066, 'learning_rate': 0.00015555702330196023, 'epoch': 1.72}\n",
      "{'loss': 0.1295, 'grad_norm': 0.6226291060447693, 'learning_rate': 0.00015446390350150273, 'epoch': 1.74}\n",
      "{'loss': 0.3912, 'grad_norm': 1.8520933389663696, 'learning_rate': 0.00015336145159156115, 'epoch': 1.76}\n",
      "{'loss': 0.3669, 'grad_norm': 0.911938488483429, 'learning_rate': 0.0001522498564715949, 'epoch': 1.78}\n",
      "{'loss': 0.2555, 'grad_norm': 2.928708553314209, 'learning_rate': 0.0001511293086077052, 'epoch': 1.8}\n",
      "{'loss': 0.2228, 'grad_norm': 0.8935403823852539, 'learning_rate': 0.00015000000000000001, 'epoch': 1.82}\n",
      "{'loss': 0.2427, 'grad_norm': 0.7211196422576904, 'learning_rate': 0.00014886212414969553, 'epoch': 1.84}\n",
      "{'loss': 0.1775, 'grad_norm': 0.6231023073196411, 'learning_rate': 0.00014771587602596084, 'epoch': 1.86}\n",
      "{'loss': 0.0817, 'grad_norm': 0.6570730805397034, 'learning_rate': 0.00014656145203251114, 'epoch': 1.88}\n",
      "{'loss': 0.2918, 'grad_norm': 1.2202973365783691, 'learning_rate': 0.00014539904997395468, 'epoch': 1.9}\n",
      "{'loss': 0.3233, 'grad_norm': 1.0097811222076416, 'learning_rate': 0.00014422886902190014, 'epoch': 1.92}\n",
      "{'loss': 0.2827, 'grad_norm': 0.6969637870788574, 'learning_rate': 0.00014305110968082952, 'epoch': 1.94}\n",
      "{'loss': 0.1373, 'grad_norm': 0.5678955912590027, 'learning_rate': 0.0001418659737537428, 'epoch': 1.96}\n",
      "{'loss': 0.2783, 'grad_norm': 1.3035064935684204, 'learning_rate': 0.00014067366430758004, 'epoch': 1.98}\n",
      "{'loss': 0.1473, 'grad_norm': 0.5696385502815247, 'learning_rate': 0.0001394743856384267, 'epoch': 2.0}\n",
      " 40%|████████████████▍                        | 100/250 [32:47<38:57, 15.59s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.48it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.10it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.08104546368122101, 'eval_runtime': 15.5023, 'eval_samples_per_second': 2.064, 'eval_steps_per_second': 1.032, 'epoch': 2.0}\n",
      " 40%|████████████████▍                        | 100/250 [33:03<38:57, 15.59s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.05it/s]\u001b[A\n",
      "{'loss': 0.1292, 'grad_norm': 0.4528033435344696, 'learning_rate': 0.000138268343236509, 'epoch': 2.02}\n",
      "{'loss': 0.117, 'grad_norm': 0.48422664403915405, 'learning_rate': 0.00013705574375098365, 'epoch': 2.04}\n",
      "{'loss': 0.1284, 'grad_norm': 0.36397048830986023, 'learning_rate': 0.00013583679495453, 'epoch': 2.06}\n",
      "{'loss': 0.1526, 'grad_norm': 0.6246386766433716, 'learning_rate': 0.0001346117057077493, 'epoch': 2.08}\n",
      "{'loss': 0.1301, 'grad_norm': 0.5181320309638977, 'learning_rate': 0.0001333806859233771, 'epoch': 2.1}\n",
      "{'loss': 0.2753, 'grad_norm': 1.514590859413147, 'learning_rate': 0.00013214394653031616, 'epoch': 2.12}\n",
      "{'loss': 0.2481, 'grad_norm': 0.8117044568061829, 'learning_rate': 0.00013090169943749476, 'epoch': 2.14}\n",
      "{'loss': 0.1309, 'grad_norm': 0.4661629796028137, 'learning_rate': 0.00012965415749755709, 'epoch': 2.16}\n",
      "{'loss': 0.1382, 'grad_norm': 0.6638111472129822, 'learning_rate': 0.00012840153447039228, 'epoch': 2.18}\n",
      "{'loss': 0.1494, 'grad_norm': 0.5364065170288086, 'learning_rate': 0.00012714404498650743, 'epoch': 2.2}\n",
      "{'loss': 0.216, 'grad_norm': 4.498599052429199, 'learning_rate': 0.00012588190451025207, 'epoch': 2.22}\n",
      "{'loss': 0.0793, 'grad_norm': 0.4989791214466095, 'learning_rate': 0.00012461532930289933, 'epoch': 2.24}\n",
      "{'loss': 0.1358, 'grad_norm': 0.4850667417049408, 'learning_rate': 0.00012334453638559057, 'epoch': 2.26}\n",
      "{'loss': 0.2663, 'grad_norm': 0.8544490337371826, 'learning_rate': 0.00012206974350215015, 'epoch': 2.28}\n",
      "{'loss': 0.1238, 'grad_norm': 0.676806628704071, 'learning_rate': 0.00012079116908177593, 'epoch': 2.3}\n",
      "{'loss': 0.0643, 'grad_norm': 0.6519524455070496, 'learning_rate': 0.00011950903220161285, 'epoch': 2.32}\n",
      "{'loss': 0.1999, 'grad_norm': 1.203258991241455, 'learning_rate': 0.00011822355254921478, 'epoch': 2.34}\n",
      "{'loss': 0.3307, 'grad_norm': 0.9475129246711731, 'learning_rate': 0.00011693495038490245, 'epoch': 2.36}\n",
      "{'loss': 0.048, 'grad_norm': 0.3155464828014374, 'learning_rate': 0.0001156434465040231, 'epoch': 2.38}\n",
      "{'loss': 0.0865, 'grad_norm': 0.3550707995891571, 'learning_rate': 0.00011434926219911793, 'epoch': 2.4}\n",
      "{'loss': 0.1468, 'grad_norm': 0.6604567766189575, 'learning_rate': 0.00011305261922200519, 'epoch': 2.42}\n",
      "{'loss': 0.1375, 'grad_norm': 0.5091627836227417, 'learning_rate': 0.00011175373974578378, 'epoch': 2.44}\n",
      "{'loss': 0.0829, 'grad_norm': 0.6370760798454285, 'learning_rate': 0.00011045284632676536, 'epoch': 2.46}\n",
      "{'loss': 0.081, 'grad_norm': 0.6135268211364746, 'learning_rate': 0.00010915016186634026, 'epoch': 2.48}\n",
      "{'loss': 0.1472, 'grad_norm': 0.803310751914978, 'learning_rate': 0.0001078459095727845, 'epoch': 2.5}\n",
      " 50%|████████████████████▌                    | 125/250 [39:55<32:27, 15.58s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09857296943664551, 'eval_runtime': 15.4986, 'eval_samples_per_second': 2.065, 'eval_steps_per_second': 1.032, 'epoch': 2.5}\n",
      " 50%|████████████████████▌                    | 125/250 [40:10<32:27, 15.58s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.05it/s]\u001b[A\n",
      "{'loss': 0.071, 'grad_norm': 0.4348026514053345, 'learning_rate': 0.00010654031292301432, 'epoch': 2.52}\n",
      "{'loss': 0.1029, 'grad_norm': 0.4494713842868805, 'learning_rate': 0.0001052335956242944, 'epoch': 2.54}\n",
      "{'loss': 0.1194, 'grad_norm': 0.6389365792274475, 'learning_rate': 0.00010392598157590688, 'epoch': 2.56}\n",
      "{'loss': 0.1287, 'grad_norm': 0.5584472417831421, 'learning_rate': 0.00010261769483078733, 'epoch': 2.58}\n",
      "{'loss': 0.171, 'grad_norm': 0.5980357527732849, 'learning_rate': 0.00010130895955713445, 'epoch': 2.6}\n",
      "{'loss': 0.1842, 'grad_norm': 0.850792407989502, 'learning_rate': 0.0001, 'epoch': 2.62}\n",
      "{'loss': 0.0755, 'grad_norm': 0.6540268659591675, 'learning_rate': 9.869104044286558e-05, 'epoch': 2.64}\n",
      "{'loss': 0.1642, 'grad_norm': 1.8469873666763306, 'learning_rate': 9.73823051692127e-05, 'epoch': 2.66}\n",
      "{'loss': 0.0505, 'grad_norm': 0.3537342846393585, 'learning_rate': 9.607401842409317e-05, 'epoch': 2.68}\n",
      "{'loss': 0.1405, 'grad_norm': 0.8895271420478821, 'learning_rate': 9.476640437570562e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1297, 'grad_norm': 0.4539913833141327, 'learning_rate': 9.345968707698569e-05, 'epoch': 2.72}\n",
      "{'loss': 0.1331, 'grad_norm': 0.48248517513275146, 'learning_rate': 9.215409042721552e-05, 'epoch': 2.74}\n",
      "{'loss': 0.195, 'grad_norm': 0.5909149050712585, 'learning_rate': 9.084983813365978e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0949, 'grad_norm': 0.6158178448677063, 'learning_rate': 8.954715367323468e-05, 'epoch': 2.78}\n",
      "{'loss': 0.0996, 'grad_norm': 0.6994155645370483, 'learning_rate': 8.824626025421626e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3296, 'grad_norm': 1.1262935400009155, 'learning_rate': 8.694738077799488e-05, 'epoch': 2.82}\n",
      "{'loss': 0.233, 'grad_norm': 0.7819103002548218, 'learning_rate': 8.565073780088208e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1935, 'grad_norm': 1.1819766759872437, 'learning_rate': 8.435655349597689e-05, 'epoch': 2.86}\n",
      "{'loss': 0.1019, 'grad_norm': 0.8923036456108093, 'learning_rate': 8.306504961509754e-05, 'epoch': 2.88}\n",
      "{'loss': 0.1235, 'grad_norm': 2.4404571056365967, 'learning_rate': 8.177644745078526e-05, 'epoch': 2.9}\n",
      "{'loss': 0.1134, 'grad_norm': 0.4176887571811676, 'learning_rate': 8.049096779838719e-05, 'epoch': 2.92}\n",
      "{'loss': 0.098, 'grad_norm': 0.8398229479789734, 'learning_rate': 7.920883091822408e-05, 'epoch': 2.94}\n",
      "{'loss': 0.2203, 'grad_norm': 0.9325093030929565, 'learning_rate': 7.79302564978499e-05, 'epoch': 2.96}\n",
      "{'loss': 0.1363, 'grad_norm': 0.4723805785179138, 'learning_rate': 7.66554636144095e-05, 'epoch': 2.98}\n",
      "{'loss': 0.1492, 'grad_norm': 0.5673199892044067, 'learning_rate': 7.53846706971007e-05, 'epoch': 3.0}\n",
      " 60%|████████████████████████▌                | 150/250 [46:40<25:57, 15.58s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.10it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.48it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.10it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.03it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.08513502776622772, 'eval_runtime': 15.5704, 'eval_samples_per_second': 2.055, 'eval_steps_per_second': 1.028, 'epoch': 3.0}\n",
      " 60%|████████████████████████▌                | 150/250 [46:56<25:57, 15.58s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 0.1156, 'grad_norm': 0.300691157579422, 'learning_rate': 7.411809548974792e-05, 'epoch': 3.02}\n",
      "{'loss': 0.1877, 'grad_norm': 0.9444217681884766, 'learning_rate': 7.285595501349258e-05, 'epoch': 3.04}\n",
      "{'loss': 0.0626, 'grad_norm': 0.3203740119934082, 'learning_rate': 7.159846552960774e-05, 'epoch': 3.06}\n",
      "{'loss': 0.0592, 'grad_norm': 0.29597508907318115, 'learning_rate': 7.034584250244291e-05, 'epoch': 3.08}\n",
      "{'loss': 0.0803, 'grad_norm': 0.40066391229629517, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.1}\n",
      "{'loss': 0.136, 'grad_norm': 0.4676614999771118, 'learning_rate': 6.785605346968386e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0557, 'grad_norm': 0.45261621475219727, 'learning_rate': 6.661931407662292e-05, 'epoch': 3.14}\n",
      "{'loss': 0.1422, 'grad_norm': 0.34548673033714294, 'learning_rate': 6.538829429225069e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0927, 'grad_norm': 0.3697788715362549, 'learning_rate': 6.416320504546997e-05, 'epoch': 3.18}\n",
      "{'loss': 0.0451, 'grad_norm': 0.3561696410179138, 'learning_rate': 6.294425624901638e-05, 'epoch': 3.2}\n",
      "{'loss': 0.1063, 'grad_norm': 0.43023237586021423, 'learning_rate': 6.173165676349103e-05, 'epoch': 3.22}\n",
      "{'loss': 0.036, 'grad_norm': 0.3495849370956421, 'learning_rate': 6.052561436157329e-05, 'epoch': 3.24}\n",
      "{'loss': 0.1024, 'grad_norm': 0.47392699122428894, 'learning_rate': 5.9326335692419995e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0438, 'grad_norm': 0.3974296450614929, 'learning_rate': 5.8134026246257225e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0283, 'grad_norm': 0.26435065269470215, 'learning_rate': 5.694889031917047e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0936, 'grad_norm': 0.8527318239212036, 'learning_rate': 5.577113097809989e-05, 'epoch': 3.32}\n",
      "{'loss': 0.0413, 'grad_norm': 0.5769069194793701, 'learning_rate': 5.4600950026045326e-05, 'epoch': 3.34}\n",
      "{'loss': 0.0345, 'grad_norm': 0.38833892345428467, 'learning_rate': 5.343854796748886e-05, 'epoch': 3.36}\n",
      "{'loss': 0.08, 'grad_norm': 0.3726791441440582, 'learning_rate': 5.2284123974039154e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0713, 'grad_norm': 0.683254063129425, 'learning_rate': 5.113787585030454e-05, 'epoch': 3.4}\n",
      "{'loss': 0.0257, 'grad_norm': 0.35879212617874146, 'learning_rate': 5.000000000000002e-05, 'epoch': 3.42}\n",
      "{'loss': 0.1148, 'grad_norm': 0.5910689234733582, 'learning_rate': 4.887069139229481e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0312, 'grad_norm': 0.3102186918258667, 'learning_rate': 4.7750143528405126e-05, 'epoch': 3.46}\n",
      "{'loss': 0.0373, 'grad_norm': 0.38639286160469055, 'learning_rate': 4.6638548408438856e-05, 'epoch': 3.48}\n",
      "{'loss': 0.0337, 'grad_norm': 0.9283379316329956, 'learning_rate': 4.5536096498497295e-05, 'epoch': 3.5}\n",
      " 70%|████████████████████████████▋            | 175/250 [53:53<19:32, 15.64s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.08697209507226944, 'eval_runtime': 15.4985, 'eval_samples_per_second': 2.065, 'eval_steps_per_second': 1.032, 'epoch': 3.5}\n",
      " 70%|████████████████████████████▋            | 175/250 [54:08<19:32, 15.64s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.05it/s]\u001b[A\n",
      "{'loss': 0.2249, 'grad_norm': 0.7006185054779053, 'learning_rate': 4.444297669803981e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0429, 'grad_norm': 0.3749503195285797, 'learning_rate': 4.335937630751674e-05, 'epoch': 3.54}\n",
      "{'loss': 0.1198, 'grad_norm': 0.534498393535614, 'learning_rate': 4.228548099627665e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0502, 'grad_norm': 0.22778502106666565, 'learning_rate': 4.12214747707527e-05, 'epoch': 3.58}\n",
      "{'loss': 0.0874, 'grad_norm': 0.5774415731430054, 'learning_rate': 4.01675399429341e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0702, 'grad_norm': 0.4219587743282318, 'learning_rate': 3.9123857099127936e-05, 'epoch': 3.62}\n",
      "{'loss': 0.0122, 'grad_norm': 0.1991691142320633, 'learning_rate': 3.8090605069016595e-05, 'epoch': 3.64}\n",
      "{'loss': 0.0657, 'grad_norm': 0.5870933532714844, 'learning_rate': 3.7067960895016275e-05, 'epoch': 3.66}\n",
      "{'loss': 0.0706, 'grad_norm': 0.6591281294822693, 'learning_rate': 3.6056099801941534e-05, 'epoch': 3.68}\n",
      "{'loss': 0.061, 'grad_norm': 0.3748343884944916, 'learning_rate': 3.5055195166981645e-05, 'epoch': 3.7}\n",
      "{'loss': 0.2406, 'grad_norm': 0.7400772571563721, 'learning_rate': 3.406541848999312e-05, 'epoch': 3.72}\n",
      "{'loss': 0.0244, 'grad_norm': 0.3246171772480011, 'learning_rate': 3.308693936411421e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0371, 'grad_norm': 0.3679971992969513, 'learning_rate': 3.211992544670582e-05, 'epoch': 3.76}\n",
      "{'loss': 0.068, 'grad_norm': 0.4657290279865265, 'learning_rate': 3.116454243062459e-05, 'epoch': 3.78}\n",
      "{'loss': 0.1217, 'grad_norm': 0.729621410369873, 'learning_rate': 3.0220954015832003e-05, 'epoch': 3.8}\n",
      "{'loss': 0.0597, 'grad_norm': 0.9192330837249756, 'learning_rate': 2.9289321881345254e-05, 'epoch': 3.82}\n",
      "{'loss': 0.1268, 'grad_norm': 0.6015221476554871, 'learning_rate': 2.8369805657534575e-05, 'epoch': 3.84}\n",
      "{'loss': 0.1032, 'grad_norm': 0.6065577864646912, 'learning_rate': 2.746256289877126e-05, 'epoch': 3.86}\n",
      "{'loss': 0.0468, 'grad_norm': 0.5920534133911133, 'learning_rate': 2.6567749056431467e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0168, 'grad_norm': 0.22864899039268494, 'learning_rate': 2.5685517452260567e-05, 'epoch': 3.9}\n",
      "{'loss': 0.0806, 'grad_norm': 1.20942223072052, 'learning_rate': 2.4816019252102273e-05, 'epoch': 3.92}\n",
      "{'loss': 0.0902, 'grad_norm': 0.7123748064041138, 'learning_rate': 2.3959403439996907e-05, 'epoch': 3.94}\n",
      "{'loss': 0.0214, 'grad_norm': 0.4283508062362671, 'learning_rate': 2.3115816792654056e-05, 'epoch': 3.96}\n",
      "{'loss': 0.0743, 'grad_norm': 2.3388736248016357, 'learning_rate': 2.2285403854302912e-05, 'epoch': 3.98}\n",
      "{'loss': 0.0157, 'grad_norm': 0.29964151978492737, 'learning_rate': 2.146830691192553e-05, 'epoch': 4.0}\n",
      " 80%|███████████████████████████████▏       | 200/250 [1:00:36<12:32, 15.06s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.48it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.10it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09013520181179047, 'eval_runtime': 15.468, 'eval_samples_per_second': 2.069, 'eval_steps_per_second': 1.034, 'epoch': 4.0}\n",
      " 80%|███████████████████████████████▏       | 200/250 [1:00:52<12:32, 15.06s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.05it/s]\u001b[A\n",
      "{'loss': 0.0521, 'grad_norm': 0.2966751158237457, 'learning_rate': 2.0664665970876496e-05, 'epoch': 4.02}\n",
      "{'loss': 0.0819, 'grad_norm': 0.47996556758880615, 'learning_rate': 1.9874618730893946e-05, 'epoch': 4.04}\n",
      "{'loss': 0.0703, 'grad_norm': 0.3398633599281311, 'learning_rate': 1.9098300562505266e-05, 'epoch': 4.06}\n",
      "{'loss': 0.0489, 'grad_norm': 0.45796704292297363, 'learning_rate': 1.833584448383211e-05, 'epoch': 4.08}\n",
      "{'loss': 0.0161, 'grad_norm': 0.25059959292411804, 'learning_rate': 1.7587381137798432e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0202, 'grad_norm': 0.29287728667259216, 'learning_rate': 1.6853038769745467e-05, 'epoch': 4.12}\n",
      "{'loss': 0.0963, 'grad_norm': 0.5409361124038696, 'learning_rate': 1.6132943205457606e-05, 'epoch': 4.14}\n",
      "{'loss': 0.0587, 'grad_norm': 0.293560266494751, 'learning_rate': 1.542721782960268e-05, 'epoch': 4.16}\n",
      "{'loss': 0.1827, 'grad_norm': 0.5913267135620117, 'learning_rate': 1.4735983564590783e-05, 'epoch': 4.18}\n",
      "{'loss': 0.0375, 'grad_norm': 0.3992407023906708, 'learning_rate': 1.405935884985473e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0336, 'grad_norm': 0.489620178937912, 'learning_rate': 1.339745962155613e-05, 'epoch': 4.22}\n",
      "{'loss': 0.0188, 'grad_norm': 0.20556966960430145, 'learning_rate': 1.2750399292720283e-05, 'epoch': 4.24}\n",
      "{'loss': 0.0369, 'grad_norm': 0.42201918363571167, 'learning_rate': 1.2118288733803473e-05, 'epoch': 4.26}\n",
      "{'loss': 0.0694, 'grad_norm': 0.3690929114818573, 'learning_rate': 1.1501236253695823e-05, 'epoch': 4.28}\n",
      "{'loss': 0.0548, 'grad_norm': 0.4552825391292572, 'learning_rate': 1.0899347581163221e-05, 'epoch': 4.3}\n",
      "{'loss': 0.0228, 'grad_norm': 0.22311653196811676, 'learning_rate': 1.0312725846731175e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0346, 'grad_norm': 0.38481220602989197, 'learning_rate': 9.74147156501396e-06, 'epoch': 4.34}\n",
      "{'loss': 0.0317, 'grad_norm': 0.3149450123310089, 'learning_rate': 9.185682617491863e-06, 'epoch': 4.36}\n",
      "{'loss': 0.0413, 'grad_norm': 0.26530784368515015, 'learning_rate': 8.645454235739903e-06, 'epoch': 4.38}\n",
      "{'loss': 0.0537, 'grad_norm': 0.5454449653625488, 'learning_rate': 8.12087898511018e-06, 'epoch': 4.4}\n",
      "{'loss': 0.0092, 'grad_norm': 0.17091989517211914, 'learning_rate': 7.612046748871327e-06, 'epoch': 4.42}\n",
      "{'loss': 0.0513, 'grad_norm': 0.26821646094322205, 'learning_rate': 7.119044712807577e-06, 'epoch': 4.44}\n",
      "{'loss': 0.0204, 'grad_norm': 0.31219032406806946, 'learning_rate': 6.6419573502798374e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0231, 'grad_norm': 0.27476397156715393, 'learning_rate': 6.180866407751595e-06, 'epoch': 4.48}\n",
      "{'loss': 0.0751, 'grad_norm': 0.5197102427482605, 'learning_rate': 5.735850890782157e-06, 'epoch': 4.5}\n",
      " 90%|███████████████████████████████████    | 225/250 [1:07:44<06:29, 15.58s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09238056093454361, 'eval_runtime': 15.4924, 'eval_samples_per_second': 2.066, 'eval_steps_per_second': 1.033, 'epoch': 4.5}\n",
      " 90%|███████████████████████████████████    | 225/250 [1:07:59<06:29, 15.58s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.05it/s]\u001b[A\n",
      "{'loss': 0.0561, 'grad_norm': 0.23637989163398743, 'learning_rate': 5.306987050489442e-06, 'epoch': 4.52}\n",
      "{'loss': 0.0221, 'grad_norm': 0.32920384407043457, 'learning_rate': 4.8943483704846475e-06, 'epoch': 4.54}\n",
      "{'loss': 0.0088, 'grad_norm': 0.1494336873292923, 'learning_rate': 4.498005554281337e-06, 'epoch': 4.56}\n",
      "{'loss': 0.0661, 'grad_norm': 0.4237229526042938, 'learning_rate': 4.118026513180695e-06, 'epoch': 4.58}\n",
      "{'loss': 0.0333, 'grad_norm': 0.3193747103214264, 'learning_rate': 3.7544763546352834e-06, 'epoch': 4.6}\n",
      "{'loss': 0.1768, 'grad_norm': 0.4977461099624634, 'learning_rate': 3.40741737109318e-06, 'epoch': 4.62}\n",
      "{'loss': 0.0609, 'grad_norm': 0.40460100769996643, 'learning_rate': 3.0769090293245705e-06, 'epoch': 4.64}\n",
      "{'loss': 0.0587, 'grad_norm': 0.31321221590042114, 'learning_rate': 2.7630079602323442e-06, 'epoch': 4.66}\n",
      "{'loss': 0.0046, 'grad_norm': 0.08244764804840088, 'learning_rate': 2.465767949148734e-06, 'epoch': 4.68}\n",
      "{'loss': 0.0247, 'grad_norm': 0.18082323670387268, 'learning_rate': 2.1852399266194314e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0882, 'grad_norm': 0.4544087052345276, 'learning_rate': 1.921471959676957e-06, 'epoch': 4.72}\n",
      "{'loss': 0.0544, 'grad_norm': 0.3585197925567627, 'learning_rate': 1.6745092436045494e-06, 'epoch': 4.74}\n",
      "{'loss': 0.1195, 'grad_norm': 0.46502625942230225, 'learning_rate': 1.444394094192225e-06, 'epoch': 4.76}\n",
      "{'loss': 0.0688, 'grad_norm': 0.3536054491996765, 'learning_rate': 1.231165940486234e-06, 'epoch': 4.78}\n",
      "{'loss': 0.0485, 'grad_norm': 0.39938053488731384, 'learning_rate': 1.0348613180329757e-06, 'epoch': 4.8}\n",
      "{'loss': 0.0087, 'grad_norm': 0.24748995900154114, 'learning_rate': 8.555138626189618e-07, 'epoch': 4.82}\n",
      "{'loss': 0.0307, 'grad_norm': 0.6202878355979919, 'learning_rate': 6.931543045073708e-07, 'epoch': 4.84}\n",
      "{'loss': 0.028, 'grad_norm': 0.21602952480316162, 'learning_rate': 5.478104631726711e-07, 'epoch': 4.86}\n",
      "{'loss': 0.0134, 'grad_norm': 0.22876104712486267, 'learning_rate': 4.1950724253383423e-07, 'epoch': 4.88}\n",
      "{'loss': 0.0099, 'grad_norm': 0.13746275007724762, 'learning_rate': 3.0826662668720364e-07, 'epoch': 4.9}\n",
      "{'loss': 0.0956, 'grad_norm': 0.41904518008232117, 'learning_rate': 2.141076761396521e-07, 'epoch': 4.92}\n",
      "{'loss': 0.0225, 'grad_norm': 0.38109105825424194, 'learning_rate': 1.3704652454261668e-07, 'epoch': 4.94}\n",
      "{'loss': 0.0191, 'grad_norm': 0.3151032030582428, 'learning_rate': 7.709637592770991e-08, 'epoch': 4.96}\n",
      "{'loss': 0.0908, 'grad_norm': 0.4483965039253235, 'learning_rate': 3.4267502444274015e-08, 'epoch': 4.98}\n",
      "{'loss': 0.0685, 'grad_norm': 0.6462976932525635, 'learning_rate': 8.567242599299973e-09, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 250/250 [1:14:29<00:00, 15.58s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.09it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.10it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09196493029594421, 'eval_runtime': 15.4894, 'eval_samples_per_second': 2.066, 'eval_steps_per_second': 1.033, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 250/250 [1:14:44<00:00, 15.58s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.05it/s]\u001b[A\n",
      "{'train_runtime': 4486.4678, 'train_samples_per_second': 0.446, 'train_steps_per_second': 0.056, 'train_loss': 0.4547728468701243, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 250/250 [1:14:46<00:00, 17.95s/it]\n",
      "[2025-10-07 13:37:59,229] [INFO] [axolotl.train.save_trained_model:244] [PID:3090169] [RANK:0] Training completed! Saving trained model to ./out-gemma-3-4b-it.\u001b[39m\n",
      "[2025-10-07 13:37:59,765] [INFO] [axolotl.train.save_trained_model:341] [PID:3090169] [RANK:0] Model successfully saved to ./out-gemma-3-4b-it\u001b[39m\n",
      "\u001b[0mCPU times: user 26.8 s, sys: 3.17 s, total: 30 s\n",
      "Wall time: 1h 26min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the LoRA/DoRA into the base model (for inference & quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 13:38:13,082] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3100918] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-07 13:38:13,083] [WARNING] [axolotl.utils.schemas.config.check_sample_packing_wo_flash:482] [PID:3100918] [RANK:0] sample_packing without flash, sdp, xformers or flex attention does not handle cross sample decontamination.\u001b[39m\n",
      "\u001b[33m[2025-10-07 13:38:13,083] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3100918] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-07 13:38:13,428] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3100918] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-07 13:38:13,440] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:318] [PID:3100918] [RANK:0] loading tokenizer... google/gemma-3-4b-it\u001b[39m\n",
      "[2025-10-07 13:38:14,881] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:321] [PID:3100918] [RANK:0] loading model...\u001b[39m\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  6.81s/it]\n",
      "[2025-10-07 13:38:31,704] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3100918] [RANK:0] cuda memory usage after model load: 8.010GB (+1.252GB cache, +1.223GB misc)\u001b[39m\n",
      "[2025-10-07 13:38:31,734] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3100918] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-07 13:38:31,742] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:3100918] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 60,614,656 || all params: 4,360,694,128 || trainable%: 1.3900\n",
      "[2025-10-07 13:38:33,208] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3100918] [RANK:0] cuda memory usage after adapters: 8.243GB (+3.723GB cache, +1.317GB misc)\u001b[39m\n",
      "[2025-10-07 13:38:33,657] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:327] [PID:3100918] [RANK:0] loading processor...\u001b[39m\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[2025-10-07 13:38:38,631] [INFO] [axolotl.cli.merge_lora.do_merge_lora:31] [PID:3100918] [RANK:0] Running merge of LoRA with base model...\u001b[39m\n",
      "Unloading and merging model: 100%|████████| 1160/1160 [00:00<00:00, 5544.59it/s]\n",
      "[2025-10-07 13:38:38,859] [INFO] [axolotl.cli.merge_lora.do_merge_lora:44] [PID:3100918] [RANK:0] Saving merged model to: out-gemma-3-4b-it/merged...\u001b[39m\n",
      "\u001b[0mCPU times: user 362 ms, sys: 52.8 ms, total: 415 ms\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/axolotl merge-lora {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-07 13:39:14 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-07 13:39:18 [utils.py:328] non-default args: {'max_model_len': 8192, 'disable_log_stats': True, 'model': 'out-gemma-3-4b-it/merged'}\n",
      "INFO 10-07 13:39:29 [__init__.py:742] Resolved architecture: Gemma3ForConditionalGeneration\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO 10-07 13:39:29 [__init__.py:1815] Using max model len 8192\n",
      "INFO 10-07 13:39:30 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:35 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:35 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='out-gemma-3-4b-it/merged', speculative_config=None, tokenizer='out-gemma-3-4b-it/merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=out-gemma-3-4b-it/merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[W1007 13:39:37.356501570 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:37 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m WARNING 10-07 13:39:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:42 [gpu_model_runner.py:2338] Starting to load model out-gemma-3-4b-it/merged...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:42 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:42 [cuda.py:379] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:43 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:45 [default_loader.py:268] Loading weights took 2.15 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:45 [gpu_model_runner.py:2392] Model loading took 8.5834 GiB and 2.661030 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:39:46 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 31 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:00 [backends.py:539] Using cache directory: /home/oisuomin/.cache/vllm/torch_compile_cache/7f409a8543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:00 [backends.py:550] Dynamo bytecode transform time: 9.65 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 14.270 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:22 [monitor.py:34] torch.compile takes 9.65 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:23 [gpu_worker.py:298] Available KV cache memory: 59.39 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m WARNING 10-07 13:40:23 [kv_cache_utils.py:986] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:23 [kv_cache_utils.py:1028] GPU KV cache size: 444,784 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:23 [kv_cache_utils.py:1032] Maximum concurrency for 8,192 tokens per request: 54.21x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:02<00\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:26 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 2.13 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:26 [gpu_worker.py:391] Free memory on device (78.7/79.18 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 8.58 GiB for weight, 3.27 GiB for peak activation, 0.02 GiB for non-torch memory, and 2.13 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=61316677632` to fit into requested memory, or `--kv-cache-memory=69297487872` to fully utilize gpu memory. Current kv cache memory in use is 63766151168 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3101380)\u001b[0;0m INFO 10-07 13:40:27 [core.py:218] init engine (profile, create kv cache, warmup model) took 41.10 seconds\n",
      "INFO 10-07 13:40:28 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-07 13:40:28 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Adding requests: 100%|███████████████████████| 182/182 [00:00<00:00, 469.32it/s]\n",
      "Processed prompts: 100%|█| 182/182 [00:24<00:00,  7.55it/s, est. speed input: 18\n",
      "Errors: 1 out of 182 records (0.55%)\n",
      "ERROR 10-07 13:40:54 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n",
      "| language   | field     |   mean |   size |\n",
      "|------------|-----------|--------|--------|\n",
      "| en         | alt_title | 0.8033 |     61 |\n",
      "| en         | creator   | 0.8826 |     61 |\n",
      "| en         | doi       | 0.9836 |     61 |\n",
      "| en         | e-isbn    | 0.8470 |     61 |\n",
      "| en         | e-issn    | 0.9508 |     61 |\n",
      "| en         | language  | 1.0000 |     61 |\n",
      "| en         | p-isbn    | 0.8361 |     61 |\n",
      "| en         | p-issn    | 0.9344 |     61 |\n",
      "| en         | publisher | 0.7377 |     61 |\n",
      "| en         | title     | 0.8689 |     61 |\n",
      "| en         | type_coar | 0.8525 |     61 |\n",
      "| en         | year      | 0.8852 |     61 |\n",
      "| fi         | alt_title | 0.8767 |     73 |\n",
      "| fi         | creator   | 0.8927 |     73 |\n",
      "| fi         | doi       | 0.9863 |     73 |\n",
      "| fi         | e-isbn    | 0.9863 |     73 |\n",
      "| fi         | e-issn    | 0.9452 |     73 |\n",
      "| fi         | language  | 0.9726 |     73 |\n",
      "| fi         | p-isbn    | 0.9863 |     73 |\n",
      "| fi         | p-issn    | 0.9726 |     73 |\n",
      "| fi         | publisher | 0.8630 |     73 |\n",
      "| fi         | title     | 0.7945 |     73 |\n",
      "| fi         | type_coar | 0.8904 |     73 |\n",
      "| fi         | year      | 0.8356 |     73 |\n",
      "| se         | alt_title | 1.0000 |      3 |\n",
      "| se         | creator   | 1.0000 |      3 |\n",
      "| se         | doi       | 1.0000 |      3 |\n",
      "| se         | e-isbn    | 1.0000 |      3 |\n",
      "| se         | e-issn    | 1.0000 |      3 |\n",
      "| se         | language  | 1.0000 |      3 |\n",
      "| se         | p-isbn    | 1.0000 |      3 |\n",
      "| se         | p-issn    | 1.0000 |      3 |\n",
      "| se         | publisher | 0.3333 |      3 |\n",
      "| se         | title     | 0.3333 |      3 |\n",
      "| se         | type_coar | 0.6667 |      3 |\n",
      "| se         | year      | 0.6667 |      3 |\n",
      "| sv         | alt_title | 0.7556 |     45 |\n",
      "| sv         | creator   | 0.9222 |     45 |\n",
      "| sv         | doi       | 1.0000 |     45 |\n",
      "| sv         | e-isbn    | 0.9333 |     45 |\n",
      "| sv         | e-issn    | 0.9778 |     45 |\n",
      "| sv         | language  | 1.0000 |     45 |\n",
      "| sv         | p-isbn    | 0.9111 |     45 |\n",
      "| sv         | p-issn    | 0.9556 |     45 |\n",
      "| sv         | publisher | 0.8000 |     45 |\n",
      "| sv         | title     | 0.9333 |     45 |\n",
      "| sv         | type_coar | 0.9333 |     45 |\n",
      "| sv         | year      | 0.9111 |     45 |\n",
      "CPU times: user 680 ms, sys: 98.7 ms, total: 779 ms\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate using the evaluate-model script, which needs venv with vLLM installed\n",
    "!../dspy/venv/bin/python evaluate-model.py out-{MODEL_SHORT_NAME}/merged axolotl-test.jsonl ../../eval/results-{MODEL_SHORT_NAME}.md\n",
    "!cat ../../eval/results-{MODEL_SHORT_NAME}.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greylitlm-axolotl",
   "language": "python",
   "name": "greylitlm-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
