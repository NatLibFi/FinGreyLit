{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Gemma-3-4B-it model using Axolotl framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/appl/easybuild/opt/CUDA/12.6.0\n"
     ]
    }
   ],
   "source": [
    "!printenv CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit\"\n",
    "#SLICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1224 train records\n",
      "Wrote 377 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'assistant', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"axolotl-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"axolotl-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"axolotl-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: axolotl-train.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "test_datasets:\n",
    "  - path: axolotl-eval.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "chat_template: gemma3\n",
    "eot_tokens:\n",
    "  - <end_of_turn>\n",
    "\n",
    "peft_use_dora: true\n",
    "adapter: lora\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "#lora_target_linear: true\n",
    "lora_target_modules: 'model.language_model.layers.[\\\\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 5\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-10-10 11:04:24,609] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3633013] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 11:04:24,610] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3633013] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 11:04:24,965] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3633013] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 11:04:26,971] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3633013] [RANK:0] Unable to find prepared dataset in last_run_prepared/cb6ba859f59fc79bea35912b9debcad8\u001b[39m\n",
      "[2025-10-10 11:04:26,971] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3633013] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 11:04:26,971] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3633013] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 1224 examples [00:00, 21364.16 examples/s]\n",
      "[2025-10-10 11:04:27,639] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3633013] [RANK:0] Loading dataset: axolotl-train.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 11:04:27,710] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3633013] [RANK:0] Using chat template:\n",
      "---\n",
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|█| 1224/1224 [00:12<00:00, 98.49 examples\n",
      "[2025-10-10 11:04:41,058] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3633013] [RANK:0] min_input_len: 160\u001b[39m\n",
      "[2025-10-10 11:04:41,058] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3633013] [RANK:0] max_input_len: 12150\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 1224/1224 [00:00<00:00, 2390.04 e\n",
      "\u001b[33m[2025-10-10 11:04:43,110] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:201] [PID:3633013] [RANK:0] Dropped 10 long samples from dataset\u001b[39m\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 1214/1214 [00:00<\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 1214/1214 [00:00<\n",
      "Saving the dataset (1/1 shards): 100%|█| 1214/1214 [00:00<00:00, 11411.13 exampl\n",
      "[2025-10-10 11:04:48,409] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3633013] [RANK:0] Unable to find prepared dataset in last_run_prepared/adda697e020e77476dddfe4a21729386\u001b[39m\n",
      "[2025-10-10 11:04:48,409] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3633013] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 11:04:48,409] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3633013] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 3439.98 examples/s]\n",
      "[2025-10-10 11:04:48,867] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3633013] [RANK:0] Loading dataset: axolotl-eval.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 11:04:48,941] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3633013] [RANK:0] Using chat template:\n",
      "---\n",
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:11<00:00,  2.71 examples/s]\n",
      "[2025-10-10 11:05:01,875] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3633013] [RANK:0] min_input_len: 386\u001b[39m\n",
      "[2025-10-10 11:05:01,875] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3633013] [RANK:0] max_input_len: 3380\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:00<00:00, 143.88 exampl\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Saving the dataset (1/1 shards): 100%|██| 32/32 [00:00<00:00, 804.66 examples/s]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 11:06:16,162] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3633013] [RANK:0] gather_len_batches: [389]\u001b[39m\n",
      "[2025-10-10 11:06:16,163] [INFO] [axolotl.utils.data.sft._prepare_standard_dataset:123] [PID:3633013] [RANK:0] Maximum number of steps set at 485\u001b[39m\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.02s/it]\n",
      "[2025-10-10 11:06:44,408] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3633013] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 11:06:44,945] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:3633013] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 60,614,656 || all params: 4,360,694,128 || trainable%: 1.3900\n",
      "[2025-10-10 11:09:37,014] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3633013] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n",
      "[2025-10-10 11:09:41,464] [WARNING] [accelerate.utils.other.check_os_kernel:441] [PID:3633013] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[2025-10-10 11:10:04,164] [INFO] [axolotl.train.save_initial_configs:403] [PID:3633013] [RANK:0] Pre-saving adapter config to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-10 11:10:04,166] [INFO] [axolotl.train.save_initial_configs:407] [PID:3633013] [RANK:0] Pre-saving tokenizer to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-10 11:10:04,642] [INFO] [axolotl.train.save_initial_configs:410] [PID:3633013] [RANK:0] Pre-saving model config to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-10 11:10:04,651] [INFO] [axolotl.train.save_initial_configs:414] [PID:3633013] [RANK:0] Pre-saving processor to ./out-gemma-3-4b-it...\u001b[39m\n",
      "[2025-10-10 11:10:06,939] [INFO] [axolotl.train.execute_training:225] [PID:3633013] [RANK:0] Starting trainer...\u001b[39m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 11:11:16,315] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3633013] [RANK:0] gather_len_batches: [389]\u001b[39m\n",
      "  0%|                                                   | 0/485 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.21it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.51it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.29it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.564026355743408, 'eval_runtime': 17.3139, 'eval_samples_per_second': 1.848, 'eval_steps_per_second': 0.924, 'epoch': 0}\n",
      "  0%|                                                   | 0/485 [00:17<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 13.13, 'grad_norm': 31.856008529663086, 'learning_rate': 0.0, 'epoch': 0.01}0m\u001b[0m\n",
      "  0%|                                         | 1/485 [00:50<6:51:10, 50.97s/it][2025-10-10 11:12:23,356] [INFO] [axolotl.utils.callbacks.log_gpu_memory_usage:107] [PID:3633013] [RANK:0] cuda memory usage while training: 8.381GB (+31.894GB cache, +1.326GB misc)\u001b[39m\n",
      "{'loss': 10.3453, 'grad_norm': 26.45602035522461, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 11.1161, 'grad_norm': 25.019351959228516, 'learning_rate': 4e-05, 'epoch': 0.03}\n",
      "{'loss': 9.2587, 'grad_norm': 20.11052894592285, 'learning_rate': 6e-05, 'epoch': 0.04}\n",
      "{'loss': 7.1834, 'grad_norm': 16.916229248046875, 'learning_rate': 8e-05, 'epoch': 0.05}\n",
      "{'loss': 6.0819, 'grad_norm': 12.74764347076416, 'learning_rate': 0.0001, 'epoch': 0.06}\n",
      "{'loss': 5.5547, 'grad_norm': 8.526792526245117, 'learning_rate': 0.00012, 'epoch': 0.07}\n",
      "{'loss': 4.7042, 'grad_norm': 5.876665115356445, 'learning_rate': 0.00014, 'epoch': 0.08}\n",
      "{'loss': 2.8572, 'grad_norm': 6.875615119934082, 'learning_rate': 0.00016, 'epoch': 0.09}\n",
      "{'loss': 2.6347, 'grad_norm': 3.710017681121826, 'learning_rate': 0.00018, 'epoch': 0.1}\n",
      "{'loss': 2.0578, 'grad_norm': 3.6049857139587402, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.6288, 'grad_norm': 3.0990538597106934, 'learning_rate': 0.0001999978128380225, 'epoch': 0.12}\n",
      "{'loss': 1.6452, 'grad_norm': 3.194932222366333, 'learning_rate': 0.0001999912514477634, 'epoch': 0.13}\n",
      "{'loss': 1.2752, 'grad_norm': 2.7498936653137207, 'learning_rate': 0.0001999803161162393, 'epoch': 0.14}\n",
      "{'loss': 0.9521, 'grad_norm': 2.153846502304077, 'learning_rate': 0.00019996500732179695, 'epoch': 0.15}\n",
      "{'loss': 0.6357, 'grad_norm': 2.25500750541687, 'learning_rate': 0.00019994532573409262, 'epoch': 0.16}\n",
      "{'loss': 0.6204, 'grad_norm': 2.1507568359375, 'learning_rate': 0.00019992127221406275, 'epoch': 0.17}\n",
      "{'loss': 0.7113, 'grad_norm': 2.1086537837982178, 'learning_rate': 0.00019989284781388617, 'epoch': 0.19}\n",
      "{'loss': 0.9614, 'grad_norm': 1.998403787612915, 'learning_rate': 0.00019986005377693825, 'epoch': 0.2}\n",
      "{'loss': 0.7855, 'grad_norm': 2.5904788970947266, 'learning_rate': 0.00019982289153773646, 'epoch': 0.21}\n",
      "{'loss': 0.7592, 'grad_norm': 1.9115339517593384, 'learning_rate': 0.00019978136272187747, 'epoch': 0.22}\n",
      "{'loss': 0.6717, 'grad_norm': 1.498032808303833, 'learning_rate': 0.00019973546914596623, 'epoch': 0.23}\n",
      "{'loss': 0.6312, 'grad_norm': 1.3255797624588013, 'learning_rate': 0.00019968521281753642, 'epoch': 0.24}\n",
      "{'loss': 0.5102, 'grad_norm': 0.9502735733985901, 'learning_rate': 0.00019963059593496268, 'epoch': 0.25}\n",
      "{'loss': 0.5355, 'grad_norm': 1.220511794090271, 'learning_rate': 0.0001995716208873644, 'epoch': 0.26}\n",
      "{'loss': 0.7439, 'grad_norm': 1.5249912738800049, 'learning_rate': 0.00019950829025450114, 'epoch': 0.27}\n",
      "{'loss': 0.4309, 'grad_norm': 1.076276421546936, 'learning_rate': 0.00019944060680666002, 'epoch': 0.28}\n",
      "{'loss': 0.7547, 'grad_norm': 1.4480377435684204, 'learning_rate': 0.0001993685735045343, 'epoch': 0.29}\n",
      "{'loss': 0.5342, 'grad_norm': 1.1861999034881592, 'learning_rate': 0.00019929219349909392, 'epoch': 0.3}\n",
      "{'loss': 0.4548, 'grad_norm': 1.7184381484985352, 'learning_rate': 0.0001992114701314478, 'epoch': 0.31}\n",
      "{'loss': 0.6445, 'grad_norm': 1.6771458387374878, 'learning_rate': 0.00019912640693269752, 'epoch': 0.32}\n",
      "{'loss': 0.9937, 'grad_norm': 1.659835934638977, 'learning_rate': 0.000199037007623783, 'epoch': 0.33}\n",
      "{'loss': 0.7161, 'grad_norm': 1.9406296014785767, 'learning_rate': 0.0001989432761153196, 'epoch': 0.34}\n",
      "{'loss': 0.411, 'grad_norm': 0.8666918873786926, 'learning_rate': 0.00019884521650742715, 'epoch': 0.35}\n",
      "{'loss': 0.4295, 'grad_norm': 1.3486360311508179, 'learning_rate': 0.00019874283308955057, 'epoch': 0.36}\n",
      "{'loss': 0.4223, 'grad_norm': 1.1567261219024658, 'learning_rate': 0.00019863613034027224, 'epoch': 0.37}\n",
      "{'loss': 0.3008, 'grad_norm': 0.7932368516921997, 'learning_rate': 0.00019852511292711608, 'epoch': 0.38}\n",
      "{'loss': 0.5795, 'grad_norm': 1.1085212230682373, 'learning_rate': 0.0001984097857063434, 'epoch': 0.39}\n",
      "{'loss': 1.5405, 'grad_norm': 1.562532663345337, 'learning_rate': 0.00019829015372274038, 'epoch': 0.4}\n",
      "{'loss': 1.1756, 'grad_norm': 1.3631798028945923, 'learning_rate': 0.0001981662222093976, 'epoch': 0.41}\n",
      "{'loss': 0.4143, 'grad_norm': 1.779801368713379, 'learning_rate': 0.00019803799658748094, 'epoch': 0.42}\n",
      "{'loss': 0.7125, 'grad_norm': 1.3960247039794922, 'learning_rate': 0.00019790548246599447, 'epoch': 0.43}\n",
      "{'loss': 0.6782, 'grad_norm': 2.0210886001586914, 'learning_rate': 0.00019776868564153516, 'epoch': 0.44}\n",
      "{'loss': 0.3493, 'grad_norm': 1.4578542709350586, 'learning_rate': 0.00019762761209803927, 'epoch': 0.45}\n",
      "{'loss': 0.3467, 'grad_norm': 1.2649827003479004, 'learning_rate': 0.0001974822680065206, 'epoch': 0.46}\n",
      "{'loss': 0.533, 'grad_norm': 1.037786602973938, 'learning_rate': 0.0001973326597248006, 'epoch': 0.47}\n",
      "{'loss': 0.3951, 'grad_norm': 1.4457162618637085, 'learning_rate': 0.00019717879379723012, 'epoch': 0.48}\n",
      "{'loss': 0.4595, 'grad_norm': 1.1638484001159668, 'learning_rate': 0.00019702067695440332, 'epoch': 0.49}\n",
      "{'loss': 0.3414, 'grad_norm': 0.9437578320503235, 'learning_rate': 0.0001968583161128631, 'epoch': 0.5}\n",
      " 10%|████                                    | 49/485 [13:37<1:54:27, 15.75s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:08,  1.58it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:02<00:10,  1.28it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:03<00:10,  1.18it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:04<00:10,  1.10it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:05<00:09,  1.08it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:06<00:08,  1.07it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:07<00:07,  1.06it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:08<00:06,  1.03it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:09<00:05,  1.04it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:10<00:04,  1.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:11<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:12<00:02,  1.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10461201518774033, 'eval_runtime': 15.9425, 'eval_samples_per_second': 2.007, 'eval_steps_per_second': 1.004, 'epoch': 0.5}\n",
      " 10%|████                                    | 49/485 [13:53<1:54:27, 15.75s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 0.452, 'grad_norm': 0.9797642230987549, 'learning_rate': 0.00019669171837479873, 'epoch': 0.51}\n",
      "{'loss': 0.4619, 'grad_norm': 4.578958511352539, 'learning_rate': 0.00019652089102773488, 'epoch': 0.52}\n",
      "{'loss': 0.4264, 'grad_norm': 0.9561588764190674, 'learning_rate': 0.00019634584154421317, 'epoch': 0.53}\n",
      "{'loss': 0.2043, 'grad_norm': 0.7727487087249756, 'learning_rate': 0.00019616657758146503, 'epoch': 0.54}\n",
      "{'loss': 0.2783, 'grad_norm': 0.90667724609375, 'learning_rate': 0.00019598310698107702, 'epoch': 0.56}\n",
      "{'loss': 0.4454, 'grad_norm': 1.1537208557128906, 'learning_rate': 0.0001957954377686475, 'epoch': 0.57}\n",
      "{'loss': 0.4183, 'grad_norm': 1.0712287425994873, 'learning_rate': 0.00019560357815343577, 'epoch': 0.58}\n",
      "{'loss': 0.4924, 'grad_norm': 1.2568341493606567, 'learning_rate': 0.000195407536528003, 'epoch': 0.59}\n",
      "{'loss': 0.3534, 'grad_norm': 1.0989218950271606, 'learning_rate': 0.00019520732146784491, 'epoch': 0.6}\n",
      "{'loss': 0.3499, 'grad_norm': 0.8873760104179382, 'learning_rate': 0.00019500294173101687, 'epoch': 0.61}\n",
      "{'loss': 0.4831, 'grad_norm': 1.032456636428833, 'learning_rate': 0.0001947944062577507, 'epoch': 0.62}\n",
      "{'loss': 0.5024, 'grad_norm': 0.8906235098838806, 'learning_rate': 0.00019458172417006347, 'epoch': 0.63}\n",
      "{'loss': 0.3643, 'grad_norm': 1.0869104862213135, 'learning_rate': 0.00019436490477135878, 'epoch': 0.64}\n",
      "{'loss': 0.2983, 'grad_norm': 1.2367178201675415, 'learning_rate': 0.00019414395754601947, 'epoch': 0.65}\n",
      "{'loss': 0.3135, 'grad_norm': 0.9168172478675842, 'learning_rate': 0.00019391889215899299, 'epoch': 0.66}\n",
      "{'loss': 0.4805, 'grad_norm': 1.0701121091842651, 'learning_rate': 0.00019368971845536845, 'epoch': 0.67}\n",
      "{'loss': 0.3419, 'grad_norm': 1.1722352504730225, 'learning_rate': 0.0001934564464599461, 'epoch': 0.68}\n",
      "{'loss': 0.349, 'grad_norm': 0.8336866497993469, 'learning_rate': 0.00019321908637679865, 'epoch': 0.69}\n",
      "{'loss': 0.6736, 'grad_norm': 1.5599061250686646, 'learning_rate': 0.00019297764858882514, 'epoch': 0.7}\n",
      "{'loss': 0.3633, 'grad_norm': 0.8714211583137512, 'learning_rate': 0.00019273214365729655, 'epoch': 0.71}\n",
      "{'loss': 0.2398, 'grad_norm': 0.6903231739997864, 'learning_rate': 0.00019248258232139388, 'epoch': 0.72}\n",
      "{'loss': 0.4932, 'grad_norm': 1.1108914613723755, 'learning_rate': 0.00019222897549773848, 'epoch': 0.73}\n",
      "{'loss': 0.5158, 'grad_norm': 1.0192766189575195, 'learning_rate': 0.00019197133427991436, 'epoch': 0.74}\n",
      "{'loss': 0.1856, 'grad_norm': 0.672895073890686, 'learning_rate': 0.000191709669937983, 'epoch': 0.75}\n",
      "{'loss': 0.3452, 'grad_norm': 1.1619887351989746, 'learning_rate': 0.00019144399391799043, 'epoch': 0.76}\n",
      "{'loss': 0.3759, 'grad_norm': 1.0525503158569336, 'learning_rate': 0.00019117431784146645, 'epoch': 0.77}\n",
      "{'loss': 0.2496, 'grad_norm': 0.8944844007492065, 'learning_rate': 0.00019090065350491626, 'epoch': 0.78}\n",
      "{'loss': 0.2633, 'grad_norm': 0.6257263422012329, 'learning_rate': 0.00019062301287930446, 'epoch': 0.79}\n",
      "{'loss': 0.5347, 'grad_norm': 1.5018484592437744, 'learning_rate': 0.0001903414081095315, 'epoch': 0.8}\n",
      "{'loss': 0.3946, 'grad_norm': 1.1880418062210083, 'learning_rate': 0.00019005585151390223, 'epoch': 0.81}\n",
      "{'loss': 0.4179, 'grad_norm': 0.9627914428710938, 'learning_rate': 0.00018976635558358722, 'epoch': 0.82}\n",
      "{'loss': 0.3014, 'grad_norm': 0.872073769569397, 'learning_rate': 0.00018947293298207635, 'epoch': 0.83}\n",
      "{'loss': 0.2432, 'grad_norm': 0.7994993925094604, 'learning_rate': 0.00018917559654462474, 'epoch': 0.84}\n",
      "{'loss': 0.2766, 'grad_norm': 0.7398061752319336, 'learning_rate': 0.00018887435927769137, 'epoch': 0.85}\n",
      "{'loss': 0.3346, 'grad_norm': 0.8402171730995178, 'learning_rate': 0.00018856923435837022, 'epoch': 0.86}\n",
      "{'loss': 0.6122, 'grad_norm': 1.781782627105713, 'learning_rate': 0.0001882602351338137, 'epoch': 0.87}\n",
      "{'loss': 0.3551, 'grad_norm': 0.8645707368850708, 'learning_rate': 0.0001879473751206489, 'epoch': 0.88}\n",
      "{'loss': 0.3953, 'grad_norm': 0.904132068157196, 'learning_rate': 0.00018763066800438636, 'epoch': 0.89}\n",
      "{'loss': 0.2305, 'grad_norm': 1.055055022239685, 'learning_rate': 0.00018731012763882133, 'epoch': 0.9}\n",
      "{'loss': 0.4958, 'grad_norm': 1.1122102737426758, 'learning_rate': 0.00018698576804542777, 'epoch': 0.92}\n",
      "{'loss': 0.2213, 'grad_norm': 0.7686111927032471, 'learning_rate': 0.00018665760341274505, 'epoch': 0.93}\n",
      "{'loss': 0.2116, 'grad_norm': 0.7528559565544128, 'learning_rate': 0.00018632564809575742, 'epoch': 0.94}\n",
      "{'loss': 0.4887, 'grad_norm': 1.013811707496643, 'learning_rate': 0.00018598991661526572, 'epoch': 0.95}\n",
      "{'loss': 0.4472, 'grad_norm': 1.310968279838562, 'learning_rate': 0.00018565042365725258, 'epoch': 0.96}\n",
      "{'loss': 0.6229, 'grad_norm': 1.1753458976745605, 'learning_rate': 0.00018530718407223974, 'epoch': 0.97}\n",
      "{'loss': 0.3354, 'grad_norm': 0.8278579711914062, 'learning_rate': 0.0001849602128746387, 'epoch': 0.98}\n",
      "{'loss': 0.2807, 'grad_norm': 1.7592978477478027, 'learning_rate': 0.00018460952524209355, 'epoch': 0.99}\n",
      "{'loss': 0.1658, 'grad_norm': 0.6590973138809204, 'learning_rate': 0.00018425513651481747, 'epoch': 1.0}\n",
      "{'loss': 0.3154, 'grad_norm': 0.6885104179382324, 'learning_rate': 0.00018389706219492147, 'epoch': 1.0}\n",
      " 20%|████████                                | 98/485 [26:47<1:22:56, 12.86s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.01it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:08<00:07,  1.01s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.01it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.03it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.01it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.08472086489200592, 'eval_runtime': 15.7962, 'eval_samples_per_second': 2.026, 'eval_steps_per_second': 1.013, 'epoch': 1.0}\n",
      " 20%|████████                                | 98/485 [27:03<1:22:56, 12.86s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 0.4895, 'grad_norm': 0.8268596529960632, 'learning_rate': 0.00018353531794573625, 'epoch': 1.01}\n",
      "{'loss': 0.9141, 'grad_norm': 0.8074506521224976, 'learning_rate': 0.00018316991959112716, 'epoch': 1.02}\n",
      "{'loss': 0.3511, 'grad_norm': 0.5625840425491333, 'learning_rate': 0.00018280088311480201, 'epoch': 1.03}\n",
      "{'loss': 1.0785, 'grad_norm': 0.7673054933547974, 'learning_rate': 0.00018242822465961176, 'epoch': 1.04}\n",
      "{'loss': 0.8068, 'grad_norm': 0.8528258800506592, 'learning_rate': 0.00018205196052684445, 'epoch': 1.05}\n",
      "{'loss': 1.1578, 'grad_norm': 0.9540959596633911, 'learning_rate': 0.00018167210717551224, 'epoch': 1.06}\n",
      "{'loss': 0.3269, 'grad_norm': 0.9940904974937439, 'learning_rate': 0.00018128868122163123, 'epoch': 1.07}\n",
      "{'loss': 0.2876, 'grad_norm': 0.756807804107666, 'learning_rate': 0.00018090169943749476, 'epoch': 1.08}\n",
      "{'loss': 0.5107, 'grad_norm': 0.5623844861984253, 'learning_rate': 0.00018051117875093976, 'epoch': 1.09}\n",
      "{'loss': 0.9304, 'grad_norm': 0.8876741528511047, 'learning_rate': 0.00018011713624460608, 'epoch': 1.1}\n",
      "{'loss': 0.2594, 'grad_norm': 0.7365004420280457, 'learning_rate': 0.0001797195891551896, 'epoch': 1.11}\n",
      "{'loss': 0.3014, 'grad_norm': 0.7800759077072144, 'learning_rate': 0.00017931855487268782, 'epoch': 1.12}\n",
      "{'loss': 0.3529, 'grad_norm': 0.812404215335846, 'learning_rate': 0.00017891405093963938, 'epoch': 1.13}\n",
      "{'loss': 0.1862, 'grad_norm': 1.10379958152771, 'learning_rate': 0.0001785060950503568, 'epoch': 1.14}\n",
      "{'loss': 0.2329, 'grad_norm': 0.7519966959953308, 'learning_rate': 0.0001780947050501522, 'epoch': 1.15}\n",
      "{'loss': 0.2404, 'grad_norm': 0.8714112639427185, 'learning_rate': 0.00017767989893455698, 'epoch': 1.16}\n",
      "{'loss': 0.2716, 'grad_norm': 0.8532187938690186, 'learning_rate': 0.00017726169484853438, 'epoch': 1.17}\n",
      "{'loss': 0.2582, 'grad_norm': 1.034058928489685, 'learning_rate': 0.00017684011108568592, 'epoch': 1.19}\n",
      "{'loss': 0.2839, 'grad_norm': 0.8002893924713135, 'learning_rate': 0.00017641516608745114, 'epoch': 1.2}\n",
      "{'loss': 0.2107, 'grad_norm': 0.9473869204521179, 'learning_rate': 0.00017598687844230088, 'epoch': 1.21}\n",
      "{'loss': 0.1229, 'grad_norm': 0.7028695344924927, 'learning_rate': 0.0001755552668849242, 'epoch': 1.22}\n",
      "{'loss': 0.2775, 'grad_norm': 0.9099118113517761, 'learning_rate': 0.00017512035029540885, 'epoch': 1.23}\n",
      "{'loss': 0.14, 'grad_norm': 0.49095818400382996, 'learning_rate': 0.0001746821476984154, 'epoch': 1.24}\n",
      "{'loss': 0.2739, 'grad_norm': 0.7503119707107544, 'learning_rate': 0.000174240678262345, 'epoch': 1.25}\n",
      "{'loss': 0.136, 'grad_norm': 0.7837440967559814, 'learning_rate': 0.00017379596129850098, 'epoch': 1.26}\n",
      "{'loss': 0.5619, 'grad_norm': 0.9614096879959106, 'learning_rate': 0.000173348016260244, 'epoch': 1.27}\n",
      "{'loss': 0.4409, 'grad_norm': 0.8304548859596252, 'learning_rate': 0.00017289686274214118, 'epoch': 1.28}\n",
      "{'loss': 0.1699, 'grad_norm': 0.5615501403808594, 'learning_rate': 0.00017244252047910892, 'epoch': 1.29}\n",
      "{'loss': 0.2929, 'grad_norm': 0.9695115685462952, 'learning_rate': 0.00017198500934554966, 'epoch': 1.3}\n",
      "{'loss': 0.1882, 'grad_norm': 0.60114985704422, 'learning_rate': 0.00017152434935448256, 'epoch': 1.31}\n",
      "{'loss': 0.2059, 'grad_norm': 0.8714609146118164, 'learning_rate': 0.00017106056065666793, 'epoch': 1.32}\n",
      "{'loss': 0.1869, 'grad_norm': 0.6488279104232788, 'learning_rate': 0.0001705936635397259, 'epoch': 1.33}\n",
      "{'loss': 0.1652, 'grad_norm': 0.6489734053611755, 'learning_rate': 0.00017012367842724887, 'epoch': 1.34}\n",
      "{'loss': 0.2113, 'grad_norm': 0.8251059651374817, 'learning_rate': 0.00016965062587790823, 'epoch': 1.35}\n",
      "{'loss': 0.4268, 'grad_norm': 0.822625994682312, 'learning_rate': 0.00016917452658455495, 'epoch': 1.36}\n",
      "{'loss': 0.112, 'grad_norm': 0.6062455177307129, 'learning_rate': 0.00016869540137331445, 'epoch': 1.37}\n",
      "{'loss': 0.1537, 'grad_norm': 0.5778626799583435, 'learning_rate': 0.00016821327120267567, 'epoch': 1.38}\n",
      "{'loss': 0.3049, 'grad_norm': 0.729773759841919, 'learning_rate': 0.00016772815716257412, 'epoch': 1.39}\n",
      "{'loss': 0.1464, 'grad_norm': 0.6948752403259277, 'learning_rate': 0.00016724008047346947, 'epoch': 1.4}\n",
      "{'loss': 0.0893, 'grad_norm': 0.8039988279342651, 'learning_rate': 0.00016674906248541726, 'epoch': 1.41}\n",
      "{'loss': 0.1391, 'grad_norm': 0.7219467163085938, 'learning_rate': 0.000166255124677135, 'epoch': 1.42}\n",
      "{'loss': 0.2555, 'grad_norm': 0.7390284538269043, 'learning_rate': 0.00016575828865506245, 'epoch': 1.43}\n",
      "{'loss': 0.2949, 'grad_norm': 1.3266428709030151, 'learning_rate': 0.00016525857615241687, 'epoch': 1.44}\n",
      "{'loss': 0.2669, 'grad_norm': 1.0519911050796509, 'learning_rate': 0.0001647560090282419, 'epoch': 1.45}\n",
      "{'loss': 0.1794, 'grad_norm': 0.8303832411766052, 'learning_rate': 0.00016425060926645167, 'epoch': 1.46}\n",
      "{'loss': 0.129, 'grad_norm': 0.4832349419593811, 'learning_rate': 0.000163742398974869, 'epoch': 1.47}\n",
      "{'loss': 0.2471, 'grad_norm': 1.2860238552093506, 'learning_rate': 0.00016323140038425842, 'epoch': 1.48}\n",
      "{'loss': 0.3691, 'grad_norm': 1.1357101202011108, 'learning_rate': 0.0001627176358473537, 'epoch': 1.49}\n",
      "{'loss': 0.1759, 'grad_norm': 1.3550959825515747, 'learning_rate': 0.0001622011278378801, 'epoch': 1.5}\n",
      " 30%|███████████▊                           | 147/485 [40:42<1:30:56, 16.14s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.07it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.07097093760967255, 'eval_runtime': 15.6049, 'eval_samples_per_second': 2.051, 'eval_steps_per_second': 1.025, 'epoch': 1.5}\n",
      " 30%|███████████▊                           | 147/485 [40:58<1:30:56, 16.14s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 0.1597, 'grad_norm': 0.5505244135856628, 'learning_rate': 0.0001616818989495711, 'epoch': 1.51}\n",
      "{'loss': 0.3125, 'grad_norm': 0.8409085869789124, 'learning_rate': 0.00016115997189518043, 'epoch': 1.52}\n",
      "{'loss': 0.1271, 'grad_norm': 0.6431383490562439, 'learning_rate': 0.00016063536950548826, 'epoch': 1.53}\n",
      "{'loss': 0.2325, 'grad_norm': 0.897996187210083, 'learning_rate': 0.00016010811472830252, 'epoch': 1.54}\n",
      "{'loss': 0.134, 'grad_norm': 0.8617600798606873, 'learning_rate': 0.0001595782306274553, 'epoch': 1.56}\n",
      "{'loss': 0.1799, 'grad_norm': 0.9170189499855042, 'learning_rate': 0.0001590457403817937, 'epoch': 1.57}\n",
      "{'loss': 0.2333, 'grad_norm': 0.6426158547401428, 'learning_rate': 0.00015851066728416618, 'epoch': 1.58}\n",
      "{'loss': 0.223, 'grad_norm': 0.9257921576499939, 'learning_rate': 0.00015797303474040332, 'epoch': 1.59}\n",
      "{'loss': 0.2118, 'grad_norm': 0.8354256749153137, 'learning_rate': 0.00015743286626829437, 'epoch': 1.6}\n",
      "{'loss': 0.156, 'grad_norm': 0.5267432928085327, 'learning_rate': 0.00015689018549655813, 'epoch': 1.61}\n",
      "{'loss': 0.2008, 'grad_norm': 0.8225706815719604, 'learning_rate': 0.00015634501616380967, 'epoch': 1.62}\n",
      "{'loss': 0.4067, 'grad_norm': 1.0356972217559814, 'learning_rate': 0.00015579738211752165, 'epoch': 1.63}\n",
      "{'loss': 0.275, 'grad_norm': 0.8076729774475098, 'learning_rate': 0.00015524730731298134, 'epoch': 1.64}\n",
      "{'loss': 0.1997, 'grad_norm': 0.7149032950401306, 'learning_rate': 0.00015469481581224272, 'epoch': 1.65}\n",
      "{'loss': 0.1457, 'grad_norm': 0.8273406624794006, 'learning_rate': 0.0001541399317830738, 'epoch': 1.66}\n",
      "{'loss': 0.1673, 'grad_norm': 0.683661162853241, 'learning_rate': 0.00015358267949789966, 'epoch': 1.67}\n",
      "{'loss': 0.2385, 'grad_norm': 0.6166409254074097, 'learning_rate': 0.0001530230833327405, 'epoch': 1.68}\n",
      "{'loss': 0.1233, 'grad_norm': 0.5448890924453735, 'learning_rate': 0.00015246116776614538, 'epoch': 1.69}\n",
      "{'loss': 0.4401, 'grad_norm': 1.2916579246520996, 'learning_rate': 0.00015189695737812152, 'epoch': 1.7}\n",
      "{'loss': 0.2703, 'grad_norm': 1.0966475009918213, 'learning_rate': 0.00015133047684905916, 'epoch': 1.71}\n",
      "{'loss': 0.2091, 'grad_norm': 0.6722729802131653, 'learning_rate': 0.0001507617509586517, 'epoch': 1.72}\n",
      "{'loss': 0.1634, 'grad_norm': 0.6230800151824951, 'learning_rate': 0.00015019080458481202, 'epoch': 1.73}\n",
      "{'loss': 0.394, 'grad_norm': 0.9288224577903748, 'learning_rate': 0.00014961766270258422, 'epoch': 1.74}\n",
      "{'loss': 0.1151, 'grad_norm': 0.6821256875991821, 'learning_rate': 0.00014904235038305083, 'epoch': 1.75}\n",
      "{'loss': 0.2549, 'grad_norm': 0.8734120726585388, 'learning_rate': 0.00014846489279223652, 'epoch': 1.76}\n",
      "{'loss': 0.2609, 'grad_norm': 0.7155976891517639, 'learning_rate': 0.00014788531519000696, 'epoch': 1.77}\n",
      "{'loss': 0.1786, 'grad_norm': 1.0202449560165405, 'learning_rate': 0.0001473036429289641, 'epoch': 1.78}\n",
      "{'loss': 0.1772, 'grad_norm': 0.8442009687423706, 'learning_rate': 0.00014671990145333696, 'epoch': 1.79}\n",
      "{'loss': 0.1009, 'grad_norm': 0.9477596879005432, 'learning_rate': 0.0001461341162978688, 'epoch': 1.8}\n",
      "{'loss': 0.3635, 'grad_norm': 1.200688123703003, 'learning_rate': 0.00014554631308669994, 'epoch': 1.81}\n",
      "{'loss': 0.3204, 'grad_norm': 0.7340750098228455, 'learning_rate': 0.00014495651753224705, 'epoch': 1.82}\n",
      "{'loss': 0.2667, 'grad_norm': 0.891011118888855, 'learning_rate': 0.00014436475543407843, 'epoch': 1.83}\n",
      "{'loss': 0.7496, 'grad_norm': 1.1334666013717651, 'learning_rate': 0.00014377105267778518, 'epoch': 1.84}\n",
      "{'loss': 0.2349, 'grad_norm': 1.6704403162002563, 'learning_rate': 0.00014317543523384928, 'epoch': 1.85}\n",
      "{'loss': 0.3457, 'grad_norm': 0.889671802520752, 'learning_rate': 0.00014257792915650728, 'epoch': 1.86}\n",
      "{'loss': 0.1136, 'grad_norm': 0.998199999332428, 'learning_rate': 0.0001419785605826106, 'epoch': 1.87}\n",
      "{'loss': 0.334, 'grad_norm': 1.0662951469421387, 'learning_rate': 0.00014137735573048233, 'epoch': 1.88}\n",
      "{'loss': 0.2815, 'grad_norm': 1.171033263206482, 'learning_rate': 0.00014077434089877037, 'epoch': 1.89}\n",
      "{'loss': 0.1882, 'grad_norm': 0.6846770644187927, 'learning_rate': 0.00014016954246529696, 'epoch': 1.9}\n",
      "{'loss': 0.5558, 'grad_norm': 1.1940083503723145, 'learning_rate': 0.00013956298688590484, 'epoch': 1.92}\n",
      "{'loss': 0.3532, 'grad_norm': 0.950919508934021, 'learning_rate': 0.00013895470069330004, 'epoch': 1.93}\n",
      "{'loss': 0.3541, 'grad_norm': 0.7408319115638733, 'learning_rate': 0.00013834471049589117, 'epoch': 1.94}\n",
      "{'loss': 0.2075, 'grad_norm': 0.6098282337188721, 'learning_rate': 0.00013773304297662559, 'epoch': 1.95}\n",
      "{'loss': 0.1977, 'grad_norm': 1.2971456050872803, 'learning_rate': 0.00013711972489182208, 'epoch': 1.96}\n",
      "{'loss': 0.1516, 'grad_norm': 0.6534513831138611, 'learning_rate': 0.00013650478307000057, 'epoch': 1.97}\n",
      "{'loss': 0.1345, 'grad_norm': 0.7555717825889587, 'learning_rate': 0.00013588824441070852, 'epoch': 1.98}\n",
      "{'loss': 0.2376, 'grad_norm': 1.0246295928955078, 'learning_rate': 0.00013527013588334415, 'epoch': 1.99}\n",
      "{'loss': 0.3643, 'grad_norm': 1.4333122968673706, 'learning_rate': 0.00013465048452597682, 'epoch': 2.0}\n",
      "{'loss': 0.0125, 'grad_norm': 0.20764005184173584, 'learning_rate': 0.00013402931744416433, 'epoch': 2.0}\n",
      " 40%|███████████████▊                       | 196/485 [53:48<1:00:17, 12.52s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:06<00:09,  1.03s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:07<00:08,  1.11s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:08<00:07,  1.12s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:09<00:06,  1.07s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:10<00:05,  1.03s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:11<00:04,  1.01s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:12<00:03,  1.01s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:13<00:01,  1.01it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:14<00:00,  1.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06497978419065475, 'eval_runtime': 16.3857, 'eval_samples_per_second': 1.953, 'eval_steps_per_second': 0.976, 'epoch': 2.0}\n",
      " 40%|███████████████▊                       | 196/485 [54:04<1:00:17, 12.52s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:15<00:00,  1.03it/s]\u001b[A\n",
      "{'loss': 0.696, 'grad_norm': 0.6296555399894714, 'learning_rate': 0.00013340666180976712, 'epoch': 2.01}\n",
      "{'loss': 0.832, 'grad_norm': 0.8036012649536133, 'learning_rate': 0.00013278254485975976, 'epoch': 2.02}\n",
      "{'loss': 0.4004, 'grad_norm': 0.5084363222122192, 'learning_rate': 0.00013215699389503954, 'epoch': 2.03}\n",
      "{'loss': 0.5884, 'grad_norm': 0.7812409996986389, 'learning_rate': 0.00013153003627923218, 'epoch': 2.04}\n",
      "{'loss': 0.4767, 'grad_norm': 0.6228088140487671, 'learning_rate': 0.00013090169943749476, 'epoch': 2.05}\n",
      "{'loss': 0.1609, 'grad_norm': 0.46504464745521545, 'learning_rate': 0.00013027201085531634, 'epoch': 2.06}\n",
      "{'loss': 0.2898, 'grad_norm': 0.8527867794036865, 'learning_rate': 0.0001296409980773154, 'epoch': 2.07}\n",
      "{'loss': 0.3345, 'grad_norm': 0.6818323135375977, 'learning_rate': 0.00012900868870603503, 'epoch': 2.08}\n",
      "{'loss': 0.481, 'grad_norm': 0.6970481276512146, 'learning_rate': 0.0001283751104007355, 'epoch': 2.09}\n",
      "{'loss': 0.131, 'grad_norm': 0.46434861421585083, 'learning_rate': 0.00012774029087618446, 'epoch': 2.1}\n",
      "{'loss': 0.126, 'grad_norm': 0.5762383341789246, 'learning_rate': 0.00012710425790144446, 'epoch': 2.11}\n",
      "{'loss': 0.0391, 'grad_norm': 0.2466854304075241, 'learning_rate': 0.00012646703929865817, 'epoch': 2.12}\n",
      "{'loss': 0.0873, 'grad_norm': 0.5206692814826965, 'learning_rate': 0.00012582866294183167, 'epoch': 2.13}\n",
      "{'loss': 0.2263, 'grad_norm': 1.350303053855896, 'learning_rate': 0.00012518915675561483, 'epoch': 2.14}\n",
      "{'loss': 0.1153, 'grad_norm': 0.44758525490760803, 'learning_rate': 0.00012454854871407994, 'epoch': 2.15}\n",
      "{'loss': 0.0691, 'grad_norm': 0.4361209273338318, 'learning_rate': 0.00012390686683949798, 'epoch': 2.16}\n",
      "{'loss': 0.1958, 'grad_norm': 0.49332988262176514, 'learning_rate': 0.00012326413920111303, 'epoch': 2.17}\n",
      "{'loss': 0.0845, 'grad_norm': 0.5533666610717773, 'learning_rate': 0.00012262039391391404, 'epoch': 2.19}\n",
      "{'loss': 0.1138, 'grad_norm': 0.8445172309875488, 'learning_rate': 0.00012197565913740531, 'epoch': 2.2}\n",
      "{'loss': 0.0572, 'grad_norm': 0.37994733452796936, 'learning_rate': 0.0001213299630743747, 'epoch': 2.21}\n",
      "{'loss': 0.1438, 'grad_norm': 0.5656107068061829, 'learning_rate': 0.00012068333396965968, 'epoch': 2.22}\n",
      "{'loss': 0.0714, 'grad_norm': 0.3995652496814728, 'learning_rate': 0.00012003580010891213, 'epoch': 2.23}\n",
      "{'loss': 0.2074, 'grad_norm': 0.8036729097366333, 'learning_rate': 0.00011938738981736085, 'epoch': 2.24}\n",
      "{'loss': 0.1296, 'grad_norm': 0.6102587580680847, 'learning_rate': 0.00011873813145857249, 'epoch': 2.25}\n",
      "{'loss': 0.1978, 'grad_norm': 1.0642577409744263, 'learning_rate': 0.000118088053433211, 'epoch': 2.26}\n",
      "{'loss': 0.0477, 'grad_norm': 0.31857892870903015, 'learning_rate': 0.00011743718417779517, 'epoch': 2.27}\n",
      "{'loss': 0.1059, 'grad_norm': 0.5356038808822632, 'learning_rate': 0.00011678555216345477, 'epoch': 2.28}\n",
      "{'loss': 0.1332, 'grad_norm': 0.6670119762420654, 'learning_rate': 0.00011613318589468511, 'epoch': 2.29}\n",
      "{'loss': 0.381, 'grad_norm': 2.81925892829895, 'learning_rate': 0.00011548011390810017, 'epoch': 2.3}\n",
      "{'loss': 0.1127, 'grad_norm': 0.5751513242721558, 'learning_rate': 0.0001148263647711842, 'epoch': 2.31}\n",
      "{'loss': 0.1639, 'grad_norm': 0.8042580485343933, 'learning_rate': 0.00011417196708104243, 'epoch': 2.32}\n",
      "{'loss': 0.1327, 'grad_norm': 0.5074213147163391, 'learning_rate': 0.0001135169494631497, 'epoch': 2.33}\n",
      "{'loss': 0.0923, 'grad_norm': 0.46423041820526123, 'learning_rate': 0.00011286134057009863, 'epoch': 2.34}\n",
      "{'loss': 0.1434, 'grad_norm': 0.5287284255027771, 'learning_rate': 0.00011220516908034601, 'epoch': 2.35}\n",
      "{'loss': 0.4498, 'grad_norm': 0.6540396809577942, 'learning_rate': 0.00011154846369695863, 'epoch': 2.36}\n",
      "{'loss': 0.2227, 'grad_norm': 0.6332671046257019, 'learning_rate': 0.00011089125314635726, 'epoch': 2.37}\n",
      "{'loss': 0.1265, 'grad_norm': 0.7643010020256042, 'learning_rate': 0.00011023356617706052, 'epoch': 2.38}\n",
      "{'loss': 0.064, 'grad_norm': 0.3640890121459961, 'learning_rate': 0.00010957543155842702, 'epoch': 2.39}\n",
      "{'loss': 0.1629, 'grad_norm': 0.8187054991722107, 'learning_rate': 0.00010891687807939707, 'epoch': 2.4}\n",
      "{'loss': 0.091, 'grad_norm': 0.615903377532959, 'learning_rate': 0.00010825793454723325, 'epoch': 2.41}\n",
      "{'loss': 0.1308, 'grad_norm': 1.2710505723953247, 'learning_rate': 0.00010759862978626031, 'epoch': 2.42}\n",
      "{'loss': 0.1189, 'grad_norm': 0.5557311177253723, 'learning_rate': 0.00010693899263660441, 'epoch': 2.43}\n",
      "{'loss': 0.3972, 'grad_norm': 0.7145495414733887, 'learning_rate': 0.00010627905195293135, 'epoch': 2.44}\n",
      "{'loss': 0.2562, 'grad_norm': 0.8202933669090271, 'learning_rate': 0.00010561883660318455, 'epoch': 2.45}\n",
      "{'loss': 0.1028, 'grad_norm': 0.5381209850311279, 'learning_rate': 0.00010495837546732224, 'epoch': 2.46}\n",
      "{'loss': 0.1419, 'grad_norm': 0.7134390473365784, 'learning_rate': 0.00010429769743605407, 'epoch': 2.47}\n",
      "{'loss': 0.1357, 'grad_norm': 0.8154945373535156, 'learning_rate': 0.00010363683140957745, 'epoch': 2.48}\n",
      "{'loss': 0.1891, 'grad_norm': 0.6476486921310425, 'learning_rate': 0.00010297580629631325, 'epoch': 2.49}\n",
      "{'loss': 0.1726, 'grad_norm': 0.7559356093406677, 'learning_rate': 0.00010231465101164139, 'epoch': 2.5}\n",
      " 51%|██████████████████▋                  | 245/485 [1:07:58<1:03:46, 15.94s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:09,  1.42it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.25it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.14it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:09,  1.11it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.08it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.07it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:02,  1.06s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:14<00:01,  1.10s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06407293677330017, 'eval_runtime': 16.1857, 'eval_samples_per_second': 1.977, 'eval_steps_per_second': 0.989, 'epoch': 2.5}\n",
      " 51%|██████████████████▋                  | 245/485 [1:08:14<1:03:46, 15.94s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:15<00:00,  1.05s/it]\u001b[A\n",
      "{'loss': 0.2113, 'grad_norm': 0.5067595839500427, 'learning_rate': 0.00010165339447663587, 'epoch': 2.51}\n",
      "{'loss': 0.1435, 'grad_norm': 0.5364310145378113, 'learning_rate': 0.00010099206561679963, 'epoch': 2.52}\n",
      "{'loss': 0.1187, 'grad_norm': 0.9448193907737732, 'learning_rate': 0.00010033069336079952, 'epoch': 2.53}\n",
      "{'loss': 0.2103, 'grad_norm': 0.6672095060348511, 'learning_rate': 9.966930663920049e-05, 'epoch': 2.54}\n",
      "{'loss': 0.147, 'grad_norm': 0.8146255016326904, 'learning_rate': 9.900793438320037e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0952, 'grad_norm': 0.451083242893219, 'learning_rate': 9.834660552336415e-05, 'epoch': 2.57}\n",
      "{'loss': 0.0968, 'grad_norm': 0.7484776377677917, 'learning_rate': 9.768534898835862e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0623, 'grad_norm': 0.46455663442611694, 'learning_rate': 9.702419370368676e-05, 'epoch': 2.59}\n",
      "{'loss': 0.1165, 'grad_norm': 0.633581817150116, 'learning_rate': 9.636316859042259e-05, 'epoch': 2.6}\n",
      "{'loss': 0.1646, 'grad_norm': 0.38784340023994446, 'learning_rate': 9.570230256394596e-05, 'epoch': 2.61}\n",
      "{'loss': 0.0512, 'grad_norm': 0.4448390305042267, 'learning_rate': 9.504162453267777e-05, 'epoch': 2.62}\n",
      "{'loss': 0.0962, 'grad_norm': 0.6329368948936462, 'learning_rate': 9.438116339681545e-05, 'epoch': 2.63}\n",
      "{'loss': 0.184, 'grad_norm': 1.471470832824707, 'learning_rate': 9.372094804706867e-05, 'epoch': 2.64}\n",
      "{'loss': 0.0591, 'grad_norm': 0.47350382804870605, 'learning_rate': 9.30610073633956e-05, 'epoch': 2.65}\n",
      "{'loss': 0.0491, 'grad_norm': 0.5136503577232361, 'learning_rate': 9.24013702137397e-05, 'epoch': 2.66}\n",
      "{'loss': 0.1244, 'grad_norm': 0.6159942150115967, 'learning_rate': 9.174206545276677e-05, 'epoch': 2.67}\n",
      "{'loss': 0.1471, 'grad_norm': 1.1355847120285034, 'learning_rate': 9.108312192060298e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0389, 'grad_norm': 0.3200993537902832, 'learning_rate': 9.042456844157299e-05, 'epoch': 2.69}\n",
      "{'loss': 0.1579, 'grad_norm': 0.585429310798645, 'learning_rate': 8.97664338229395e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1148, 'grad_norm': 1.944648265838623, 'learning_rate': 8.910874685364275e-05, 'epoch': 2.71}\n",
      "{'loss': 0.0898, 'grad_norm': 0.9831622838973999, 'learning_rate': 8.845153630304139e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0563, 'grad_norm': 0.8243109583854675, 'learning_rate': 8.7794830919654e-05, 'epoch': 2.73}\n",
      "{'loss': 0.235, 'grad_norm': 0.6760785579681396, 'learning_rate': 8.713865942990141e-05, 'epoch': 2.74}\n",
      "{'loss': 0.1421, 'grad_norm': 1.0040335655212402, 'learning_rate': 8.648305053685034e-05, 'epoch': 2.75}\n",
      "{'loss': 0.0718, 'grad_norm': 0.4481758177280426, 'learning_rate': 8.582803291895758e-05, 'epoch': 2.76}\n",
      "{'loss': 0.2467, 'grad_norm': 1.4958864450454712, 'learning_rate': 8.517363522881579e-05, 'epoch': 2.77}\n",
      "{'loss': 0.2126, 'grad_norm': 1.2638881206512451, 'learning_rate': 8.451988609189987e-05, 'epoch': 2.78}\n",
      "{'loss': 0.0622, 'grad_norm': 0.5248256325721741, 'learning_rate': 8.386681410531491e-05, 'epoch': 2.79}\n",
      "{'loss': 0.1683, 'grad_norm': 0.7041281461715698, 'learning_rate': 8.321444783654524e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0927, 'grad_norm': 0.5890410542488098, 'learning_rate': 8.256281582220485e-05, 'epoch': 2.81}\n",
      "{'loss': 0.0897, 'grad_norm': 0.7033946514129639, 'learning_rate': 8.191194656678904e-05, 'epoch': 2.82}\n",
      "{'loss': 0.0562, 'grad_norm': 0.47669529914855957, 'learning_rate': 8.126186854142752e-05, 'epoch': 2.83}\n",
      "{'loss': 0.1993, 'grad_norm': 0.7833516597747803, 'learning_rate': 8.061261018263919e-05, 'epoch': 2.84}\n",
      "{'loss': 0.2766, 'grad_norm': 1.43299400806427, 'learning_rate': 7.996419989108789e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0717, 'grad_norm': 0.5926647186279297, 'learning_rate': 7.931666603034033e-05, 'epoch': 2.86}\n",
      "{'loss': 0.0713, 'grad_norm': 0.7453040480613708, 'learning_rate': 7.867003692562534e-05, 'epoch': 2.87}\n",
      "{'loss': 0.1554, 'grad_norm': 0.5819457769393921, 'learning_rate': 7.80243408625947e-05, 'epoch': 2.88}\n",
      "{'loss': 0.1736, 'grad_norm': 0.829064667224884, 'learning_rate': 7.7379606086086e-05, 'epoch': 2.89}\n",
      "{'loss': 0.1368, 'grad_norm': 0.6623737215995789, 'learning_rate': 7.673586079888698e-05, 'epoch': 2.9}\n",
      "{'loss': 0.158, 'grad_norm': 0.7724748253822327, 'learning_rate': 7.6093133160502e-05, 'epoch': 2.92}\n",
      "{'loss': 0.1167, 'grad_norm': 0.6388201713562012, 'learning_rate': 7.54514512859201e-05, 'epoch': 2.93}\n",
      "{'loss': 0.05, 'grad_norm': 0.36923161149024963, 'learning_rate': 7.48108432443852e-05, 'epoch': 2.94}\n",
      "{'loss': 0.1033, 'grad_norm': 0.5781554579734802, 'learning_rate': 7.417133705816837e-05, 'epoch': 2.95}\n",
      "{'loss': 0.0412, 'grad_norm': 0.43650633096694946, 'learning_rate': 7.353296070134186e-05, 'epoch': 2.96}\n",
      "{'loss': 0.1376, 'grad_norm': 0.8118205070495605, 'learning_rate': 7.289574209855559e-05, 'epoch': 2.97}\n",
      "{'loss': 0.0816, 'grad_norm': 0.633734405040741, 'learning_rate': 7.225970912381556e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0878, 'grad_norm': 0.6369240283966064, 'learning_rate': 7.16248895992645e-05, 'epoch': 2.99}\n",
      "{'loss': 0.3349, 'grad_norm': 1.2835264205932617, 'learning_rate': 7.099131129396501e-05, 'epoch': 3.0}\n",
      "{'loss': 0.1528, 'grad_norm': 0.8607637882232666, 'learning_rate': 7.035900192268464e-05, 'epoch': 3.0}\n",
      " 61%|███████████████████████▋               | 294/485 [1:21:03<40:07, 12.61s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:09,  1.08it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.07it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.06it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.03it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.01it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.02it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.01it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.02it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06174883991479874, 'eval_runtime': 15.9318, 'eval_samples_per_second': 2.009, 'eval_steps_per_second': 1.004, 'epoch': 3.0}\n",
      " 61%|███████████████████████▋               | 294/485 [1:21:19<40:07, 12.61s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:15<00:00,  1.03it/s]\u001b[A\n",
      "{'loss': 0.2175, 'grad_norm': 0.4983423054218292, 'learning_rate': 6.972798914468369e-05, 'epoch': 3.01}\n",
      "{'loss': 0.6914, 'grad_norm': 0.6539009213447571, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.02}\n",
      "{'loss': 0.2788, 'grad_norm': 0.3943444788455963, 'learning_rate': 6.846996372076786e-05, 'epoch': 3.03}\n",
      "{'loss': 0.2845, 'grad_norm': 0.5336610674858093, 'learning_rate': 6.784300610496048e-05, 'epoch': 3.04}\n",
      "{'loss': 0.3522, 'grad_norm': 0.6536211967468262, 'learning_rate': 6.721745514024022e-05, 'epoch': 3.05}\n",
      "{'loss': 0.3055, 'grad_norm': 0.5853388905525208, 'learning_rate': 6.65933381902329e-05, 'epoch': 3.06}\n",
      "{'loss': 0.0405, 'grad_norm': 0.4582843482494354, 'learning_rate': 6.59706825558357e-05, 'epoch': 3.07}\n",
      "{'loss': 0.1399, 'grad_norm': 0.6852664947509766, 'learning_rate': 6.534951547402322e-05, 'epoch': 3.08}\n",
      "{'loss': 0.0473, 'grad_norm': 0.4469195306301117, 'learning_rate': 6.47298641166559e-05, 'epoch': 3.09}\n",
      "{'loss': 0.0699, 'grad_norm': 0.9286609888076782, 'learning_rate': 6.411175558929152e-05, 'epoch': 3.1}\n",
      "{'loss': 0.0709, 'grad_norm': 0.6214016675949097, 'learning_rate': 6.349521692999945e-05, 'epoch': 3.11}\n",
      "{'loss': 0.0563, 'grad_norm': 0.45817553997039795, 'learning_rate': 6.28802751081779e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0358, 'grad_norm': 0.5017681121826172, 'learning_rate': 6.226695702337442e-05, 'epoch': 3.13}\n",
      "{'loss': 0.0227, 'grad_norm': 0.2519858181476593, 'learning_rate': 6.165528950410884e-05, 'epoch': 3.14}\n",
      "{'loss': 0.0954, 'grad_norm': 0.5853819847106934, 'learning_rate': 6.10452993067e-05, 'epoch': 3.15}\n",
      "{'loss': 0.047, 'grad_norm': 0.37835031747817993, 'learning_rate': 6.0437013114095195e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0952, 'grad_norm': 0.4306288957595825, 'learning_rate': 5.983045753470308e-05, 'epoch': 3.17}\n",
      "{'loss': 0.0548, 'grad_norm': 0.4397585391998291, 'learning_rate': 5.922565910122967e-05, 'epoch': 3.19}\n",
      "{'loss': 0.0841, 'grad_norm': 0.5646506547927856, 'learning_rate': 5.862264426951768e-05, 'epoch': 3.2}\n",
      "{'loss': 0.2236, 'grad_norm': 0.7226988673210144, 'learning_rate': 5.8021439417389444e-05, 'epoch': 3.21}\n",
      "{'loss': 0.0425, 'grad_norm': 0.682912290096283, 'learning_rate': 5.7422070843492734e-05, 'epoch': 3.22}\n",
      "{'loss': 0.0998, 'grad_norm': 0.5272608995437622, 'learning_rate': 5.6824564766150726e-05, 'epoch': 3.23}\n",
      "{'loss': 0.172, 'grad_norm': 0.6237324476242065, 'learning_rate': 5.622894732221482e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0881, 'grad_norm': 0.5456816554069519, 'learning_rate': 5.563524456592163e-05, 'epoch': 3.25}\n",
      "{'loss': 0.1254, 'grad_norm': 2.336921215057373, 'learning_rate': 5.504348246775299e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0955, 'grad_norm': 0.5698842406272888, 'learning_rate': 5.4453686913300074e-05, 'epoch': 3.27}\n",
      "{'loss': 0.1183, 'grad_norm': 0.8235353827476501, 'learning_rate': 5.386588370213124e-05, 'epoch': 3.28}\n",
      "{'loss': 0.1065, 'grad_norm': 0.935493528842926, 'learning_rate': 5.328009854666303e-05, 'epoch': 3.29}\n",
      "{'loss': 0.0999, 'grad_norm': 0.4629177451133728, 'learning_rate': 5.269635707103593e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0865, 'grad_norm': 0.5674570202827454, 'learning_rate': 5.2114684809993044e-05, 'epoch': 3.31}\n",
      "{'loss': 0.1481, 'grad_norm': 1.0635840892791748, 'learning_rate': 5.1535107207763534e-05, 'epoch': 3.32}\n",
      "{'loss': 0.177, 'grad_norm': 0.5313640236854553, 'learning_rate': 5.095764961694922e-05, 'epoch': 3.33}\n",
      "{'loss': 0.4318, 'grad_norm': 0.773906409740448, 'learning_rate': 5.0382337297415773e-05, 'epoch': 3.34}\n",
      "{'loss': 0.0308, 'grad_norm': 0.3145546615123749, 'learning_rate': 4.980919541518796e-05, 'epoch': 3.35}\n",
      "{'loss': 0.0671, 'grad_norm': 0.4651356041431427, 'learning_rate': 4.923824904134829e-05, 'epoch': 3.36}\n",
      "{'loss': 0.0476, 'grad_norm': 0.3712778389453888, 'learning_rate': 4.866952315094088e-05, 'epoch': 3.37}\n",
      "{'loss': 0.0259, 'grad_norm': 0.4352473318576813, 'learning_rate': 4.810304262187852e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0685, 'grad_norm': 0.5530539751052856, 'learning_rate': 4.753883223385467e-05, 'epoch': 3.39}\n",
      "{'loss': 0.032, 'grad_norm': 0.43189945816993713, 'learning_rate': 4.697691666725955e-05, 'epoch': 3.4}\n",
      "{'loss': 0.0648, 'grad_norm': 0.4608975648880005, 'learning_rate': 4.6417320502100316e-05, 'epoch': 3.41}\n",
      "{'loss': 0.1416, 'grad_norm': 0.5353226065635681, 'learning_rate': 4.58600682169262e-05, 'epoch': 3.42}\n",
      "{'loss': 0.1204, 'grad_norm': 0.6560863852500916, 'learning_rate': 4.530518418775733e-05, 'epoch': 3.43}\n",
      "{'loss': 0.0537, 'grad_norm': 0.38676363229751587, 'learning_rate': 4.475269268701868e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0548, 'grad_norm': 1.1623544692993164, 'learning_rate': 4.4202617882478405e-05, 'epoch': 3.45}\n",
      "{'loss': 0.0745, 'grad_norm': 0.4850660264492035, 'learning_rate': 4.365498383619036e-05, 'epoch': 3.46}\n",
      "{'loss': 0.0434, 'grad_norm': 0.6183072328567505, 'learning_rate': 4.310981450344189e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0347, 'grad_norm': 0.4484221935272217, 'learning_rate': 4.256713373170564e-05, 'epoch': 3.48}\n",
      "{'loss': 0.1257, 'grad_norm': 1.0919760465621948, 'learning_rate': 4.2026965259596666e-05, 'epoch': 3.49}\n",
      "{'loss': 0.0362, 'grad_norm': 0.41921043395996094, 'learning_rate': 4.148933271583385e-05, 'epoch': 3.5}\n",
      " 71%|███████████████████████████▌           | 343/485 [1:35:10<38:27, 16.25s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:06<00:09,  1.08s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:08<00:10,  1.31s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:09<00:08,  1.23s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:10<00:06,  1.14s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:11<00:05,  1.09s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:12<00:04,  1.05s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:13<00:03,  1.04s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:14<00:02,  1.01s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:14<00:00,  1.00it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06387536227703094, 'eval_runtime': 16.9759, 'eval_samples_per_second': 1.885, 'eval_steps_per_second': 0.943, 'epoch': 3.5}\n",
      " 71%|███████████████████████████▌           | 343/485 [1:35:27<38:27, 16.25s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:16<00:00,  1.02it/s]\u001b[A\n",
      "{'loss': 0.0541, 'grad_norm': 0.6033464074134827, 'learning_rate': 4.0954259618206295e-05, 'epoch': 3.51}\n",
      "{'loss': 0.0903, 'grad_norm': 0.5396535396575928, 'learning_rate': 4.0421769372544736e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0733, 'grad_norm': 0.9003540873527527, 'learning_rate': 3.9891885271697496e-05, 'epoch': 3.53}\n",
      "{'loss': 0.1157, 'grad_norm': 0.6888198256492615, 'learning_rate': 3.936463049451179e-05, 'epoch': 3.54}\n",
      "{'loss': 0.0267, 'grad_norm': 3.562377452850342, 'learning_rate': 3.884002810481958e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0766, 'grad_norm': 0.5613815784454346, 'learning_rate': 3.8318101050428904e-05, 'epoch': 3.57}\n",
      "{'loss': 0.0811, 'grad_norm': 0.6437739729881287, 'learning_rate': 3.779887216211995e-05, 'epoch': 3.58}\n",
      "{'loss': 0.1459, 'grad_norm': 0.7807638645172119, 'learning_rate': 3.7282364152646297e-05, 'epoch': 3.59}\n",
      "{'loss': 0.0416, 'grad_norm': 0.37454307079315186, 'learning_rate': 3.676859961574162e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0466, 'grad_norm': 0.8829871416091919, 'learning_rate': 3.6257601025131026e-05, 'epoch': 3.61}\n",
      "{'loss': 0.229, 'grad_norm': 0.9044964909553528, 'learning_rate': 3.574939073354838e-05, 'epoch': 3.62}\n",
      "{'loss': 0.0959, 'grad_norm': 0.5150644183158875, 'learning_rate': 3.5243990971758125e-05, 'epoch': 3.63}\n",
      "{'loss': 0.0149, 'grad_norm': 0.3804416060447693, 'learning_rate': 3.4741423847583134e-05, 'epoch': 3.64}\n",
      "{'loss': 0.0923, 'grad_norm': 0.9538113474845886, 'learning_rate': 3.424171134493756e-05, 'epoch': 3.65}\n",
      "{'loss': 0.0574, 'grad_norm': 0.6095606088638306, 'learning_rate': 3.3744875322865034e-05, 'epoch': 3.66}\n",
      "{'loss': 0.0753, 'grad_norm': 0.8339366912841797, 'learning_rate': 3.325093751458276e-05, 'epoch': 3.67}\n",
      "{'loss': 0.0469, 'grad_norm': 0.3281444013118744, 'learning_rate': 3.275991952653054e-05, 'epoch': 3.68}\n",
      "{'loss': 0.075, 'grad_norm': 0.6591441631317139, 'learning_rate': 3.227184283742591e-05, 'epoch': 3.69}\n",
      "{'loss': 0.0879, 'grad_norm': 1.019948959350586, 'learning_rate': 3.178672879732435e-05, 'epoch': 3.7}\n",
      "{'loss': 0.134, 'grad_norm': 0.49484819173812866, 'learning_rate': 3.1304598626685545e-05, 'epoch': 3.71}\n",
      "{'loss': 0.0546, 'grad_norm': 3.126469373703003, 'learning_rate': 3.0825473415445074e-05, 'epoch': 3.72}\n",
      "{'loss': 0.0137, 'grad_norm': 0.29056134819984436, 'learning_rate': 3.034937412209178e-05, 'epoch': 3.73}\n",
      "{'loss': 0.0494, 'grad_norm': 0.2537829577922821, 'learning_rate': 2.9876321572751144e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0225, 'grad_norm': 0.3399847745895386, 'learning_rate': 2.940633646027414e-05, 'epoch': 3.75}\n",
      "{'loss': 0.0318, 'grad_norm': 0.40212544798851013, 'learning_rate': 2.8939439343332086e-05, 'epoch': 3.76}\n",
      "{'loss': 0.0379, 'grad_norm': 0.32675519585609436, 'learning_rate': 2.8475650645517472e-05, 'epoch': 3.77}\n",
      "{'loss': 0.0622, 'grad_norm': 0.6229008436203003, 'learning_rate': 2.8014990654450325e-05, 'epoch': 3.78}\n",
      "{'loss': 0.0611, 'grad_norm': 0.45520520210266113, 'learning_rate': 2.7557479520891104e-05, 'epoch': 3.79}\n",
      "{'loss': 0.0907, 'grad_norm': 0.6239396333694458, 'learning_rate': 2.7103137257858868e-05, 'epoch': 3.8}\n",
      "{'loss': 0.0408, 'grad_norm': 0.9208797216415405, 'learning_rate': 2.6651983739756026e-05, 'epoch': 3.81}\n",
      "{'loss': 0.0191, 'grad_norm': 0.2603149712085724, 'learning_rate': 2.6204038701499056e-05, 'epoch': 3.82}\n",
      "{'loss': 0.049, 'grad_norm': 0.727460503578186, 'learning_rate': 2.5759321737655017e-05, 'epoch': 3.83}\n",
      "{'loss': 0.121, 'grad_norm': 1.6557399034500122, 'learning_rate': 2.5317852301584643e-05, 'epoch': 3.84}\n",
      "{'loss': 0.0686, 'grad_norm': 1.6521682739257812, 'learning_rate': 2.487964970459118e-05, 'epoch': 3.85}\n",
      "{'loss': 0.0978, 'grad_norm': 0.773327112197876, 'learning_rate': 2.4444733115075823e-05, 'epoch': 3.86}\n",
      "{'loss': 0.2905, 'grad_norm': 0.9927974939346313, 'learning_rate': 2.4013121557699157e-05, 'epoch': 3.87}\n",
      "{'loss': 0.0261, 'grad_norm': 0.39342188835144043, 'learning_rate': 2.3584833912548888e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0311, 'grad_norm': 0.4382013976573944, 'learning_rate': 2.315988891431412e-05, 'epoch': 3.89}\n",
      "{'loss': 0.0274, 'grad_norm': 0.6313307285308838, 'learning_rate': 2.2738305151465645e-05, 'epoch': 3.9}\n",
      "{'loss': 0.0464, 'grad_norm': 0.5060155987739563, 'learning_rate': 2.2320101065443056e-05, 'epoch': 3.92}\n",
      "{'loss': 0.0401, 'grad_norm': 0.42584383487701416, 'learning_rate': 2.190529494984782e-05, 'epoch': 3.93}\n",
      "{'loss': 0.0191, 'grad_norm': 0.38090839982032776, 'learning_rate': 2.149390494964323e-05, 'epoch': 3.94}\n",
      "{'loss': 0.0347, 'grad_norm': 0.47385174036026, 'learning_rate': 2.1085949060360654e-05, 'epoch': 3.95}\n",
      "{'loss': 0.1085, 'grad_norm': 0.47083085775375366, 'learning_rate': 2.0681445127312214e-05, 'epoch': 3.96}\n",
      "{'loss': 0.0474, 'grad_norm': 0.5278736352920532, 'learning_rate': 2.0280410844810428e-05, 'epoch': 3.97}\n",
      "{'loss': 0.0339, 'grad_norm': 0.4946148097515106, 'learning_rate': 1.988286375539391e-05, 'epoch': 3.98}\n",
      "{'loss': 0.0511, 'grad_norm': 0.5708422064781189, 'learning_rate': 1.9488821249060297e-05, 'epoch': 3.99}\n",
      "{'loss': 0.1476, 'grad_norm': 1.1977896690368652, 'learning_rate': 1.9098300562505266e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0264, 'grad_norm': 0.16334950923919678, 'learning_rate': 1.871131877836879e-05, 'epoch': 4.0}\n",
      " 81%|███████████████████████████████▌       | 392/485 [1:48:13<19:09, 12.36s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.07it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.061614297330379486, 'eval_runtime': 15.5806, 'eval_samples_per_second': 2.054, 'eval_steps_per_second': 1.027, 'epoch': 4.0}\n",
      " 81%|███████████████████████████████▌       | 392/485 [1:48:29<19:09, 12.36s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 0.3836, 'grad_norm': 0.5905488133430481, 'learning_rate': 1.8327892824487792e-05, 'epoch': 4.01}\n",
      "{'loss': 0.1596, 'grad_norm': 0.3299919664859772, 'learning_rate': 1.7948039473155554e-05, 'epoch': 4.02}\n",
      "{'loss': 0.4475, 'grad_norm': 0.7007402777671814, 'learning_rate': 1.7571775340388276e-05, 'epoch': 4.03}\n",
      "{'loss': 0.0519, 'grad_norm': 0.5027530789375305, 'learning_rate': 1.7199116885197995e-05, 'epoch': 4.04}\n",
      "{'loss': 0.0752, 'grad_norm': 0.31961217522621155, 'learning_rate': 1.683008040887285e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0857, 'grad_norm': 0.37051787972450256, 'learning_rate': 1.646468205426377e-05, 'epoch': 4.06}\n",
      "{'loss': 0.1119, 'grad_norm': 0.37448355555534363, 'learning_rate': 1.6102937805078544e-05, 'epoch': 4.07}\n",
      "{'loss': 0.0213, 'grad_norm': 0.17320244014263153, 'learning_rate': 1.5744863485182537e-05, 'epoch': 4.08}\n",
      "{'loss': 0.0407, 'grad_norm': 0.4181475043296814, 'learning_rate': 1.5390474757906446e-05, 'epoch': 4.09}\n",
      "{'loss': 0.0695, 'grad_norm': 0.4095434844493866, 'learning_rate': 1.5039787125361326e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0055, 'grad_norm': 0.09310019016265869, 'learning_rate': 1.4692815927760273e-05, 'epoch': 4.11}\n",
      "{'loss': 0.0543, 'grad_norm': 0.4420313835144043, 'learning_rate': 1.4349576342747462e-05, 'epoch': 4.12}\n",
      "{'loss': 0.0514, 'grad_norm': 0.47216135263442993, 'learning_rate': 1.4010083384734308e-05, 'epoch': 4.13}\n",
      "{'loss': 0.0462, 'grad_norm': 0.3174366354942322, 'learning_rate': 1.3674351904242611e-05, 'epoch': 4.14}\n",
      "{'loss': 0.1185, 'grad_norm': 0.4138883948326111, 'learning_rate': 1.3342396587254958e-05, 'epoch': 4.15}\n",
      "{'loss': 0.0469, 'grad_norm': 0.3711493909358978, 'learning_rate': 1.3014231954572287e-05, 'epoch': 4.16}\n",
      "{'loss': 0.0386, 'grad_norm': 0.2681318521499634, 'learning_rate': 1.2689872361178701e-05, 'epoch': 4.17}\n",
      "{'loss': 0.0393, 'grad_norm': 0.4604238271713257, 'learning_rate': 1.2369331995613665e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0367, 'grad_norm': 0.4002550542354584, 'learning_rate': 1.2052624879351104e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0642, 'grad_norm': 0.4033774137496948, 'learning_rate': 1.173976486618631e-05, 'epoch': 4.21}\n",
      "{'loss': 0.1108, 'grad_norm': 0.4150215685367584, 'learning_rate': 1.143076564162977e-05, 'epoch': 4.22}\n",
      "{'loss': 0.0284, 'grad_norm': 0.30268722772598267, 'learning_rate': 1.1125640722308628e-05, 'epoch': 4.23}\n",
      "{'loss': 0.0056, 'grad_norm': 0.14282138645648956, 'learning_rate': 1.0824403455375288e-05, 'epoch': 4.24}\n",
      "{'loss': 0.0164, 'grad_norm': 0.12859275937080383, 'learning_rate': 1.0527067017923654e-05, 'epoch': 4.25}\n",
      "{'loss': 0.0458, 'grad_norm': 0.3455604612827301, 'learning_rate': 1.0233644416412791e-05, 'epoch': 4.26}\n",
      "{'loss': 0.0701, 'grad_norm': 0.3459036350250244, 'learning_rate': 9.944148486097793e-06, 'epoch': 4.27}\n",
      "{'loss': 0.0326, 'grad_norm': 0.2642180621623993, 'learning_rate': 9.658591890468515e-06, 'epoch': 4.28}\n",
      "{'loss': 0.0284, 'grad_norm': 1.139702558517456, 'learning_rate': 9.376987120695545e-06, 'epoch': 4.29}\n",
      "{'loss': 0.0599, 'grad_norm': 0.3051513135433197, 'learning_rate': 9.09934649508375e-06, 'epoch': 4.3}\n",
      "{'loss': 0.049, 'grad_norm': 0.3652093708515167, 'learning_rate': 8.825682158533554e-06, 'epoch': 4.31}\n",
      "{'loss': 0.0634, 'grad_norm': 0.43966129422187805, 'learning_rate': 8.55600608200956e-06, 'epoch': 4.32}\n",
      "{'loss': 0.0063, 'grad_norm': 0.17547287046909332, 'learning_rate': 8.290330062017016e-06, 'epoch': 4.33}\n",
      "{'loss': 0.0164, 'grad_norm': 0.27658337354660034, 'learning_rate': 8.02866572008566e-06, 'epoch': 4.34}\n",
      "{'loss': 0.0252, 'grad_norm': 0.3647270202636719, 'learning_rate': 7.771024502261526e-06, 'epoch': 4.35}\n",
      "{'loss': 0.0497, 'grad_norm': 0.6536320447921753, 'learning_rate': 7.51741767860612e-06, 'epoch': 4.36}\n",
      "{'loss': 0.0057, 'grad_norm': 0.1553644984960556, 'learning_rate': 7.267856342703461e-06, 'epoch': 4.37}\n",
      "{'loss': 0.0311, 'grad_norm': 0.20541781187057495, 'learning_rate': 7.022351411174866e-06, 'epoch': 4.38}\n",
      "{'loss': 0.084, 'grad_norm': 0.6316213607788086, 'learning_rate': 6.780913623201346e-06, 'epoch': 4.39}\n",
      "{'loss': 0.0046, 'grad_norm': 0.09769979119300842, 'learning_rate': 6.543553540053926e-06, 'epoch': 4.4}\n",
      "{'loss': 0.0414, 'grad_norm': 0.26128649711608887, 'learning_rate': 6.310281544631546e-06, 'epoch': 4.41}\n",
      "{'loss': 0.0474, 'grad_norm': 0.3305209279060364, 'learning_rate': 6.081107841007006e-06, 'epoch': 4.42}\n",
      "{'loss': 0.0139, 'grad_norm': 0.255318820476532, 'learning_rate': 5.856042453980526e-06, 'epoch': 4.43}\n",
      "{'loss': 0.052, 'grad_norm': 0.5335423946380615, 'learning_rate': 5.63509522864123e-06, 'epoch': 4.44}\n",
      "{'loss': 0.0144, 'grad_norm': 0.22252753376960754, 'learning_rate': 5.418275829936537e-06, 'epoch': 4.45}\n",
      "{'loss': 0.042, 'grad_norm': 0.5081163644790649, 'learning_rate': 5.205593742249326e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0067, 'grad_norm': 0.1408403366804123, 'learning_rate': 4.997058268983135e-06, 'epoch': 4.47}\n",
      "{'loss': 0.0162, 'grad_norm': 1.2907918691635132, 'learning_rate': 4.792678532155115e-06, 'epoch': 4.48}\n",
      "{'loss': 0.0164, 'grad_norm': 1.045074462890625, 'learning_rate': 4.592463471997022e-06, 'epoch': 4.49}\n",
      "{'loss': 0.0767, 'grad_norm': 1.092566728591919, 'learning_rate': 4.3964218465642355e-06, 'epoch': 4.5}\n",
      " 91%|███████████████████████████████████▍   | 441/485 [2:02:03<11:42, 15.96s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:06,  2.08it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:01<00:08,  1.47it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:02<00:09,  1.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:03<00:09,  1.15it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:04<00:08,  1.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:05<00:08,  1.09it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:06<00:07,  1.07it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:07<00:06,  1.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:08<00:05,  1.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:09<00:04,  1.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:10<00:03,  1.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:11<00:02,  1.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:12<00:01,  1.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:13<00:00,  1.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0676569864153862, 'eval_runtime': 15.6083, 'eval_samples_per_second': 2.05, 'eval_steps_per_second': 1.025, 'epoch': 4.5}\n",
      " 91%|███████████████████████████████████▍   | 441/485 [2:02:19<11:42, 15.96s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.04it/s]\u001b[A\n",
      "{'loss': 0.0355, 'grad_norm': 0.5173442959785461, 'learning_rate': 4.204562231352516e-06, 'epoch': 4.51}\n",
      "{'loss': 0.0399, 'grad_norm': 0.36749231815338135, 'learning_rate': 4.016893018922996e-06, 'epoch': 4.52}\n",
      "{'loss': 0.0384, 'grad_norm': 0.5848685503005981, 'learning_rate': 3.83342241853496e-06, 'epoch': 4.53}\n",
      "{'loss': 0.0368, 'grad_norm': 0.28304323554039, 'learning_rate': 3.6541584557868604e-06, 'epoch': 4.54}\n",
      "{'loss': 0.0462, 'grad_norm': 0.41296684741973877, 'learning_rate': 3.4791089722651436e-06, 'epoch': 4.56}\n",
      "{'loss': 0.1141, 'grad_norm': 0.7723785638809204, 'learning_rate': 3.3082816252012926e-06, 'epoch': 4.57}\n",
      "{'loss': 0.0205, 'grad_norm': 0.33317476511001587, 'learning_rate': 3.1416838871368924e-06, 'epoch': 4.58}\n",
      "{'loss': 0.1134, 'grad_norm': 0.542835533618927, 'learning_rate': 2.9793230455966937e-06, 'epoch': 4.59}\n",
      "{'loss': 0.0935, 'grad_norm': 0.6031984686851501, 'learning_rate': 2.821206202769899e-06, 'epoch': 4.6}\n",
      "{'loss': 0.0066, 'grad_norm': 0.2338397353887558, 'learning_rate': 2.667340275199426e-06, 'epoch': 4.61}\n",
      "{'loss': 0.0254, 'grad_norm': 0.5264034867286682, 'learning_rate': 2.5177319934794e-06, 'epoch': 4.62}\n",
      "{'loss': 0.0255, 'grad_norm': 0.24964098632335663, 'learning_rate': 2.3723879019607374e-06, 'epoch': 4.63}\n",
      "{'loss': 0.0391, 'grad_norm': 0.7436255812644958, 'learning_rate': 2.2313143584648423e-06, 'epoch': 4.64}\n",
      "{'loss': 0.0441, 'grad_norm': 0.5477535724639893, 'learning_rate': 2.0945175340055357e-06, 'epoch': 4.65}\n",
      "{'loss': 0.2564, 'grad_norm': 0.9803288578987122, 'learning_rate': 1.9620034125190644e-06, 'epoch': 4.66}\n",
      "{'loss': 0.0744, 'grad_norm': 0.5081140398979187, 'learning_rate': 1.8337777906023978e-06, 'epoch': 4.67}\n",
      "{'loss': 0.0759, 'grad_norm': 0.393028199672699, 'learning_rate': 1.7098462772596302e-06, 'epoch': 4.68}\n",
      "{'loss': 0.1071, 'grad_norm': 0.7238120436668396, 'learning_rate': 1.5902142936566334e-06, 'epoch': 4.69}\n",
      "{'loss': 0.0387, 'grad_norm': 0.43794476985931396, 'learning_rate': 1.4748870728839347e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0498, 'grad_norm': 0.6862562298774719, 'learning_rate': 1.3638696597277679e-06, 'epoch': 4.71}\n",
      "{'loss': 0.0496, 'grad_norm': 0.28776291012763977, 'learning_rate': 1.2571669104494256e-06, 'epoch': 4.72}\n",
      "{'loss': 0.1004, 'grad_norm': 0.625662624835968, 'learning_rate': 1.1547834925728528e-06, 'epoch': 4.73}\n",
      "{'loss': 0.0718, 'grad_norm': 0.45430779457092285, 'learning_rate': 1.0567238846803996e-06, 'epoch': 4.74}\n",
      "{'loss': 0.0138, 'grad_norm': 0.27704814076423645, 'learning_rate': 9.62992376217009e-07, 'epoch': 4.75}\n",
      "{'loss': 0.0185, 'grad_norm': 0.2973017394542694, 'learning_rate': 8.735930673024806e-07, 'epoch': 4.76}\n",
      "{'loss': 0.0079, 'grad_norm': 0.2534196078777313, 'learning_rate': 7.885298685522235e-07, 'epoch': 4.77}\n",
      "{'loss': 0.0497, 'grad_norm': 0.46163448691368103, 'learning_rate': 7.078065009060941e-07, 'epoch': 4.78}\n",
      "{'loss': 0.0138, 'grad_norm': 0.3378329277038574, 'learning_rate': 6.314264954657256e-07, 'epoch': 4.79}\n",
      "{'loss': 0.0351, 'grad_norm': 0.9492939114570618, 'learning_rate': 5.593931933399854e-07, 'epoch': 4.8}\n",
      "{'loss': 0.0108, 'grad_norm': 0.2514699101448059, 'learning_rate': 4.917097454988584e-07, 'epoch': 4.81}\n",
      "{'loss': 0.0134, 'grad_norm': 0.2761445939540863, 'learning_rate': 4.2837911263562404e-07, 'epoch': 4.82}\n",
      "{'loss': 0.0663, 'grad_norm': 0.5135055780410767, 'learning_rate': 3.694040650373154e-07, 'epoch': 4.83}\n",
      "{'loss': 0.0524, 'grad_norm': 0.32663148641586304, 'learning_rate': 3.1478718246357173e-07, 'epoch': 4.84}\n",
      "{'loss': 0.0253, 'grad_norm': 0.11732497066259384, 'learning_rate': 2.645308540337843e-07, 'epoch': 4.85}\n",
      "{'loss': 0.0644, 'grad_norm': 0.447466105222702, 'learning_rate': 2.1863727812254653e-07, 'epoch': 4.86}\n",
      "{'loss': 0.0689, 'grad_norm': 0.42033651471138, 'learning_rate': 1.7710846226355328e-07, 'epoch': 4.87}\n",
      "{'loss': 0.2517, 'grad_norm': 0.568496823310852, 'learning_rate': 1.3994622306173765e-07, 'epoch': 4.88}\n",
      "{'loss': 0.0771, 'grad_norm': 0.4527602791786194, 'learning_rate': 1.0715218611384581e-07, 'epoch': 4.89}\n",
      "{'loss': 0.0122, 'grad_norm': 0.4943557679653168, 'learning_rate': 7.872778593728258e-08, 'epoch': 4.9}\n",
      "{'loss': 0.0463, 'grad_norm': 1.5069785118103027, 'learning_rate': 5.467426590739511e-08, 'epoch': 4.92}\n",
      "{'loss': 0.0159, 'grad_norm': 0.38418328762054443, 'learning_rate': 3.499267820307184e-08, 'epoch': 4.93}\n",
      "{'loss': 0.0204, 'grad_norm': 0.27081382274627686, 'learning_rate': 1.9683883760723832e-08, 'epoch': 4.94}\n",
      "{'loss': 0.083, 'grad_norm': 0.49490615725517273, 'learning_rate': 8.748552236603757e-09, 'epoch': 4.95}\n",
      "{'loss': 0.021, 'grad_norm': 0.6148945689201355, 'learning_rate': 2.187161977540431e-09, 'epoch': 4.96}\n",
      "{'train_runtime': 8043.2995, 'train_samples_per_second': 0.482, 'train_steps_per_second': 0.06, 'train_loss': 0.370496275871224, 'epoch': 4.96}\n",
      "100%|███████████████████████████████████████| 485/485 [2:14:03<00:00, 16.58s/it]\n",
      "[2025-10-10 13:25:19,959] [INFO] [axolotl.train.save_trained_model:244] [PID:3633013] [RANK:0] Training completed! Saving trained model to ./out-gemma-3-4b-it.\u001b[39m\n",
      "[2025-10-10 13:25:20,646] [INFO] [axolotl.train.save_trained_model:341] [PID:3633013] [RANK:0] Model successfully saved to ./out-gemma-3-4b-it\u001b[39m\n",
      "\u001b[0mCPU times: user 49.9 s, sys: 6.5 s, total: 56.4 s\n",
      "Wall time: 2h 21min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the LoRA/DoRA into the base model (for inference & quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-10 13:25:47,400] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3645855] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 13:25:47,400] [WARNING] [axolotl.utils.schemas.config.check_sample_packing_wo_flash:482] [PID:3645855] [RANK:0] sample_packing without flash, sdp, xformers or flex attention does not handle cross sample decontamination.\u001b[39m\n",
      "\u001b[33m[2025-10-10 13:25:47,400] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3645855] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 13:25:47,637] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3645855] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 13:25:47,652] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:318] [PID:3645855] [RANK:0] loading tokenizer... google/gemma-3-4b-it\u001b[39m\n",
      "[2025-10-10 13:25:49,521] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:321] [PID:3645855] [RANK:0] loading model...\u001b[39m\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.11s/it]\n",
      "[2025-10-10 13:26:09,007] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3645855] [RANK:0] cuda memory usage after model load: 8.010GB (+1.252GB cache, +1.223GB misc)\u001b[39m\n",
      "[2025-10-10 13:26:09,040] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3645855] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 13:26:09,050] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:3645855] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 60,614,656 || all params: 4,360,694,128 || trainable%: 1.3900\n",
      "[2025-10-10 13:26:10,902] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3645855] [RANK:0] cuda memory usage after adapters: 8.243GB (+3.723GB cache, +1.317GB misc)\u001b[39m\n",
      "[2025-10-10 13:26:11,350] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:327] [PID:3645855] [RANK:0] loading processor...\u001b[39m\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[2025-10-10 13:26:16,314] [INFO] [axolotl.cli.merge_lora.do_merge_lora:31] [PID:3645855] [RANK:0] Running merge of LoRA with base model...\u001b[39m\n",
      "Unloading and merging model: 100%|████████| 1160/1160 [00:00<00:00, 3659.96it/s]\n",
      "[2025-10-10 13:26:16,664] [INFO] [axolotl.cli.merge_lora.do_merge_lora:44] [PID:3645855] [RANK:0] Saving merged model to: out-gemma-3-4b-it/merged...\u001b[39m\n",
      "\u001b[0mCPU times: user 487 ms, sys: 75.6 ms, total: 562 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/axolotl merge-lora {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 13:26:58 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-10 13:27:02 [utils.py:328] non-default args: {'max_model_len': 8192, 'disable_log_stats': True, 'model': 'out-gemma-3-4b-it/merged'}\n",
      "INFO 10-10 13:27:14 [__init__.py:742] Resolved architecture: Gemma3ForConditionalGeneration\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO 10-10 13:27:14 [__init__.py:1815] Using max model len 8192\n",
      "INFO 10-10 13:27:15 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:19 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:19 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='out-gemma-3-4b-it/merged', speculative_config=None, tokenizer='out-gemma-3-4b-it/merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=out-gemma-3-4b-it/merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[W1010 13:27:22.514928912 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:22 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m WARNING 10-10 13:27:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:28 [gpu_model_runner.py:2338] Starting to load model out-gemma-3-4b-it/merged...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:28 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:28 [cuda.py:379] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:28 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  3.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  3.00s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:34 [default_loader.py:268] Loading weights took 6.08 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:35 [gpu_model_runner.py:2392] Model loading took 8.5834 GiB and 6.596069 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:35 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 31 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:51 [backends.py:539] Using cache directory: /home/oisuomin/.cache/vllm/torch_compile_cache/7f409a8543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:27:51 [backends.py:550] Dynamo bytecode transform time: 9.39 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:05 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 13.994 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:12 [monitor.py:34] torch.compile takes 9.39 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:13 [gpu_worker.py:298] Available KV cache memory: 59.39 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m WARNING 10-10 13:28:13 [kv_cache_utils.py:986] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:13 [kv_cache_utils.py:1028] GPU KV cache size: 444,784 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:13 [kv_cache_utils.py:1032] Maximum concurrency for 8,192 tokens per request: 54.21x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:02<00\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:17 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 2.13 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:17 [gpu_worker.py:391] Free memory on device (78.7/79.18 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 8.58 GiB for weight, 3.27 GiB for peak activation, 0.02 GiB for non-torch memory, and 2.13 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=61316677632` to fit into requested memory, or `--kv-cache-memory=69297487872` to fully utilize gpu memory. Current kv cache memory in use is 63766151168 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3646357)\u001b[0;0m INFO 10-10 13:28:17 [core.py:218] init engine (profile, create kv cache, warmup model) took 41.85 seconds\n",
      "INFO 10-10 13:28:18 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-10 13:28:18 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Adding requests: 100%|███████████████████████| 377/377 [00:00<00:00, 445.91it/s]\n",
      "Processed prompts: 100%|█| 377/377 [00:46<00:00,  8.07it/s, est. speed input: 20\n",
      "Errors: 1 out of 377 records (0.27%)\n",
      "ERROR 10-10 13:29:08 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n",
      "| language   | field     |   mean |   size |\n",
      "|------------|-----------|--------|--------|\n",
      "| en         | alt_title | 0.8815 |    135 |\n",
      "| en         | creator   | 0.8583 |    135 |\n",
      "| en         | doi       | 1.0000 |    135 |\n",
      "| en         | e-isbn    | 0.9012 |    135 |\n",
      "| en         | e-issn    | 0.9481 |    135 |\n",
      "| en         | language  | 0.9704 |    135 |\n",
      "| en         | p-isbn    | 0.8889 |    135 |\n",
      "| en         | p-issn    | 0.9481 |    135 |\n",
      "| en         | publisher | 0.7160 |    135 |\n",
      "| en         | title     | 0.9037 |    135 |\n",
      "| en         | type_coar | 0.8667 |    135 |\n",
      "| en         | year      | 0.9407 |    135 |\n",
      "| fi         | alt_title | 0.8895 |    181 |\n",
      "| fi         | creator   | 0.8463 |    181 |\n",
      "| fi         | doi       | 1.0000 |    181 |\n",
      "| fi         | e-isbn    | 0.9208 |    181 |\n",
      "| fi         | e-issn    | 0.8950 |    181 |\n",
      "| fi         | language  | 0.9890 |    181 |\n",
      "| fi         | p-isbn    | 0.9834 |    181 |\n",
      "| fi         | p-issn    | 0.9724 |    181 |\n",
      "| fi         | publisher | 0.8112 |    181 |\n",
      "| fi         | title     | 0.8122 |    181 |\n",
      "| fi         | type_coar | 0.7514 |    181 |\n",
      "| fi         | year      | 0.8674 |    181 |\n",
      "| se         | alt_title | 1.0000 |      3 |\n",
      "| se         | creator   | 1.0000 |      3 |\n",
      "| se         | doi       | 1.0000 |      3 |\n",
      "| se         | e-isbn    | 1.0000 |      3 |\n",
      "| se         | e-issn    | 1.0000 |      3 |\n",
      "| se         | language  | 1.0000 |      3 |\n",
      "| se         | p-isbn    | 1.0000 |      3 |\n",
      "| se         | p-issn    | 1.0000 |      3 |\n",
      "| se         | publisher | 0.3333 |      3 |\n",
      "| se         | title     | 0.3333 |      3 |\n",
      "| se         | type_coar | 0.6667 |      3 |\n",
      "| se         | year      | 0.6667 |      3 |\n",
      "| sv         | alt_title | 0.8448 |     58 |\n",
      "| sv         | creator   | 0.8937 |     58 |\n",
      "| sv         | doi       | 1.0000 |     58 |\n",
      "| sv         | e-isbn    | 0.9253 |     58 |\n",
      "| sv         | e-issn    | 0.9310 |     58 |\n",
      "| sv         | language  | 1.0000 |     58 |\n",
      "| sv         | p-isbn    | 0.8966 |     58 |\n",
      "| sv         | p-issn    | 0.9483 |     58 |\n",
      "| sv         | publisher | 0.8621 |     58 |\n",
      "| sv         | title     | 0.8966 |     58 |\n",
      "| sv         | type_coar | 0.8276 |     58 |\n",
      "| sv         | year      | 0.9138 |     58 |\n",
      "CPU times: user 1.1 s, sys: 148 ms, total: 1.25 s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate using the evaluate-model script, which needs venv with vLLM installed\n",
    "!../dspy/venv/bin/python evaluate-model.py out-{MODEL_SHORT_NAME}/merged axolotl-test.jsonl ../../eval/results-{MODEL_SHORT_NAME}.md\n",
    "!cat ../../eval/results-{MODEL_SHORT_NAME}.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged\n",
      "INFO:hf-to-gguf:Model architecture: Gemma3ForConditionalGeneration\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {2560, 262208}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.26.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.27.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.28.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.29.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.30.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.31.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.32.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,            torch.bfloat16 --> Q8_0, shape = {10240, 2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 10240}\n",
      "INFO:hf-to-gguf:blk.33.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 2048}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,              torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
      "INFO:gguf.vocab:Setting special token type bos to 2\n",
      "INFO:gguf.vocab:Setting special token type eos to 1\n",
      "INFO:gguf.vocab:Setting special token type unk to 3\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:gemma-3-4b-it-GreyLitLM-Q8_0.gguf: n_tensors = 444, total_size = 4.1G\n",
      "Writing: 100%|██████████████████████████| 4.12G/4.12G [01:53<00:00, 36.2Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to gemma-3-4b-it-GreyLitLM-Q8_0.gguf\n",
      "CPU times: user 892 ms, sys: 203 ms, total: 1.1 s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LLAMA_CPP_PATH = \"../../../llama.cpp\"\n",
    "!{LLAMA_CPP_PATH}/venv/bin/python {LLAMA_CPP_PATH}/convert_hf_to_gguf.py out-{MODEL_SHORT_NAME}/merged --outfile {MODEL_SHORT_NAME}-GreyLitLM-Q8_0.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
      "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf:   1%|▏             | 42.0MB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   1%|▏             | 42.0MB / 4.13GB, 23.3MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   3%|▎             |  109MB / 4.13GB, 54.5MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   4%|▋             |  185MB / 4.13GB, 83.9MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   6%|▉             |  260MB / 4.13GB,  108MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   8%|█▏            |  335MB / 4.13GB,  129MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  10%|█▍            |  411MB / 4.13GB,  147MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  12%|█▋            |  487MB / 4.13GB,  162MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  14%|█▉            |  562MB / 4.13GB,  176MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  15%|██▏           |  629MB / 4.13GB,  185MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  16%|██▎           |  669MB / 4.13GB,  186MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  16%|██▎           |  671MB / 4.13GB,  177MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :   2%|▎             | 1.63MB / 67.0MB,  429kB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  16%|██▎           |  672MB / 4.13GB,  168MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :   4%|▌             | 2.72MB / 67.0MB,  679kB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  16%|██▎           |  673MB / 4.13GB,  160MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :   5%|▋             | 3.26MB / 67.0MB,  776kB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  16%|██▎           |  678MB / 4.13GB,  154MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :   6%|▊             | 8.09MB /  134MB, 1.84MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  17%|██▎           |  682MB / 4.13GB,  148MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :   9%|█▎            | 12.4MB /  134MB, 2.70MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  17%|██▎           |  688MB / 4.13GB,  143MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  14%|█▉            | 18.9MB /  134MB, 3.94MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  17%|██▎           |  699MB / 4.13GB,  140MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  14%|██            | 29.1MB /  201MB, 5.81MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  17%|██▍           |  708MB / 4.13GB,  136MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  19%|██▋           | 38.7MB /  201MB, 7.44MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  17%|██▍           |  718MB / 4.13GB,  133MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  24%|███▍          | 48.9MB /  201MB, 9.05MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  18%|██▍           |  729MB / 4.13GB,  130MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  22%|███▏          | 60.0MB /  268MB, 10.7MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  18%|██▌           |  742MB / 4.13GB,  128MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  22%|███           | 72.8MB /  335MB, 12.5MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  18%|██▌           |  755MB / 4.13GB,  126MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  21%|██▉           | 85.9MB /  402MB, 14.3MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  19%|██▌           |  773MB / 4.13GB,  125MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  26%|███▌          |  104MB /  402MB, 16.7MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  19%|██▋           |  794MB / 4.13GB,  124MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  31%|████▎         |  124MB /  402MB, 19.4MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  20%|██▊           |  815MB / 4.13GB,  124MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  31%|████▎         |  146MB /  469MB, 22.1MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  20%|██▊           |  836MB / 4.13GB,  123MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  36%|████▉         |  167MB /  469MB, 24.5MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  21%|██▉           |  862MB / 4.13GB,  123MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  41%|█████▊        |  193MB /  469MB, 27.6MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  21%|██▉           |  883MB / 4.13GB,  123MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  46%|██████▍       |  214MB /  469MB, 29.7MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  22%|███           |  908MB / 4.13GB,  123MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  45%|██████▏       |  239MB /  536MB, 32.3MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  23%|███▏          |  931MB / 4.13GB,  122MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  49%|██████▊       |  261MB /  536MB, 34.4MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  23%|███▏          |  951MB / 4.13GB,  122MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  53%|███████▎      |  282MB /  536MB, 36.1MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  24%|███▎          |  976MB / 4.13GB,  122MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  51%|███████       |  306MB /  603MB, 38.3MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  24%|███▍          |  996MB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  54%|███████▌      |  327MB /  603MB, 39.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  25%|███▍          | 1.02GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  57%|████████      |  347MB /  603MB, 41.3MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  25%|███▌          | 1.04GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  61%|████████▌     |  370MB /  603MB, 43.0MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  26%|███▌          | 1.06GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  65%|█████████▏    |  395MB /  603MB, 44.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  26%|███▋          | 1.09GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  69%|█████████▋    |  418MB /  603MB, 46.5MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  27%|███▊          | 1.11GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  73%|██████████▏   |  441MB /  603MB, 48.0MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  28%|███▊          | 1.14GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  70%|█████████▋    |  467MB /  670MB, 49.7MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  28%|███▉          | 1.16GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  73%|██████████▏   |  490MB /  670MB, 51.0MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  29%|████          | 1.18GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  77%|██████████▊   |  516MB /  670MB, 52.6MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  29%|████          | 1.21GB / 4.13GB,  121MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  73%|██████████▏   |  539MB /  737MB, 53.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  30%|████▏         | 1.23GB / 4.13GB,  120MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  76%|██████████▌   |  559MB /  737MB, 54.8MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  30%|████▏         | 1.25GB / 4.13GB,  123MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  79%|███████████   |  582MB /  737MB, 57.1MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  31%|████▎         | 1.27GB / 4.13GB,  125MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  75%|██████████▍   |  601MB /  804MB, 59.0MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  31%|████▎         | 1.29GB / 4.13GB,  126MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  77%|██████████▊   |  620MB /  804MB, 60.8MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  32%|████▍         | 1.31GB / 4.13GB,  129MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  80%|███████████▏  |  642MB /  804MB, 62.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  32%|████▌         | 1.33GB / 4.13GB,  131MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  76%|██████████▋   |  664MB /  872MB, 65.1MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  33%|████▌         | 1.36GB / 4.13GB,  133MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  73%|██████████▎   |  690MB /  939MB, 67.6MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  33%|████▋         | 1.38GB / 4.13GB,  136MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  71%|█████████▉    |  714MB / 1.01GB, 70.0MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  34%|████▊         | 1.41GB / 4.13GB,  138MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  69%|█████████▌    |  737MB / 1.07GB, 72.2MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  35%|████▊         | 1.43GB / 4.13GB,  136MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  67%|█████████▎    |  760MB / 1.14GB, 74.5MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  35%|████▉         | 1.45GB / 4.13GB,  132MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  69%|█████████▌    |  781MB / 1.14GB, 76.6MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  36%|█████         | 1.48GB / 4.13GB,  127MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  71%|█████████▉    |  807MB / 1.14GB, 79.2MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  36%|█████         | 1.50GB / 4.13GB,  122MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  73%|██████████▏   |  830MB / 1.14GB, 81.4MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  37%|█████▏        | 1.53GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  75%|██████████▌   |  856MB / 1.14GB, 83.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  38%|█████▎        | 1.55GB / 4.13GB,  112MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  73%|██████████▏   |  881MB / 1.21GB, 86.3MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  38%|█████▎        | 1.58GB / 4.13GB,  107MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  75%|██████████▌   |  906MB / 1.21GB, 88.8MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  39%|█████▍        | 1.60GB / 4.13GB,  101MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  77%|██████████▊   |  927MB / 1.21GB, 90.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  39%|█████▍        | 1.61GB / 4.13GB, 96.6MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  78%|██████████▉   |  945MB / 1.21GB, 92.7MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  40%|█████▌        | 1.64GB / 4.13GB, 94.9MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  80%|███████████▏  |  968MB / 1.21GB, 94.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  40%|█████▋        | 1.66GB / 4.13GB, 96.9MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  82%|███████████▍  |  990MB / 1.21GB, 96.9MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  41%|█████▋        | 1.68GB / 4.13GB, 99.2MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  84%|███████████▊  | 1.01GB / 1.21GB, 99.2MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  41%|█████▊        | 1.71GB / 4.13GB,  101MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  86%|████████████  | 1.04GB / 1.21GB,  101MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  42%|█████▊        | 1.73GB / 4.13GB,  103MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  88%|████████████▎ | 1.06GB / 1.21GB,  103MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  43%|█████▉        | 1.76GB / 4.13GB,  106MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  86%|███████████▉  | 1.09GB / 1.27GB,  106MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  43%|██████        | 1.78GB / 4.13GB,  107MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  83%|███████████▌  | 1.11GB / 1.34GB,  107MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  44%|██████        | 1.80GB / 4.13GB,  108MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  85%|███████████▊  | 1.13GB / 1.34GB,  108MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  44%|██████▏       | 1.83GB / 4.13GB,  110MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  82%|███████████▍  | 1.16GB / 1.41GB,  110MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  45%|██████▎       | 1.85GB / 4.13GB,  111MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  84%|███████████▋  | 1.18GB / 1.41GB,  111MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  45%|██████▎       | 1.87GB / 4.13GB,  111MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  85%|███████████▉  | 1.20GB / 1.41GB,  111MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  46%|██████▍       | 1.88GB / 4.13GB,  112MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  86%|████████████  | 1.21GB / 1.41GB,  112MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  46%|██████▍       | 1.91GB / 4.13GB,  113MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  84%|███████████▋  | 1.24GB / 1.48GB,  113MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  47%|██████▌       | 1.93GB / 4.13GB,  113MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  82%|███████████▍  | 1.26GB / 1.54GB,  113MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  47%|██████▌       | 1.95GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  83%|███████████▋  | 1.28GB / 1.54GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  48%|██████▋       | 1.98GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  85%|███████████▊  | 1.31GB / 1.54GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  49%|██████▊       | 2.00GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  83%|███████████▌  | 1.34GB / 1.61GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  49%|██████▉       | 2.03GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  84%|███████████▊  | 1.36GB / 1.61GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  50%|██████▉       | 2.05GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  82%|███████████▌  | 1.38GB / 1.68GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  50%|███████       | 2.07GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  84%|███████████▋  | 1.40GB / 1.68GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  51%|███████       | 2.10GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  85%|███████████▉  | 1.43GB / 1.68GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  51%|███████▏      | 2.12GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  86%|████████████  | 1.45GB / 1.68GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  52%|███████▎      | 2.14GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  84%|███████████▊  | 1.47GB / 1.74GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  52%|███████▎      | 2.16GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  86%|███████████▉  | 1.49GB / 1.74GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  53%|███████▍      | 2.19GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  87%|████████████▏ | 1.52GB / 1.74GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  54%|███████▌      | 2.22GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▍ | 1.55GB / 1.74GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  54%|███████▌      | 2.24GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  87%|████████████▏ | 1.57GB / 1.81GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  55%|███████▋      | 2.27GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  85%|███████████▉  | 1.60GB / 1.88GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  55%|███████▋      | 2.29GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  86%|████████████  | 1.62GB / 1.88GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  56%|███████▊      | 2.31GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  87%|████████████▏ | 1.64GB / 1.88GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  56%|███████▉      | 2.33GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  88%|████████████▍ | 1.66GB / 1.88GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  57%|███████▉      | 2.35GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▌ | 1.68GB / 1.88GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  57%|████████      | 2.37GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  87%|████████████▏ | 1.70GB / 1.94GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  58%|████████      | 2.39GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▍ | 1.72GB / 1.94GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  59%|████████▏     | 2.42GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▌ | 1.75GB / 1.94GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  59%|████████▎     | 2.44GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▊ | 1.77GB / 1.94GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  60%|████████▎     | 2.47GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▌ | 1.80GB / 2.01GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  60%|████████▍     | 2.50GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  88%|████████████▎ | 1.83GB / 2.08GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  61%|████████▌     | 2.52GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  86%|████████████  | 1.85GB / 2.15GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  62%|████████▌     | 2.54GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  87%|████████████▏ | 1.87GB / 2.15GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  62%|████████▋     | 2.57GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  88%|████████████▎ | 1.90GB / 2.15GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  63%|████████▊     | 2.59GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  87%|████████████▏ | 1.92GB / 2.21GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  63%|████████▊     | 2.61GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  88%|████████████▎ | 1.94GB / 2.21GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  64%|████████▉     | 2.63GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▍ | 1.96GB / 2.21GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  64%|████████▉     | 2.66GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▌ | 1.99GB / 2.21GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  65%|█████████     | 2.68GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.01GB / 2.21GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  66%|█████████▏    | 2.71GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▌ | 2.04GB / 2.28GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  66%|█████████▏    | 2.73GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▋ | 2.06GB / 2.28GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  67%|█████████▎    | 2.75GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▍ | 2.08GB / 2.35GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  67%|█████████▍    | 2.77GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▌ | 2.10GB / 2.35GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  68%|█████████▍    | 2.80GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.13GB / 2.35GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  68%|█████████▌    | 2.83GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▌ | 2.16GB / 2.41GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  69%|█████████▋    | 2.85GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▋ | 2.18GB / 2.41GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  70%|█████████▋    | 2.87GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▍ | 2.20GB / 2.48GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  70%|█████████▊    | 2.89GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▌ | 2.22GB / 2.48GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  71%|█████████▉    | 2.91GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▋ | 2.24GB / 2.48GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  71%|█████████▉    | 2.93GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▊ | 2.27GB / 2.48GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  72%|██████████    | 2.96GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▌ | 2.29GB / 2.55GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  72%|██████████    | 2.99GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.32GB / 2.55GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  73%|██████████▏   | 3.01GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  92%|████████████▊ | 2.34GB / 2.55GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  74%|██████████▎   | 3.04GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.37GB / 2.62GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  74%|██████████▍   | 3.06GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  89%|████████████▍ | 2.39GB / 2.68GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  75%|██████████▍   | 3.08GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▌ | 2.41GB / 2.68GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  75%|██████████▌   | 3.10GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.43GB / 2.68GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  76%|██████████▌   | 3.12GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▊ | 2.45GB / 2.68GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  76%|██████████▋   | 3.15GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  92%|████████████▉ | 2.48GB / 2.68GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  77%|██████████▋   | 3.17GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  93%|█████████████ | 2.50GB / 2.68GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  77%|██████████▊   | 3.19GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  94%|█████████████▏| 2.52GB / 2.68GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  78%|██████████▉   | 3.21GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▎| 2.54GB / 2.68GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  78%|██████████▉   | 3.24GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  93%|█████████████ | 2.57GB / 2.75GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  79%|███████████   | 3.26GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  92%|████████████▉ | 2.59GB / 2.82GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  80%|███████████▏  | 3.29GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.62GB / 2.88GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  80%|███████████▏  | 3.31GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▌ | 2.64GB / 2.95GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  81%|███████████▎  | 3.34GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▋ | 2.67GB / 2.95GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  81%|███████████▍  | 3.36GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▊ | 2.69GB / 2.95GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  82%|███████████▍  | 3.38GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▌ | 2.71GB / 3.02GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  82%|███████████▌  | 3.40GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▋ | 2.73GB / 3.02GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  83%|███████████▌  | 3.42GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▊ | 2.76GB / 3.02GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  84%|███████████▋  | 3.45GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  92%|████████████▉ | 2.78GB / 3.02GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  84%|███████████▊  | 3.47GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.80GB / 3.08GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  85%|███████████▊  | 3.50GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  92%|████████████▊ | 2.83GB / 3.08GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  85%|███████████▉  | 3.52GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  93%|████████████▉ | 2.85GB / 3.08GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  86%|████████████  | 3.55GB / 4.13GB,  118MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▊ | 2.88GB / 3.15GB,  118MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  87%|████████████  | 3.58GB / 4.13GB,  118MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  90%|████████████▋ | 2.91GB / 3.22GB,  118MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  87%|████████████▏ | 3.60GB / 4.13GB,  118MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  91%|████████████▋ | 2.93GB / 3.22GB,  118MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  88%|████████████▎ | 3.62GB / 4.13GB,  118MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  92%|████████████▊ | 2.95GB / 3.22GB,  118MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  88%|████████████▎ | 3.64GB / 4.13GB,  118MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  92%|████████████▉ | 2.97GB / 3.22GB,  118MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  89%|████████████▍ | 3.66GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  93%|█████████████ | 2.99GB / 3.22GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  89%|████████████▍ | 3.68GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  94%|█████████████ | 3.01GB / 3.22GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  90%|████████████▌ | 3.71GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  94%|█████████████▏| 3.04GB / 3.22GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  90%|████████████▋ | 3.73GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▎| 3.06GB / 3.22GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  91%|████████████▋ | 3.75GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  94%|█████████████▏| 3.08GB / 3.29GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  91%|████████████▊ | 3.77GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  94%|█████████████▏| 3.10GB / 3.29GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  92%|████████████▊ | 3.80GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▎| 3.13GB / 3.29GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  93%|████████████▉ | 3.82GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  96%|█████████████▍| 3.15GB / 3.29GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  93%|█████████████ | 3.84GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▎| 3.18GB / 3.35GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  94%|█████████████ | 3.87GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▎| 3.20GB / 3.35GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  94%|█████████████▏| 3.89GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  94%|█████████████▏| 3.22GB / 3.42GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  95%|█████████████▎| 3.92GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  94%|█████████████▏| 3.25GB / 3.46GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  95%|█████████████▎| 3.94GB / 4.13GB,  117MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▏| 3.27GB / 3.46GB,  117MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  96%|█████████████▍| 3.96GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▎| 3.29GB / 3.46GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  96%|█████████████▍| 3.98GB / 4.13GB,  116MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  96%|█████████████▍| 3.31GB / 3.46GB,  116MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  97%|█████████████▌| 4.00GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  96%|█████████████▍| 3.33GB / 3.46GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  97%|█████████████▋| 4.02GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  97%|█████████████▌| 3.35GB / 3.46GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  98%|█████████████▋| 4.04GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  97%|█████████████▋| 3.37GB / 3.46GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  98%|█████████████▊| 4.06GB / 4.13GB,  115MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  98%|█████████████▋| 3.39GB / 3.46GB,  115MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  99%|█████████████▊| 4.08GB / 4.13GB,  114MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  99%|█████████████▊| 3.41GB / 3.46GB,  114MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  99%|█████████████▊| 4.09GB / 4.13GB,  113MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  99%|█████████████▊| 3.42GB / 3.46GB,  113MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  99%|█████████████▉| 4.10GB / 4.13GB,  111MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  99%|█████████████▉| 3.43GB / 3.46GB,  111MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  99%|█████████████▉| 4.11GB / 4.13GB,  110MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  99%|█████████████▉| 3.44GB / 3.46GB,  110MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.11GB / 4.13GB,  108MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  99%|█████████████▉| 3.44GB / 3.46GB,  108MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.12GB / 4.13GB,  106MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉| 3.45GB / 3.46GB,  106MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.12GB / 4.13GB,  104MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉| 3.45GB / 3.46GB,  104MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.13GB / 4.13GB,  103MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉| 3.46GB / 3.46GB,  103MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.13GB / 4.13GB,  101MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉| 3.46GB / 3.46GB,  101MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.13GB / 4.13GB, 98.6MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉| 3.46GB / 3.46GB, 98.6MB/s  \u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.13GB / 4.13GB, 87.6MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|██████████████| 3.46GB / 3.46GB, 87.6MB/s  \u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉| 4.13GB / 4.13GB, 77.0MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|██████████████| 3.46GB / 3.46GB, 77.0MB/s  \n",
      "  ...4b-it-GreyLitLM-Q8_0.gguf: 100%|█████████████▉| 4.13GB / 4.13GB            \n",
      "https://huggingface.co/NatLibFi/gemma-3-4b-it-GreyLitLM-GGUF/blob/main/gemma-3-4b-it-GreyLitLM-Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# Upload the GGUF to Hugging Face Hub\n",
    "FINAL_MODEL_NAME = f\"NatLibFi/{MODEL_SHORT_NAME}-GreyLitLM-GGUF\"\n",
    "\n",
    "!{LLAMA_CPP_PATH}/venv/bin/hf upload {FINAL_MODEL_NAME} {MODEL_SHORT_NAME}-GreyLitLM-Q8_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greylitlm-axolotl",
   "language": "python",
   "name": "greylitlm-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
