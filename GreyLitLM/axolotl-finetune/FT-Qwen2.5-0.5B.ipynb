{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Qwen2.5-0.5B-Instruct model using Axolotl framework\n",
    "\n",
    "How to install dependencies (in HPC environment):\n",
    "\n",
    "- load Python and cuDNN modules\n",
    "- create a Python venv and activate it\n",
    "- install dependencies from requirements.txt (e.g. torch)\n",
    "- install Axolotl from git clone (pip won't work, see [this issue](https://github.com/OpenAccess-AI-Collective/axolotl/issues/945)):\n",
    "\n",
    "```\n",
    "git clone git@github.com:OpenAccess-AI-Collective/axolotl.git\n",
    "cd axolotl\n",
    "pip install -e '.[flash-attn,deepspeed]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/appl/easybuild/opt/CUDA/12.6.0\n"
     ]
    }
   ],
   "source": [
    "!printenv CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1224 train records\n",
      "Wrote 377 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'assistant', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"axolotl-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"axolotl-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"axolotl-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: axolotl-train.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "test_datasets:\n",
    "  - path: axolotl-eval.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "#chat_template: chatml\n",
    "\n",
    "adapter: lora\n",
    "lora_r: 32\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 7\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-10-10 12:13:41,399] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:4136480] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 12:13:41,400] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:4136480] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 12:13:41,623] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:4136480] [RANK:0] cuda memory usage baseline: 0.000GB (+1.667GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 12:13:42,372] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:4136480] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-10-10 12:13:42,376] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:4136480] [RANK:0] Unable to find prepared dataset in last_run_prepared/17cf1644cd1cdae47e6acfaa84bdba96\u001b[39m\n",
      "[2025-10-10 12:13:42,376] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:4136480] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 12:13:42,376] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:4136480] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 1224 examples [00:00, 35032.91 examples/s]\n",
      "[2025-10-10 12:13:42,864] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:4136480] [RANK:0] Loading dataset: axolotl-train.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 12:13:42,894] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:4136480] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|█| 1224/1224 [00:06<00:00, 187.95 example\n",
      "[2025-10-10 12:13:49,864] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:4136480] [RANK:0] min_input_len: 163\u001b[39m\n",
      "[2025-10-10 12:13:49,864] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:4136480] [RANK:0] max_input_len: 8780\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 1224/1224 [00:00<00:00, 2668.59 e\n",
      "\u001b[33m[2025-10-10 12:13:50,990] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:201] [PID:4136480] [RANK:0] Dropped 6 long samples from dataset\u001b[39m\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 1218/1218 [00:00<\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 1218/1218 [00:00<\n",
      "Saving the dataset (1/1 shards): 100%|█| 1218/1218 [00:00<00:00, 10713.71 exampl\n",
      "[2025-10-10 12:13:53,699] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:4136480] [RANK:0] Unable to find prepared dataset in last_run_prepared/d1a000016f7de3936e986d6cb1ba7681\u001b[39m\n",
      "[2025-10-10 12:13:53,699] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:4136480] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 12:13:53,699] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:4136480] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 6675.84 examples/s]\n",
      "[2025-10-10 12:13:54,164] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:4136480] [RANK:0] Loading dataset: axolotl-eval.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 12:13:54,191] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:4136480] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:03<00:00,  8.50 examples/s]\n",
      "[2025-10-10 12:13:58,482] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:4136480] [RANK:0] min_input_len: 389\u001b[39m\n",
      "[2025-10-10 12:13:58,483] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:4136480] [RANK:0] max_input_len: 3304\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:00<00:00, 151.46 exampl\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Saving the dataset (1/1 shards): 100%|█| 32/32 [00:00<00:00, 2293.93 examples/s]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 12:15:10,573] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:4136480] [RANK:0] gather_len_batches: [382]\u001b[39m\n",
      "[2025-10-10 12:15:10,573] [INFO] [axolotl.utils.data.sft._prepare_standard_dataset:123] [PID:4136480] [RANK:0] Maximum number of steps set at 665\u001b[39m\n",
      "[2025-10-10 12:15:11,191] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:4136480] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[2025-10-10 12:15:12,123] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:4136480] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 12:15:12,172] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:4136480] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "trainable params: 17,596,416 || all params: 511,629,184 || trainable%: 3.4393\n",
      "[2025-10-10 12:15:12,532] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:4136480] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n",
      "[2025-10-10 12:15:15,470] [WARNING] [accelerate.utils.other.check_os_kernel:441] [PID:4136480] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[2025-10-10 12:15:19,884] [INFO] [axolotl.train.save_initial_configs:403] [PID:4136480] [RANK:0] Pre-saving adapter config to ./out-Qwen2.5-0.5B-Instruct...\u001b[39m\n",
      "[2025-10-10 12:15:19,886] [INFO] [axolotl.train.save_initial_configs:407] [PID:4136480] [RANK:0] Pre-saving tokenizer to ./out-Qwen2.5-0.5B-Instruct...\u001b[39m\n",
      "[2025-10-10 12:15:20,025] [INFO] [axolotl.train.save_initial_configs:410] [PID:4136480] [RANK:0] Pre-saving model config to ./out-Qwen2.5-0.5B-Instruct...\u001b[39m\n",
      "[2025-10-10 12:15:20,030] [INFO] [axolotl.train.execute_training:225] [PID:4136480] [RANK:0] Starting trainer...\u001b[39m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 12:16:30,707] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:4136480] [RANK:0] gather_len_batches: [383]\u001b[39m\n",
      "  0%|                                                   | 0/665 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 11.99it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:03,  3.86it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:01<00:03,  3.06it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:01<00:03,  3.12it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:02<00:03,  2.78it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:02<00:02,  2.84it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:02<00:02,  2.56it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:03<00:02,  2.74it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:03<00:01,  3.23it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:03<00:01,  2.84it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:04<00:01,  2.85it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:04<00:00,  2.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:05<00:00,  2.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.1820948123931885, 'eval_runtime': 6.2202, 'eval_samples_per_second': 5.145, 'eval_steps_per_second': 2.572, 'epoch': 0}\n",
      "  0%|                                                   | 0/665 [00:06<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.74it/s]\u001b[A\n",
      "{'loss': 1.4292, 'grad_norm': 2.737630844116211, 'learning_rate': 0.0, 'epoch': 0.01}0m\u001b[0m\n",
      "  0%|                                         | 1/665 [00:26<4:53:51, 26.55s/it][2025-10-10 12:17:02,455] [INFO] [axolotl.utils.callbacks.log_gpu_memory_usage:107] [PID:4136480] [RANK:0] cuda memory usage while training: 1.044GB (+17.255GB cache, +34.574GB misc)\u001b[39m\n",
      "{'loss': 1.5269, 'grad_norm': 2.983628988265991, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4308, 'grad_norm': 2.452099323272705, 'learning_rate': 4e-05, 'epoch': 0.03}\n",
      "{'loss': 1.3775, 'grad_norm': 2.7998850345611572, 'learning_rate': 6e-05, 'epoch': 0.04}\n",
      "{'loss': 1.1732, 'grad_norm': 1.6744787693023682, 'learning_rate': 8e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9152, 'grad_norm': 1.4699407815933228, 'learning_rate': 0.0001, 'epoch': 0.06}\n",
      "{'loss': 0.9124, 'grad_norm': 1.4301501512527466, 'learning_rate': 0.00012, 'epoch': 0.07}\n",
      "{'loss': 0.7316, 'grad_norm': 1.396487832069397, 'learning_rate': 0.00014, 'epoch': 0.08}\n",
      "{'loss': 0.5351, 'grad_norm': 1.3378151655197144, 'learning_rate': 0.00016, 'epoch': 0.09}\n",
      "{'loss': 0.4631, 'grad_norm': 1.1812629699707031, 'learning_rate': 0.00018, 'epoch': 0.1}\n",
      "{'loss': 0.4829, 'grad_norm': 1.178439974784851, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.2776, 'grad_norm': 0.9485999345779419, 'learning_rate': 0.00019999884976569195, 'epoch': 0.13}\n",
      "{'loss': 0.3126, 'grad_norm': 1.0563535690307617, 'learning_rate': 0.00019999539908922847, 'epoch': 0.14}\n",
      "{'loss': 0.3953, 'grad_norm': 1.0244652032852173, 'learning_rate': 0.00019998964804999135, 'epoch': 0.15}\n",
      "{'loss': 0.4362, 'grad_norm': 0.9379242658615112, 'learning_rate': 0.00019998159678028148, 'epoch': 0.16}\n",
      "{'loss': 0.2068, 'grad_norm': 0.9980999827384949, 'learning_rate': 0.0001999712454653157, 'epoch': 0.17}\n",
      "{'loss': 0.2186, 'grad_norm': 0.8475062251091003, 'learning_rate': 0.00019995859434322283, 'epoch': 0.18}\n",
      "{'loss': 0.1848, 'grad_norm': 0.6779054999351501, 'learning_rate': 0.00019994364370503793, 'epoch': 0.19}\n",
      "{'loss': 0.2786, 'grad_norm': 0.9581112265586853, 'learning_rate': 0.00019992639389469573, 'epoch': 0.2}\n",
      "{'loss': 0.3061, 'grad_norm': 1.103895664215088, 'learning_rate': 0.00019990684530902275, 'epoch': 0.21}\n",
      "{'loss': 0.1441, 'grad_norm': 0.6515065431594849, 'learning_rate': 0.00019988499839772804, 'epoch': 0.22}\n",
      "{'loss': 0.2411, 'grad_norm': 0.857947587966919, 'learning_rate': 0.00019986085366339293, 'epoch': 0.23}\n",
      "{'loss': 0.1699, 'grad_norm': 0.6614753007888794, 'learning_rate': 0.00019983441166145948, 'epoch': 0.24}\n",
      "{'loss': 0.2126, 'grad_norm': 0.7303705215454102, 'learning_rate': 0.00019980567300021765, 'epoch': 0.25}\n",
      "{'loss': 0.1774, 'grad_norm': 0.6297079920768738, 'learning_rate': 0.00019977463834079128, 'epoch': 0.26}\n",
      "{'loss': 0.1748, 'grad_norm': 0.679882287979126, 'learning_rate': 0.000199741308397123, 'epoch': 0.27}\n",
      "{'loss': 0.1406, 'grad_norm': 0.7111223340034485, 'learning_rate': 0.00019970568393595767, 'epoch': 0.28}\n",
      "{'loss': 0.2067, 'grad_norm': 0.9220317602157593, 'learning_rate': 0.00019966776577682492, 'epoch': 0.29}\n",
      "{'loss': 0.1351, 'grad_norm': 0.5875383615493774, 'learning_rate': 0.00019962755479202001, 'epoch': 0.3}\n",
      "{'loss': 0.1576, 'grad_norm': 0.5478199124336243, 'learning_rate': 0.00019958505190658407, 'epoch': 0.31}\n",
      "{'loss': 0.2965, 'grad_norm': 0.6455506086349487, 'learning_rate': 0.00019954025809828266, 'epoch': 0.32}\n",
      "{'loss': 0.2027, 'grad_norm': 0.7343990802764893, 'learning_rate': 0.00019949317439758323, 'epoch': 0.33}\n",
      "{'loss': 0.1284, 'grad_norm': 0.44257014989852905, 'learning_rate': 0.00019944380188763157, 'epoch': 0.34}\n",
      "{'loss': 0.2121, 'grad_norm': 0.8107408881187439, 'learning_rate': 0.00019939214170422678, 'epoch': 0.36}\n",
      "{'loss': 0.1368, 'grad_norm': 0.41441768407821655, 'learning_rate': 0.00019933819503579513, 'epoch': 0.37}\n",
      "{'loss': 0.2176, 'grad_norm': 0.6817620992660522, 'learning_rate': 0.00019928196312336285, 'epoch': 0.38}\n",
      "{'loss': 0.1968, 'grad_norm': 0.5468997955322266, 'learning_rate': 0.0001992234472605274, 'epoch': 0.39}\n",
      "{'loss': 0.2375, 'grad_norm': 0.629729688167572, 'learning_rate': 0.00019916264879342785, 'epoch': 0.4}\n",
      "{'loss': 0.5349, 'grad_norm': 0.9249168038368225, 'learning_rate': 0.00019909956912071388, 'epoch': 0.41}\n",
      "{'loss': 0.2402, 'grad_norm': 0.5979878902435303, 'learning_rate': 0.0001990342096935135, 'epoch': 0.42}\n",
      "{'loss': 0.2653, 'grad_norm': 0.535660445690155, 'learning_rate': 0.0001989665720153999, 'epoch': 0.43}\n",
      "{'loss': 0.1856, 'grad_norm': 0.5778257250785828, 'learning_rate': 0.0001988966576423566, 'epoch': 0.44}\n",
      "{'loss': 0.239, 'grad_norm': 0.6015192270278931, 'learning_rate': 0.00019882446818274177, 'epoch': 0.45}\n",
      "{'loss': 0.1914, 'grad_norm': 0.5826672911643982, 'learning_rate': 0.00019875000529725133, 'epoch': 0.46}\n",
      "{'loss': 0.1601, 'grad_norm': 0.6122952699661255, 'learning_rate': 0.0001986732706988806, 'epoch': 0.47}\n",
      "{'loss': 0.1103, 'grad_norm': 0.4019988477230072, 'learning_rate': 0.00019859426615288488, 'epoch': 0.48}\n",
      "{'loss': 0.1299, 'grad_norm': 0.4011634290218353, 'learning_rate': 0.00019851299347673894, 'epoch': 0.49}\n",
      "{'loss': 0.3399, 'grad_norm': 0.7344416975975037, 'learning_rate': 0.00019842945454009528, 'epoch': 0.5}\n",
      "  7%|███                                       | 48/665 [03:43<48:38,  4.73s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:03,  4.25it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:00<00:03,  3.79it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:01<00:03,  3.00it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:01<00:03,  2.99it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:02<00:03,  2.68it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:02<00:03,  2.81it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:02<00:03,  2.54it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:03<00:02,  2.69it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:03<00:01,  3.13it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:03<00:01,  2.78it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:04<00:01,  2.87it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:04<00:01,  2.56it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:04<00:00,  2.71it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:05<00:00,  2.58it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.15890467166900635, 'eval_runtime': 6.2349, 'eval_samples_per_second': 5.132, 'eval_steps_per_second': 2.566, 'epoch': 0.5}\n",
      "  7%|███                                       | 48/665 [03:50<48:38,  4.73s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.73it/s]\u001b[A\n",
      "{'loss': 0.2515, 'grad_norm': 0.6097093820571899, 'learning_rate': 0.0001983436512647409, 'epoch': 0.51}\n",
      "{'loss': 0.107, 'grad_norm': 0.39944419264793396, 'learning_rate': 0.00019825558562455313, 'epoch': 0.52}\n",
      "{'loss': 0.0968, 'grad_norm': 0.40174317359924316, 'learning_rate': 0.00019816525964545448, 'epoch': 0.53}\n",
      "{'loss': 0.2048, 'grad_norm': 0.525233268737793, 'learning_rate': 0.0001980726754053657, 'epoch': 0.54}\n",
      "{'loss': 0.1756, 'grad_norm': 0.5904780626296997, 'learning_rate': 0.0001979778350341582, 'epoch': 0.55}\n",
      "{'loss': 0.124, 'grad_norm': 0.38304540514945984, 'learning_rate': 0.00019788074071360492, 'epoch': 0.56}\n",
      "{'loss': 0.1278, 'grad_norm': 0.5603179335594177, 'learning_rate': 0.00019778139467733027, 'epoch': 0.57}\n",
      "{'loss': 0.3575, 'grad_norm': 0.6755090355873108, 'learning_rate': 0.00019767979921075866, 'epoch': 0.58}\n",
      "{'loss': 0.0953, 'grad_norm': 0.6548300385475159, 'learning_rate': 0.00019757595665106185, 'epoch': 0.6}\n",
      "{'loss': 0.2155, 'grad_norm': 0.47062259912490845, 'learning_rate': 0.00019746986938710541, 'epoch': 0.61}\n",
      "{'loss': 0.2842, 'grad_norm': 0.5807684063911438, 'learning_rate': 0.00019736153985939345, 'epoch': 0.62}\n",
      "{'loss': 0.1916, 'grad_norm': 0.5386192202568054, 'learning_rate': 0.00019725097056001286, 'epoch': 0.63}\n",
      "{'loss': 0.2447, 'grad_norm': 0.5788316130638123, 'learning_rate': 0.0001971381640325756, 'epoch': 0.64}\n",
      "{'loss': 0.1152, 'grad_norm': 0.3546507656574249, 'learning_rate': 0.0001970231228721605, 'epoch': 0.65}\n",
      "{'loss': 0.1335, 'grad_norm': 0.4366220533847809, 'learning_rate': 0.00019690584972525325, 'epoch': 0.66}\n",
      "{'loss': 0.0862, 'grad_norm': 0.40814441442489624, 'learning_rate': 0.0001967863472896859, 'epoch': 0.67}\n",
      "{'loss': 0.0918, 'grad_norm': 0.5160290002822876, 'learning_rate': 0.00019666461831457439, 'epoch': 0.68}\n",
      "{'loss': 0.157, 'grad_norm': 0.473012775182724, 'learning_rate': 0.00019654066560025567, 'epoch': 0.69}\n",
      "{'loss': 0.2365, 'grad_norm': 0.6276211142539978, 'learning_rate': 0.00019641449199822293, 'epoch': 0.7}\n",
      "{'loss': 0.0906, 'grad_norm': 0.36680424213409424, 'learning_rate': 0.00019628610041106038, 'epoch': 0.71}\n",
      "{'loss': 0.1105, 'grad_norm': 0.4338741898536682, 'learning_rate': 0.00019615549379237612, 'epoch': 0.72}\n",
      "{'loss': 0.1149, 'grad_norm': 0.41186249256134033, 'learning_rate': 0.0001960226751467345, 'epoch': 0.73}\n",
      "{'loss': 0.1464, 'grad_norm': 0.45032837986946106, 'learning_rate': 0.00019588764752958668, 'epoch': 0.74}\n",
      "{'loss': 0.1914, 'grad_norm': 0.36610791087150574, 'learning_rate': 0.0001957504140472007, 'epoch': 0.75}\n",
      "{'loss': 0.1173, 'grad_norm': 0.34322550892829895, 'learning_rate': 0.00019561097785658973, 'epoch': 0.76}\n",
      "{'loss': 0.1393, 'grad_norm': 0.4233575463294983, 'learning_rate': 0.00019546934216543955, 'epoch': 0.77}\n",
      "{'loss': 0.1685, 'grad_norm': 0.47638365626335144, 'learning_rate': 0.00019532551023203484, 'epoch': 0.78}\n",
      "{'loss': 0.1652, 'grad_norm': 0.49180281162261963, 'learning_rate': 0.000195179485365184, 'epoch': 0.79}\n",
      "{'loss': 0.1346, 'grad_norm': 0.41268786787986755, 'learning_rate': 0.00019503127092414333, 'epoch': 0.8}\n",
      "{'loss': 0.1196, 'grad_norm': 0.5328729748725891, 'learning_rate': 0.0001948808703185395, 'epoch': 0.81}\n",
      "{'loss': 0.2223, 'grad_norm': 0.6212854981422424, 'learning_rate': 0.0001947282870082913, 'epoch': 0.83}\n",
      "{'loss': 0.1728, 'grad_norm': 0.517931342124939, 'learning_rate': 0.0001945735245035298, 'epoch': 0.84}\n",
      "{'loss': 0.0959, 'grad_norm': 0.42503687739372253, 'learning_rate': 0.00019441658636451794, 'epoch': 0.85}\n",
      "{'loss': 0.1404, 'grad_norm': 0.40332698822021484, 'learning_rate': 0.00019425747620156828, 'epoch': 0.86}\n",
      "{'loss': 0.2432, 'grad_norm': 0.6686099767684937, 'learning_rate': 0.00019409619767496024, 'epoch': 0.87}\n",
      "{'loss': 0.1843, 'grad_norm': 0.5410374999046326, 'learning_rate': 0.0001939327544948557, 'epoch': 0.88}\n",
      "{'loss': 0.12, 'grad_norm': 0.43908050656318665, 'learning_rate': 0.0001937671504212137, 'epoch': 0.89}\n",
      "{'loss': 0.2324, 'grad_norm': 0.5661675930023193, 'learning_rate': 0.000193599389263704, 'epoch': 0.9}\n",
      "{'loss': 0.1033, 'grad_norm': 0.38322070240974426, 'learning_rate': 0.0001934294748816194, 'epoch': 0.91}\n",
      "{'loss': 0.1109, 'grad_norm': 0.35085034370422363, 'learning_rate': 0.00019325741118378687, 'epoch': 0.92}\n",
      "{'loss': 0.1161, 'grad_norm': 0.5008989572525024, 'learning_rate': 0.0001930832021284778, 'epoch': 0.93}\n",
      "{'loss': 0.0808, 'grad_norm': 0.33121341466903687, 'learning_rate': 0.0001929068517233169, 'epoch': 0.94}\n",
      "{'loss': 0.5223, 'grad_norm': 0.6888706088066101, 'learning_rate': 0.0001927283640251898, 'epoch': 0.95}\n",
      "{'loss': 0.114, 'grad_norm': 0.3599347770214081, 'learning_rate': 0.00019254774314015003, 'epoch': 0.96}\n",
      "{'loss': 0.1236, 'grad_norm': 0.40764251351356506, 'learning_rate': 0.00019236499322332438, 'epoch': 0.97}\n",
      "{'loss': 0.1428, 'grad_norm': 0.5288941264152527, 'learning_rate': 0.0001921801184788173, 'epoch': 0.98}\n",
      "{'loss': 0.1288, 'grad_norm': 0.4096103310585022, 'learning_rate': 0.0001919931231596143, 'epoch': 0.99}\n",
      "{'loss': 0.1539, 'grad_norm': 0.6152717471122742, 'learning_rate': 0.00019180401156748396, 'epoch': 1.0}\n",
      " 14%|██████                                    | 96/665 [07:11<20:44,  2.19s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:02,  4.98it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:00<00:03,  4.28it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:01<00:03,  3.21it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:01<00:03,  3.16it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:01<00:03,  2.78it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:02<00:03,  2.88it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:02<00:02,  2.75it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:03<00:02,  2.45it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:03<00:02,  2.36it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:03<00:01,  2.59it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:04<00:01,  2.59it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:04<00:01,  2.61it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:04<00:00,  2.86it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:05<00:00,  2.66it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1253015697002411, 'eval_runtime': 6.037, 'eval_samples_per_second': 5.301, 'eval_steps_per_second': 2.65, 'epoch': 1.0}\n",
      " 14%|██████                                    | 96/665 [07:17<20:44,  2.19s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.83it/s]\u001b[A\n",
      "{'loss': 0.4035, 'grad_norm': 0.38453003764152527, 'learning_rate': 0.00019161278805287913, 'epoch': 1.01}\n",
      "{'loss': 0.5881, 'grad_norm': 0.5189369320869446, 'learning_rate': 0.00019141945701483677, 'epoch': 1.02}\n",
      "{'loss': 0.5099, 'grad_norm': 0.4886781573295593, 'learning_rate': 0.0001912240229008767, 'epoch': 1.03}\n",
      "{'loss': 0.3787, 'grad_norm': 0.4098633825778961, 'learning_rate': 0.00019102649020689944, 'epoch': 1.04}\n",
      "{'loss': 0.2155, 'grad_norm': 0.34376904368400574, 'learning_rate': 0.00019082686347708254, 'epoch': 1.05}\n",
      "{'loss': 0.1576, 'grad_norm': 0.3954451382160187, 'learning_rate': 0.00019062514730377634, 'epoch': 1.06}\n",
      "{'loss': 0.1866, 'grad_norm': 0.32890069484710693, 'learning_rate': 0.00019042134632739805, 'epoch': 1.07}\n",
      "{'loss': 0.0977, 'grad_norm': 0.3127346932888031, 'learning_rate': 0.00019021546523632522, 'epoch': 1.08}\n",
      "{'loss': 0.0855, 'grad_norm': 0.2859688401222229, 'learning_rate': 0.0001900075087667877, 'epoch': 1.09}\n",
      "{'loss': 0.0625, 'grad_norm': 0.3106573224067688, 'learning_rate': 0.0001897974817027588, 'epoch': 1.1}\n",
      "{'loss': 0.2349, 'grad_norm': 0.43736040592193604, 'learning_rate': 0.00018958538887584522, 'epoch': 1.11}\n",
      "{'loss': 0.0935, 'grad_norm': 0.3449939787387848, 'learning_rate': 0.00018937123516517588, 'epoch': 1.13}\n",
      "{'loss': 0.0857, 'grad_norm': 0.4088436961174011, 'learning_rate': 0.0001891550254972897, 'epoch': 1.14}\n",
      "{'loss': 0.0367, 'grad_norm': 0.2273719310760498, 'learning_rate': 0.00018893676484602225, 'epoch': 1.15}\n",
      "{'loss': 0.1478, 'grad_norm': 0.38786718249320984, 'learning_rate': 0.00018871645823239128, 'epoch': 1.16}\n",
      "{'loss': 0.1249, 'grad_norm': 0.37885990738868713, 'learning_rate': 0.0001884941107244813, 'epoch': 1.17}\n",
      "{'loss': 0.0774, 'grad_norm': 0.325528621673584, 'learning_rate': 0.00018826972743732698, 'epoch': 1.18}\n",
      "{'loss': 0.0927, 'grad_norm': 0.39462482929229736, 'learning_rate': 0.0001880433135327954, 'epoch': 1.19}\n",
      "{'loss': 0.0687, 'grad_norm': 0.24640043079853058, 'learning_rate': 0.00018781487421946734, 'epoch': 1.2}\n",
      "{'loss': 0.1252, 'grad_norm': 0.37492331862449646, 'learning_rate': 0.00018758441475251754, 'epoch': 1.21}\n",
      "{'loss': 0.1184, 'grad_norm': 0.3902024030685425, 'learning_rate': 0.00018735194043359373, 'epoch': 1.22}\n",
      "{'loss': 0.0761, 'grad_norm': 0.43262800574302673, 'learning_rate': 0.0001871174566106946, 'epoch': 1.23}\n",
      "{'loss': 0.0622, 'grad_norm': 0.32970309257507324, 'learning_rate': 0.000186880968678047, 'epoch': 1.24}\n",
      "{'loss': 0.0935, 'grad_norm': 0.4172106385231018, 'learning_rate': 0.00018664248207598153, 'epoch': 1.25}\n",
      "{'loss': 0.0423, 'grad_norm': 0.27480974793434143, 'learning_rate': 0.00018640200229080763, 'epoch': 1.26}\n",
      "{'loss': 0.1384, 'grad_norm': 0.40845805406570435, 'learning_rate': 0.00018615953485468732, 'epoch': 1.27}\n",
      "{'loss': 0.2769, 'grad_norm': 0.6290938854217529, 'learning_rate': 0.00018591508534550783, 'epoch': 1.28}\n",
      "{'loss': 0.0558, 'grad_norm': 0.3759308159351349, 'learning_rate': 0.00018566865938675345, 'epoch': 1.29}\n",
      "{'loss': 0.0895, 'grad_norm': 0.37291714549064636, 'learning_rate': 0.00018542026264737596, 'epoch': 1.3}\n",
      "{'loss': 0.1011, 'grad_norm': 0.2841090261936188, 'learning_rate': 0.00018516990084166442, 'epoch': 1.31}\n",
      "{'loss': 0.1392, 'grad_norm': 0.4086074233055115, 'learning_rate': 0.00018491757972911366, 'epoch': 1.32}\n",
      "{'loss': 0.134, 'grad_norm': 0.4362228214740753, 'learning_rate': 0.00018466330511429158, 'epoch': 1.33}\n",
      "{'loss': 0.12, 'grad_norm': 0.370982825756073, 'learning_rate': 0.000184407082846706, 'epoch': 1.34}\n",
      "{'loss': 0.0438, 'grad_norm': 0.2543007731437683, 'learning_rate': 0.00018414891882066964, 'epoch': 1.36}\n",
      "{'loss': 0.0463, 'grad_norm': 0.26539239287376404, 'learning_rate': 0.000183888818975165, 'epoch': 1.37}\n",
      "{'loss': 0.0758, 'grad_norm': 0.47319212555885315, 'learning_rate': 0.0001836267892937074, 'epoch': 1.38}\n",
      "{'loss': 0.0713, 'grad_norm': 0.3746775984764099, 'learning_rate': 0.00018336283580420738, 'epoch': 1.39}\n",
      "{'loss': 0.1113, 'grad_norm': 0.3224603533744812, 'learning_rate': 0.00018309696457883214, 'epoch': 1.4}\n",
      "{'loss': 0.1156, 'grad_norm': 0.3555184304714203, 'learning_rate': 0.0001828291817338658, 'epoch': 1.41}\n",
      "{'loss': 0.1448, 'grad_norm': 0.4020776152610779, 'learning_rate': 0.00018255949342956863, 'epoch': 1.42}\n",
      "{'loss': 0.09, 'grad_norm': 0.29905638098716736, 'learning_rate': 0.00018228790587003548, 'epoch': 1.43}\n",
      "{'loss': 0.0549, 'grad_norm': 0.33943086862564087, 'learning_rate': 0.00018201442530305285, 'epoch': 1.44}\n",
      "{'loss': 0.0896, 'grad_norm': 0.27743232250213623, 'learning_rate': 0.00018173905801995545, 'epoch': 1.45}\n",
      "{'loss': 0.0835, 'grad_norm': 0.32900118827819824, 'learning_rate': 0.00018146181035548113, 'epoch': 1.46}\n",
      "{'loss': 0.1379, 'grad_norm': 0.4235698878765106, 'learning_rate': 0.00018118268868762546, 'epoch': 1.47}\n",
      "{'loss': 0.1002, 'grad_norm': 0.34799665212631226, 'learning_rate': 0.00018090169943749476, 'epoch': 1.48}\n",
      "{'loss': 0.0955, 'grad_norm': 0.26199257373809814, 'learning_rate': 0.00018061884906915855, 'epoch': 1.49}\n",
      "{'loss': 0.1038, 'grad_norm': 0.2668314576148987, 'learning_rate': 0.00018033414408950081, 'epoch': 1.5}\n",
      " 22%|████████▉                                | 144/665 [11:25<42:02,  4.84s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:03,  4.30it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:00<00:03,  3.77it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:01<00:03,  3.00it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:01<00:03,  2.99it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:02<00:03,  2.65it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:02<00:03,  2.80it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:02<00:03,  2.57it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:03<00:02,  2.71it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:03<00:01,  3.15it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:03<00:01,  2.81it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:04<00:01,  2.89it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:04<00:01,  2.57it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:04<00:00,  2.72it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:05<00:00,  2.56it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10751385986804962, 'eval_runtime': 5.9281, 'eval_samples_per_second': 5.398, 'eval_steps_per_second': 2.699, 'epoch': 1.5}\n",
      " 22%|████████▉                                | 144/665 [11:31<42:02,  4.84s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.72it/s]\u001b[A\n",
      "{'loss': 0.141, 'grad_norm': 0.47794875502586365, 'learning_rate': 0.0001800475910480702, 'epoch': 1.51}\n",
      "{'loss': 0.0871, 'grad_norm': 0.447041779756546, 'learning_rate': 0.0001797591965369296, 'epoch': 1.52}\n",
      "{'loss': 0.0911, 'grad_norm': 0.33175694942474365, 'learning_rate': 0.00017946896719050413, 'epoch': 1.53}\n",
      "{'loss': 0.0995, 'grad_norm': 0.334933340549469, 'learning_rate': 0.00017917690968542887, 'epoch': 1.54}\n",
      "{'loss': 0.1069, 'grad_norm': 0.43032708764076233, 'learning_rate': 0.00017888303074039503, 'epoch': 1.55}\n",
      "{'loss': 0.0607, 'grad_norm': 0.3851527273654938, 'learning_rate': 0.00017858733711599554, 'epoch': 1.56}\n",
      "{'loss': 0.0613, 'grad_norm': 0.35776931047439575, 'learning_rate': 0.00017828983561456941, 'epoch': 1.57}\n",
      "{'loss': 0.0961, 'grad_norm': 0.2970654368400574, 'learning_rate': 0.00017799053308004535, 'epoch': 1.58}\n",
      "{'loss': 0.1192, 'grad_norm': 0.35916537046432495, 'learning_rate': 0.00017768943639778423, 'epoch': 1.6}\n",
      "{'loss': 0.0452, 'grad_norm': 0.2550223469734192, 'learning_rate': 0.00017738655249442066, 'epoch': 1.61}\n",
      "{'loss': 0.1319, 'grad_norm': 0.44941043853759766, 'learning_rate': 0.00017708188833770385, 'epoch': 1.62}\n",
      "{'loss': 0.0802, 'grad_norm': 0.24026654660701752, 'learning_rate': 0.00017677545093633713, 'epoch': 1.63}\n",
      "{'loss': 0.114, 'grad_norm': 0.38415274024009705, 'learning_rate': 0.00017646724733981665, 'epoch': 1.64}\n",
      "{'loss': 0.1099, 'grad_norm': 0.3951566815376282, 'learning_rate': 0.00017615728463826947, 'epoch': 1.65}\n",
      "{'loss': 0.074, 'grad_norm': 0.4213639497756958, 'learning_rate': 0.0001758455699622903, 'epoch': 1.66}\n",
      "{'loss': 0.0497, 'grad_norm': 0.2769744098186493, 'learning_rate': 0.0001755321104827774, 'epoch': 1.67}\n",
      "{'loss': 0.0957, 'grad_norm': 0.4020242691040039, 'learning_rate': 0.00017521691341076774, 'epoch': 1.68}\n",
      "{'loss': 0.0661, 'grad_norm': 0.27574318647384644, 'learning_rate': 0.00017489998599727098, 'epoch': 1.69}\n",
      "{'loss': 0.0566, 'grad_norm': 0.32766324281692505, 'learning_rate': 0.00017458133553310286, 'epoch': 1.7}\n",
      "{'loss': 0.0419, 'grad_norm': 0.2628079354763031, 'learning_rate': 0.0001742609693487173, 'epoch': 1.71}\n",
      "{'loss': 0.1358, 'grad_norm': 0.46769946813583374, 'learning_rate': 0.00017393889481403784, 'epoch': 1.72}\n",
      "{'loss': 0.1072, 'grad_norm': 0.3559507429599762, 'learning_rate': 0.00017361511933828801, 'epoch': 1.73}\n",
      "{'loss': 0.1289, 'grad_norm': 0.35006189346313477, 'learning_rate': 0.00017328965036982107, 'epoch': 1.74}\n",
      "{'loss': 0.3058, 'grad_norm': 0.6204463839530945, 'learning_rate': 0.00017296249539594847, 'epoch': 1.75}\n",
      "{'loss': 0.0735, 'grad_norm': 0.3189657926559448, 'learning_rate': 0.00017263366194276771, 'epoch': 1.76}\n",
      "{'loss': 0.1419, 'grad_norm': 0.49988171458244324, 'learning_rate': 0.00017230315757498922, 'epoch': 1.77}\n",
      "{'loss': 0.092, 'grad_norm': 0.3954809606075287, 'learning_rate': 0.00017197098989576222, 'epoch': 1.78}\n",
      "{'loss': 0.0686, 'grad_norm': 0.336260586977005, 'learning_rate': 0.00017163716654649994, 'epoch': 1.79}\n",
      "{'loss': 0.1137, 'grad_norm': 0.3951905369758606, 'learning_rate': 0.00017130169520670373, 'epoch': 1.8}\n",
      "{'loss': 0.0787, 'grad_norm': 0.31883060932159424, 'learning_rate': 0.00017096458359378648, 'epoch': 1.81}\n",
      "{'loss': 0.0785, 'grad_norm': 0.34809839725494385, 'learning_rate': 0.0001706258394628951, 'epoch': 1.83}\n",
      "{'loss': 0.0858, 'grad_norm': 0.3383559286594391, 'learning_rate': 0.000170285470606732, 'epoch': 1.84}\n",
      "{'loss': 0.1066, 'grad_norm': 0.46588483452796936, 'learning_rate': 0.00016994348485537583, 'epoch': 1.85}\n",
      "{'loss': 0.0539, 'grad_norm': 0.3084208369255066, 'learning_rate': 0.00016959989007610154, 'epoch': 1.86}\n",
      "{'loss': 0.1515, 'grad_norm': 0.36115095019340515, 'learning_rate': 0.00016925469417319918, 'epoch': 1.87}\n",
      "{'loss': 0.0489, 'grad_norm': 0.28197595477104187, 'learning_rate': 0.0001689079050877921, 'epoch': 1.88}\n",
      "{'loss': 0.3668, 'grad_norm': 0.5337404012680054, 'learning_rate': 0.00016855953079765448, 'epoch': 1.89}\n",
      "{'loss': 0.1843, 'grad_norm': 0.4622839093208313, 'learning_rate': 0.00016820957931702743, 'epoch': 1.9}\n",
      "{'loss': 0.0638, 'grad_norm': 0.2658364772796631, 'learning_rate': 0.000167858058696435, 'epoch': 1.91}\n",
      "{'loss': 0.1064, 'grad_norm': 0.34407851099967957, 'learning_rate': 0.00016750497702249875, 'epoch': 1.92}\n",
      "{'loss': 0.0403, 'grad_norm': 0.2604653835296631, 'learning_rate': 0.0001671503424177517, 'epoch': 1.93}\n",
      "{'loss': 0.0801, 'grad_norm': 0.3907472789287567, 'learning_rate': 0.0001667941630404517, 'epoch': 1.94}\n",
      "{'loss': 0.1328, 'grad_norm': 0.39621323347091675, 'learning_rate': 0.00016643644708439354, 'epoch': 1.95}\n",
      "{'loss': 0.1553, 'grad_norm': 0.5169503092765808, 'learning_rate': 0.00016607720277872053, 'epoch': 1.96}\n",
      "{'loss': 0.1144, 'grad_norm': 0.3605830669403076, 'learning_rate': 0.00016571643838773515, 'epoch': 1.97}\n",
      "{'loss': 0.0292, 'grad_norm': 0.2195204347372055, 'learning_rate': 0.000165354162210709, 'epoch': 1.98}\n",
      "{'loss': 0.1505, 'grad_norm': 0.4816761314868927, 'learning_rate': 0.0001649903825816918, 'epoch': 1.99}\n",
      "{'loss': 0.1534, 'grad_norm': 0.7467468976974487, 'learning_rate': 0.00016462510786931987, 'epoch': 2.0}\n",
      " 29%|███████████▊                             | 192/665 [14:53<35:25,  4.49s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:03,  4.27it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:00<00:03,  3.79it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:01<00:03,  3.01it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:01<00:03,  3.02it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:02<00:03,  2.67it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:02<00:03,  2.80it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:02<00:03,  2.54it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:03<00:02,  2.71it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:03<00:01,  3.13it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:03<00:01,  2.81it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:04<00:01,  2.88it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:04<00:01,  2.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:04<00:00,  2.73it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:05<00:00,  2.57it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10586172342300415, 'eval_runtime': 5.9258, 'eval_samples_per_second': 5.4, 'eval_steps_per_second': 2.7, 'epoch': 2.0}\n",
      " 29%|███████████▊                             | 192/665 [14:59<35:25,  4.49s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.74it/s]\u001b[A\n",
      "{'loss': 0.4272, 'grad_norm': 0.4315052032470703, 'learning_rate': 0.0001642583464766232, 'epoch': 2.01}\n",
      "{'loss': 0.2463, 'grad_norm': 0.32270509004592896, 'learning_rate': 0.00016389010684083257, 'epoch': 2.02}\n",
      "{'loss': 0.103, 'grad_norm': 0.40495461225509644, 'learning_rate': 0.00016352039743318525, 'epoch': 2.03}\n",
      "{'loss': 0.3319, 'grad_norm': 0.5056995749473572, 'learning_rate': 0.0001631492267587301, 'epoch': 2.04}\n",
      "{'loss': 0.1728, 'grad_norm': 0.3123030364513397, 'learning_rate': 0.00016277660335613204, 'epoch': 2.05}\n",
      "{'loss': 0.1374, 'grad_norm': 0.6917164921760559, 'learning_rate': 0.00016240253579747547, 'epoch': 2.06}\n",
      "{'loss': 0.0516, 'grad_norm': 0.20575235784053802, 'learning_rate': 0.0001620270326880672, 'epoch': 2.07}\n",
      "{'loss': 0.0209, 'grad_norm': 0.15003563463687897, 'learning_rate': 0.00016165010266623838, 'epoch': 2.08}\n",
      "{'loss': 0.0831, 'grad_norm': 0.33062756061553955, 'learning_rate': 0.00016127175440314596, 'epoch': 2.09}\n",
      "{'loss': 0.0712, 'grad_norm': 0.31868216395378113, 'learning_rate': 0.00016089199660257285, 'epoch': 2.1}\n",
      "{'loss': 0.048, 'grad_norm': 0.22874358296394348, 'learning_rate': 0.00016051083800072818, 'epoch': 2.11}\n",
      "{'loss': 0.0506, 'grad_norm': 0.2886662483215332, 'learning_rate': 0.00016012828736604594, 'epoch': 2.13}\n",
      "{'loss': 0.1205, 'grad_norm': 0.3449811339378357, 'learning_rate': 0.0001597443534989834, 'epoch': 2.14}\n",
      "{'loss': 0.0763, 'grad_norm': 0.3722965121269226, 'learning_rate': 0.0001593590452318187, 'epoch': 2.15}\n",
      "{'loss': 0.0321, 'grad_norm': 0.2870776355266571, 'learning_rate': 0.00015897237142844758, 'epoch': 2.16}\n",
      "{'loss': 0.0308, 'grad_norm': 0.30382612347602844, 'learning_rate': 0.00015858434098417953, 'epoch': 2.17}\n",
      "{'loss': 0.06, 'grad_norm': 0.43336254358291626, 'learning_rate': 0.00015819496282553318, 'epoch': 2.18}\n",
      "{'loss': 0.0713, 'grad_norm': 0.3126120865345001, 'learning_rate': 0.00015780424591003086, 'epoch': 2.19}\n",
      "{'loss': 0.0462, 'grad_norm': 0.37923672795295715, 'learning_rate': 0.00015741219922599253, 'epoch': 2.2}\n",
      "{'loss': 0.1081, 'grad_norm': 0.44089651107788086, 'learning_rate': 0.00015701883179232917, 'epoch': 2.21}\n",
      "{'loss': 0.0851, 'grad_norm': 0.42973601818084717, 'learning_rate': 0.00015662415265833515, 'epoch': 2.22}\n",
      "{'loss': 0.0504, 'grad_norm': 0.2253236323595047, 'learning_rate': 0.00015622817090348004, 'epoch': 2.23}\n",
      "{'loss': 0.0874, 'grad_norm': 0.33787766098976135, 'learning_rate': 0.00015583089563719985, 'epoch': 2.24}\n",
      "{'loss': 0.0273, 'grad_norm': 0.3090411126613617, 'learning_rate': 0.00015543233599868742, 'epoch': 2.25}\n",
      "{'loss': 0.0473, 'grad_norm': 0.395832896232605, 'learning_rate': 0.00015503250115668214, 'epoch': 2.26}\n",
      "{'loss': 0.1872, 'grad_norm': 0.4442891776561737, 'learning_rate': 0.00015463140030925904, 'epoch': 2.27}\n",
      "{'loss': 0.0748, 'grad_norm': 0.4358023703098297, 'learning_rate': 0.0001542290426836173, 'epoch': 2.28}\n",
      "{'loss': 0.0653, 'grad_norm': 0.3483898937702179, 'learning_rate': 0.00015382543753586774, 'epoch': 2.29}\n",
      "{'loss': 0.0257, 'grad_norm': 0.24463433027267456, 'learning_rate': 0.0001534205941508202, 'epoch': 2.3}\n",
      "{'loss': 0.04, 'grad_norm': 0.23776082694530487, 'learning_rate': 0.00015301452184176962, 'epoch': 2.31}\n",
      "{'loss': 0.0957, 'grad_norm': 0.3138956129550934, 'learning_rate': 0.00015260722995028207, 'epoch': 2.32}\n",
      "{'loss': 0.064, 'grad_norm': 0.317902535200119, 'learning_rate': 0.00015219872784597968, 'epoch': 2.33}\n",
      "{'loss': 0.0707, 'grad_norm': 0.4338032305240631, 'learning_rate': 0.00015178902492632518, 'epoch': 2.34}\n",
      "{'loss': 0.03, 'grad_norm': 0.1887415200471878, 'learning_rate': 0.00015137813061640563, 'epoch': 2.36}\n",
      "{'loss': 0.06, 'grad_norm': 0.39138126373291016, 'learning_rate': 0.00015096605436871566, 'epoch': 2.37}\n",
      "{'loss': 0.0849, 'grad_norm': 0.31688013672828674, 'learning_rate': 0.00015055280566294007, 'epoch': 2.38}\n",
      "{'loss': 0.0373, 'grad_norm': 0.24473370611667633, 'learning_rate': 0.00015013839400573558, 'epoch': 2.39}\n",
      "{'loss': 0.0543, 'grad_norm': 0.26370710134506226, 'learning_rate': 0.00014972282893051237, 'epoch': 2.4}\n",
      "{'loss': 0.0509, 'grad_norm': 0.36819902062416077, 'learning_rate': 0.00014930611999721457, 'epoch': 2.41}\n",
      "{'loss': 0.0347, 'grad_norm': 0.26178625226020813, 'learning_rate': 0.00014888827679210032, 'epoch': 2.42}\n",
      "{'loss': 0.0511, 'grad_norm': 0.2977314889431, 'learning_rate': 0.0001484693089275216, 'epoch': 2.43}\n",
      "{'loss': 0.0612, 'grad_norm': 0.3327866196632385, 'learning_rate': 0.00014804922604170242, 'epoch': 2.44}\n",
      "{'loss': 0.0494, 'grad_norm': 0.31925955414772034, 'learning_rate': 0.00014762803779851789, 'epoch': 2.45}\n",
      "{'loss': 0.0611, 'grad_norm': 0.2550084590911865, 'learning_rate': 0.00014720575388727132, 'epoch': 2.46}\n",
      "{'loss': 0.0432, 'grad_norm': 0.2989666759967804, 'learning_rate': 0.00014678238402247154, 'epoch': 2.47}\n",
      "{'loss': 0.0467, 'grad_norm': 0.3461245000362396, 'learning_rate': 0.00014635793794360943, 'epoch': 2.48}\n",
      "{'loss': 0.0955, 'grad_norm': 0.3224312961101532, 'learning_rate': 0.00014593242541493381, 'epoch': 2.49}\n",
      "{'loss': 0.0622, 'grad_norm': 0.31298500299453735, 'learning_rate': 0.00014550585622522693, 'epoch': 2.5}\n",
      " 36%|██████████████▊                          | 240/665 [18:43<15:23,  2.17s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.83it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  7.26it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:01<00:02,  3.83it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:01<00:02,  3.63it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:01<00:02,  3.10it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:02<00:02,  3.08it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:02<00:02,  2.82it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:02<00:02,  2.87it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:03<00:01,  3.08it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:03<00:01,  2.74it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:03<00:01,  2.86it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:04<00:00,  3.27it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:04<00:00,  2.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10176125913858414, 'eval_runtime': 5.1147, 'eval_samples_per_second': 6.256, 'eval_steps_per_second': 3.128, 'epoch': 2.5}\n",
      " 36%|██████████████▊                          | 240/665 [18:48<15:23,  2.17s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:04<00:00,  2.96it/s]\u001b[A\n",
      "{'loss': 0.0362, 'grad_norm': 0.21520917117595673, 'learning_rate': 0.00014507824018757906, 'epoch': 2.51}\n",
      "{'loss': 0.1053, 'grad_norm': 0.3755909502506256, 'learning_rate': 0.00014464958713916295, 'epoch': 2.52}\n",
      "{'loss': 0.0393, 'grad_norm': 0.30962589383125305, 'learning_rate': 0.0001442199069410074, 'epoch': 2.53}\n",
      "{'loss': 0.0942, 'grad_norm': 0.25448018312454224, 'learning_rate': 0.00014378920947777068, 'epoch': 2.54}\n",
      "{'loss': 0.0289, 'grad_norm': 0.3037593960762024, 'learning_rate': 0.00014335750465751263, 'epoch': 2.55}\n",
      "{'loss': 0.267, 'grad_norm': 0.41871559619903564, 'learning_rate': 0.00014292480241146716, 'epoch': 2.56}\n",
      "{'loss': 0.133, 'grad_norm': 0.46693143248558044, 'learning_rate': 0.0001424911126938137, 'epoch': 2.57}\n",
      "{'loss': 0.0729, 'grad_norm': 0.3223332464694977, 'learning_rate': 0.000142056445481448, 'epoch': 2.58}\n",
      "{'loss': 0.055, 'grad_norm': 0.21275348961353302, 'learning_rate': 0.00014162081077375297, 'epoch': 2.6}\n",
      "{'loss': 0.0777, 'grad_norm': 0.27093833684921265, 'learning_rate': 0.0001411842185923683, 'epoch': 2.61}\n",
      "{'loss': 0.1386, 'grad_norm': 0.39264655113220215, 'learning_rate': 0.0001407466789809601, 'epoch': 2.62}\n",
      "{'loss': 0.1059, 'grad_norm': 0.44656381011009216, 'learning_rate': 0.00014030820200498982, 'epoch': 2.63}\n",
      "{'loss': 0.0266, 'grad_norm': 0.1791973114013672, 'learning_rate': 0.00013986879775148265, 'epoch': 2.64}\n",
      "{'loss': 0.0602, 'grad_norm': 0.32731592655181885, 'learning_rate': 0.00013942847632879564, 'epoch': 2.65}\n",
      "{'loss': 0.0418, 'grad_norm': 0.26549139618873596, 'learning_rate': 0.00013898724786638478, 'epoch': 2.66}\n",
      "{'loss': 0.0567, 'grad_norm': 0.3904646039009094, 'learning_rate': 0.00013854512251457247, 'epoch': 2.67}\n",
      "{'loss': 0.0521, 'grad_norm': 0.27273011207580566, 'learning_rate': 0.00013810211044431366, 'epoch': 2.68}\n",
      "{'loss': 0.0715, 'grad_norm': 0.42350131273269653, 'learning_rate': 0.000137658221846962, 'epoch': 2.69}\n",
      "{'loss': 0.0292, 'grad_norm': 0.2850184738636017, 'learning_rate': 0.00013721346693403534, 'epoch': 2.7}\n",
      "{'loss': 0.049, 'grad_norm': 0.3586154282093048, 'learning_rate': 0.00013676785593698084, 'epoch': 2.71}\n",
      "{'loss': 0.0463, 'grad_norm': 0.35565823316574097, 'learning_rate': 0.0001363213991069397, 'epoch': 2.72}\n",
      "{'loss': 0.0409, 'grad_norm': 0.23525431752204895, 'learning_rate': 0.00013587410671451116, 'epoch': 2.73}\n",
      "{'loss': 0.0426, 'grad_norm': 0.40007418394088745, 'learning_rate': 0.00013542598904951636, 'epoch': 2.74}\n",
      "{'loss': 0.0784, 'grad_norm': 0.35764366388320923, 'learning_rate': 0.00013497705642076147, 'epoch': 2.75}\n",
      "{'loss': 0.0453, 'grad_norm': 0.25763121247291565, 'learning_rate': 0.00013452731915580075, 'epoch': 2.76}\n",
      "{'loss': 0.0436, 'grad_norm': 0.3661086857318878, 'learning_rate': 0.00013407678760069891, 'epoch': 2.77}\n",
      "{'loss': 0.0393, 'grad_norm': 0.231770321726799, 'learning_rate': 0.00013362547211979287, 'epoch': 2.78}\n",
      "{'loss': 0.0299, 'grad_norm': 0.30806344747543335, 'learning_rate': 0.0001331733830954537, 'epoch': 2.79}\n",
      "{'loss': 0.0244, 'grad_norm': 0.22224316000938416, 'learning_rate': 0.00013272053092784747, 'epoch': 2.8}\n",
      "{'loss': 0.0664, 'grad_norm': 0.40646132826805115, 'learning_rate': 0.00013226692603469627, 'epoch': 2.81}\n",
      "{'loss': 0.1021, 'grad_norm': 0.3039313554763794, 'learning_rate': 0.00013181257885103818, 'epoch': 2.83}\n",
      "{'loss': 0.0822, 'grad_norm': 0.3517586290836334, 'learning_rate': 0.00013135749982898766, 'epoch': 2.84}\n",
      "{'loss': 0.0401, 'grad_norm': 0.3100050389766693, 'learning_rate': 0.00013090169943749476, 'epoch': 2.85}\n",
      "{'loss': 0.0751, 'grad_norm': 0.5194991827011108, 'learning_rate': 0.00013044518816210444, 'epoch': 2.86}\n",
      "{'loss': 0.0406, 'grad_norm': 0.2612936794757843, 'learning_rate': 0.00012998797650471533, 'epoch': 2.87}\n",
      "{'loss': 0.0403, 'grad_norm': 0.29173001646995544, 'learning_rate': 0.00012953007498333808, 'epoch': 2.88}\n",
      "{'loss': 0.0793, 'grad_norm': 0.3805221617221832, 'learning_rate': 0.00012907149413185352, 'epoch': 2.89}\n",
      "{'loss': 0.0262, 'grad_norm': 0.3604467511177063, 'learning_rate': 0.0001286122444997702, 'epoch': 2.9}\n",
      "{'loss': 0.0294, 'grad_norm': 0.2800906300544739, 'learning_rate': 0.00012815233665198182, 'epoch': 2.91}\n",
      "{'loss': 0.0468, 'grad_norm': 0.42175188660621643, 'learning_rate': 0.00012769178116852403, 'epoch': 2.92}\n",
      "{'loss': 0.1818, 'grad_norm': 0.5405152440071106, 'learning_rate': 0.00012723058864433118, 'epoch': 2.93}\n",
      "{'loss': 0.037, 'grad_norm': 0.3176019787788391, 'learning_rate': 0.00012676876968899264, 'epoch': 2.94}\n",
      "{'loss': 0.0359, 'grad_norm': 0.3214195668697357, 'learning_rate': 0.0001263063349265084, 'epoch': 2.95}\n",
      "{'loss': 0.0748, 'grad_norm': 0.2884376645088196, 'learning_rate': 0.00012584329499504512, 'epoch': 2.96}\n",
      "{'loss': 0.0313, 'grad_norm': 0.2678297460079193, 'learning_rate': 0.00012537966054669114, 'epoch': 2.97}\n",
      "{'loss': 0.249, 'grad_norm': 0.5928928256034851, 'learning_rate': 0.00012491544224721136, 'epoch': 2.98}\n",
      "{'loss': 0.0287, 'grad_norm': 0.28066274523735046, 'learning_rate': 0.00012445065077580213, 'epoch': 2.99}\n",
      "{'loss': 0.0336, 'grad_norm': 0.35581523180007935, 'learning_rate': 0.00012398529682484531, 'epoch': 3.0}\n",
      " 43%|█████████████████▊                       | 288/665 [22:34<28:29,  4.54s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:02,  5.26it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:00<00:03,  3.51it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:01<00:03,  3.18it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:01<00:03,  3.55it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:01<00:03,  2.94it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:02<00:02,  3.00it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:02<00:02,  2.69it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:02<00:02,  2.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:03<00:02,  2.57it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:03<00:01,  2.75it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:04<00:01,  2.67it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:04<00:01,  2.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:04<00:00,  2.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:04<00:00,  3.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09885846078395844, 'eval_runtime': 6.01, 'eval_samples_per_second': 5.324, 'eval_steps_per_second': 2.662, 'epoch': 3.0}\n",
      " 43%|█████████████████▊                       | 288/665 [22:40<28:29,  4.54s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.96it/s]\u001b[A\n",
      "{'loss': 0.2212, 'grad_norm': 0.3921110928058624, 'learning_rate': 0.0001235193910996626, 'epoch': 3.01}\n",
      "{'loss': 0.2583, 'grad_norm': 0.4203964173793793, 'learning_rate': 0.0001230529443182689, 'epoch': 3.02}\n",
      "{'loss': 0.1542, 'grad_norm': 0.2845153510570526, 'learning_rate': 0.00012258596721112608, 'epoch': 3.03}\n",
      "{'loss': 0.1243, 'grad_norm': 0.3353062570095062, 'learning_rate': 0.00012211847052089587, 'epoch': 3.04}\n",
      "{'loss': 0.1643, 'grad_norm': 0.38976114988327026, 'learning_rate': 0.00012165046500219299, 'epoch': 3.05}\n",
      "{'loss': 0.0635, 'grad_norm': 0.2600643038749695, 'learning_rate': 0.00012118196142133747, 'epoch': 3.06}\n",
      "{'loss': 0.0151, 'grad_norm': 0.22905051708221436, 'learning_rate': 0.00012071297055610717, 'epoch': 3.07}\n",
      "{'loss': 0.0715, 'grad_norm': 0.2978972792625427, 'learning_rate': 0.00012024350319548976, 'epoch': 3.08}\n",
      "{'loss': 0.0166, 'grad_norm': 0.192121684551239, 'learning_rate': 0.0001197735701394345, 'epoch': 3.09}\n",
      "{'loss': 0.0321, 'grad_norm': 0.26548662781715393, 'learning_rate': 0.00011930318219860392, 'epoch': 3.1}\n",
      "{'loss': 0.0282, 'grad_norm': 0.2826852798461914, 'learning_rate': 0.00011883235019412494, 'epoch': 3.11}\n",
      "{'loss': 0.0262, 'grad_norm': 0.2046576291322708, 'learning_rate': 0.00011836108495734005, 'epoch': 3.13}\n",
      "{'loss': 0.0226, 'grad_norm': 0.26504987478256226, 'learning_rate': 0.0001178893973295581, 'epoch': 3.14}\n",
      "{'loss': 0.0365, 'grad_norm': 0.2633729875087738, 'learning_rate': 0.000117417298161805, 'epoch': 3.15}\n",
      "{'loss': 0.0112, 'grad_norm': 0.21338970959186554, 'learning_rate': 0.0001169447983145739, 'epoch': 3.16}\n",
      "{'loss': 0.0364, 'grad_norm': 0.3017032742500305, 'learning_rate': 0.0001164719086575755, 'epoch': 3.17}\n",
      "{'loss': 0.0225, 'grad_norm': 0.22959396243095398, 'learning_rate': 0.00011599864006948795, 'epoch': 3.18}\n",
      "{'loss': 0.0118, 'grad_norm': 0.1577479988336563, 'learning_rate': 0.00011552500343770658, 'epoch': 3.19}\n",
      "{'loss': 0.1692, 'grad_norm': 0.3895634114742279, 'learning_rate': 0.00011505100965809348, 'epoch': 3.2}\n",
      "{'loss': 0.042, 'grad_norm': 0.36628708243370056, 'learning_rate': 0.00011457666963472677, 'epoch': 3.21}\n",
      "{'loss': 0.0302, 'grad_norm': 0.4252389967441559, 'learning_rate': 0.00011410199427964982, 'epoch': 3.22}\n",
      "{'loss': 0.0346, 'grad_norm': 0.2819620370864868, 'learning_rate': 0.00011362699451262025, 'epoch': 3.23}\n",
      "{'loss': 0.1371, 'grad_norm': 0.5390025973320007, 'learning_rate': 0.00011315168126085857, 'epoch': 3.24}\n",
      "{'loss': 0.0151, 'grad_norm': 0.23330354690551758, 'learning_rate': 0.00011267606545879701, 'epoch': 3.25}\n",
      "{'loss': 0.0265, 'grad_norm': 0.1858363300561905, 'learning_rate': 0.00011220015804782776, 'epoch': 3.26}\n",
      "{'loss': 0.0235, 'grad_norm': 0.26223137974739075, 'learning_rate': 0.00011172396997605152, 'epoch': 3.27}\n",
      "{'loss': 0.0679, 'grad_norm': 0.29209885001182556, 'learning_rate': 0.00011124751219802545, 'epoch': 3.28}\n",
      "{'loss': 0.0875, 'grad_norm': 0.4749593436717987, 'learning_rate': 0.00011077079567451111, 'epoch': 3.29}\n",
      "{'loss': 0.0412, 'grad_norm': 0.36445116996765137, 'learning_rate': 0.0001102938313722226, 'epoch': 3.3}\n",
      "{'loss': 0.0263, 'grad_norm': 0.22461596131324768, 'learning_rate': 0.00010981663026357396, 'epoch': 3.31}\n",
      "{'loss': 0.0329, 'grad_norm': 0.4480380117893219, 'learning_rate': 0.00010933920332642695, 'epoch': 3.32}\n",
      "{'loss': 0.0374, 'grad_norm': 0.5004912614822388, 'learning_rate': 0.00010886156154383839, 'epoch': 3.33}\n",
      "{'loss': 0.0946, 'grad_norm': 0.346500962972641, 'learning_rate': 0.00010838371590380765, 'epoch': 3.34}\n",
      "{'loss': 0.0586, 'grad_norm': 0.40593740344047546, 'learning_rate': 0.00010790567739902367, 'epoch': 3.36}\n",
      "{'loss': 0.0507, 'grad_norm': 0.35024774074554443, 'learning_rate': 0.00010742745702661223, 'epoch': 3.37}\n",
      "{'loss': 0.0485, 'grad_norm': 0.3268260657787323, 'learning_rate': 0.00010694906578788291, 'epoch': 3.38}\n",
      "{'loss': 0.057, 'grad_norm': 0.2439914494752884, 'learning_rate': 0.00010647051468807599, 'epoch': 3.39}\n",
      "{'loss': 0.0399, 'grad_norm': 0.2828788757324219, 'learning_rate': 0.0001059918147361094, 'epoch': 3.4}\n",
      "{'loss': 0.0332, 'grad_norm': 0.2509368658065796, 'learning_rate': 0.00010551297694432527, 'epoch': 3.41}\n",
      "{'loss': 0.0136, 'grad_norm': 0.15217195451259613, 'learning_rate': 0.00010503401232823671, 'epoch': 3.42}\n",
      "{'loss': 0.0198, 'grad_norm': 0.22798499464988708, 'learning_rate': 0.00010455493190627441, 'epoch': 3.43}\n",
      "{'loss': 0.0487, 'grad_norm': 0.2653799057006836, 'learning_rate': 0.0001040757466995331, 'epoch': 3.44}\n",
      "{'loss': 0.0308, 'grad_norm': 0.36009150743484497, 'learning_rate': 0.00010359646773151814, 'epoch': 3.45}\n",
      "{'loss': 0.027, 'grad_norm': 0.3814661204814911, 'learning_rate': 0.00010311710602789169, 'epoch': 3.46}\n",
      "{'loss': 0.0337, 'grad_norm': 0.6059935688972473, 'learning_rate': 0.00010263767261621933, 'epoch': 3.47}\n",
      "{'loss': 0.0325, 'grad_norm': 0.36127686500549316, 'learning_rate': 0.00010215817852571626, 'epoch': 3.48}\n",
      "{'loss': 0.0466, 'grad_norm': 0.47447440028190613, 'learning_rate': 0.00010167863478699352, 'epoch': 3.49}\n",
      "{'loss': 0.0135, 'grad_norm': 0.24251116812229156, 'learning_rate': 0.00010119905243180432, 'epoch': 3.5}\n",
      " 51%|████████████████████▋                    | 336/665 [25:06<10:53,  1.98s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.99it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.32it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.38it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  7.02it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.75it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.65it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.36it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.38it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.39it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.44it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.26it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1042710542678833, 'eval_runtime': 2.5496, 'eval_samples_per_second': 12.551, 'eval_steps_per_second': 6.276, 'epoch': 3.5}\n",
      " 51%|████████████████████▋                    | 336/665 [25:08<10:53,  1.98s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.50it/s]\u001b[A\n",
      "{'loss': 0.0277, 'grad_norm': 0.24975018203258514, 'learning_rate': 0.00010071944249279022, 'epoch': 3.51}\n",
      "{'loss': 0.034, 'grad_norm': 0.31360894441604614, 'learning_rate': 0.00010023981600322738, 'epoch': 3.52}\n",
      "{'loss': 0.0308, 'grad_norm': 0.3387521207332611, 'learning_rate': 9.976018399677263e-05, 'epoch': 3.53}\n",
      "{'loss': 0.0574, 'grad_norm': 0.6450339555740356, 'learning_rate': 9.92805575072098e-05, 'epoch': 3.54}\n",
      "{'loss': 0.0249, 'grad_norm': 0.2545158267021179, 'learning_rate': 9.880094756819572e-05, 'epoch': 3.55}\n",
      "{'loss': 0.0239, 'grad_norm': 0.28599873185157776, 'learning_rate': 9.832136521300651e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0221, 'grad_norm': 0.2789853513240814, 'learning_rate': 9.784182147428377e-05, 'epoch': 3.57}\n",
      "{'loss': 0.0359, 'grad_norm': 0.3187708854675293, 'learning_rate': 9.736232738378066e-05, 'epoch': 3.58}\n",
      "{'loss': 0.027, 'grad_norm': 0.23899787664413452, 'learning_rate': 9.688289397210835e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0369, 'grad_norm': 0.24792072176933289, 'learning_rate': 9.64035322684819e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0218, 'grad_norm': 0.20211359858512878, 'learning_rate': 9.592425330046689e-05, 'epoch': 3.62}\n",
      "{'loss': 0.049, 'grad_norm': 0.4078081548213959, 'learning_rate': 9.544506809372561e-05, 'epoch': 3.63}\n",
      "{'loss': 0.0375, 'grad_norm': 0.32335442304611206, 'learning_rate': 9.496598767176334e-05, 'epoch': 3.64}\n",
      "{'loss': 0.0323, 'grad_norm': 0.3273009657859802, 'learning_rate': 9.448702305567475e-05, 'epoch': 3.65}\n",
      "{'loss': 0.0553, 'grad_norm': 0.38916271924972534, 'learning_rate': 9.400818526389063e-05, 'epoch': 3.66}\n",
      "{'loss': 0.0151, 'grad_norm': 0.2392181158065796, 'learning_rate': 9.352948531192401e-05, 'epoch': 3.67}\n",
      "{'loss': 0.0246, 'grad_norm': 0.1901741474866867, 'learning_rate': 9.305093421211713e-05, 'epoch': 3.68}\n",
      "{'loss': 0.0304, 'grad_norm': 0.4523441791534424, 'learning_rate': 9.257254297338782e-05, 'epoch': 3.69}\n",
      "{'loss': 0.019, 'grad_norm': 0.18187394738197327, 'learning_rate': 9.209432260097636e-05, 'epoch': 3.7}\n",
      "{'loss': 0.0162, 'grad_norm': 0.18642786145210266, 'learning_rate': 9.161628409619236e-05, 'epoch': 3.71}\n",
      "{'loss': 0.0346, 'grad_norm': 0.27887552976608276, 'learning_rate': 9.113843845616162e-05, 'epoch': 3.72}\n",
      "{'loss': 0.0208, 'grad_norm': 0.2596794068813324, 'learning_rate': 9.066079667357306e-05, 'epoch': 3.73}\n",
      "{'loss': 0.0256, 'grad_norm': 0.31864577531814575, 'learning_rate': 9.018336973642607e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0375, 'grad_norm': 0.324047327041626, 'learning_rate': 8.97061686277774e-05, 'epoch': 3.75}\n",
      "{'loss': 0.0389, 'grad_norm': 0.32449468970298767, 'learning_rate': 8.92292043254889e-05, 'epoch': 3.76}\n",
      "{'loss': 0.0164, 'grad_norm': 0.24758245050907135, 'learning_rate': 8.875248780197459e-05, 'epoch': 3.77}\n",
      "{'loss': 0.0453, 'grad_norm': 0.3679339289665222, 'learning_rate': 8.827603002394848e-05, 'epoch': 3.78}\n",
      "{'loss': 0.0267, 'grad_norm': 0.20199304819107056, 'learning_rate': 8.779984195217227e-05, 'epoch': 3.79}\n",
      "{'loss': 0.062, 'grad_norm': 0.3183702826499939, 'learning_rate': 8.732393454120304e-05, 'epoch': 3.8}\n",
      "{'loss': 0.055, 'grad_norm': 0.32264652848243713, 'learning_rate': 8.684831873914145e-05, 'epoch': 3.81}\n",
      "{'loss': 0.0468, 'grad_norm': 0.4106726050376892, 'learning_rate': 8.637300548737978e-05, 'epoch': 3.83}\n",
      "{'loss': 0.0455, 'grad_norm': 0.3983931839466095, 'learning_rate': 8.589800572035017e-05, 'epoch': 3.84}\n",
      "{'loss': 0.0434, 'grad_norm': 0.3274753987789154, 'learning_rate': 8.542333036527324e-05, 'epoch': 3.85}\n",
      "{'loss': 0.0589, 'grad_norm': 0.3553791642189026, 'learning_rate': 8.494899034190656e-05, 'epoch': 3.86}\n",
      "{'loss': 0.0163, 'grad_norm': 0.2898004353046417, 'learning_rate': 8.447499656229344e-05, 'epoch': 3.87}\n",
      "{'loss': 0.0537, 'grad_norm': 0.32202205061912537, 'learning_rate': 8.400135993051208e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0618, 'grad_norm': 0.4216051399707794, 'learning_rate': 8.352809134242451e-05, 'epoch': 3.89}\n",
      "{'loss': 0.0338, 'grad_norm': 0.24332845211029053, 'learning_rate': 8.305520168542612e-05, 'epoch': 3.9}\n",
      "{'loss': 0.0963, 'grad_norm': 0.47910255193710327, 'learning_rate': 8.258270183819503e-05, 'epoch': 3.91}\n",
      "{'loss': 0.0192, 'grad_norm': 0.2772599160671234, 'learning_rate': 8.211060267044191e-05, 'epoch': 3.92}\n",
      "{'loss': 0.1111, 'grad_norm': 0.4337843656539917, 'learning_rate': 8.163891504265999e-05, 'epoch': 3.93}\n",
      "{'loss': 0.0134, 'grad_norm': 0.4655139148235321, 'learning_rate': 8.11676498058751e-05, 'epoch': 3.94}\n",
      "{'loss': 0.046, 'grad_norm': 0.30307912826538086, 'learning_rate': 8.069681780139609e-05, 'epoch': 3.95}\n",
      "{'loss': 0.0289, 'grad_norm': 0.7040572166442871, 'learning_rate': 8.022642986056552e-05, 'epoch': 3.96}\n",
      "{'loss': 0.0159, 'grad_norm': 0.20070867240428925, 'learning_rate': 7.975649680451024e-05, 'epoch': 3.97}\n",
      "{'loss': 0.0803, 'grad_norm': 0.4103938937187195, 'learning_rate': 7.928702944389284e-05, 'epoch': 3.98}\n",
      "{'loss': 0.0474, 'grad_norm': 0.5601229667663574, 'learning_rate': 7.881803857866254e-05, 'epoch': 3.99}\n",
      "{'loss': 0.0167, 'grad_norm': 0.19391581416130066, 'learning_rate': 7.834953499780702e-05, 'epoch': 4.0}\n",
      " 58%|███████████████████████▋                 | 384/665 [26:44<08:58,  1.91s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.00it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.33it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.36it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  7.00it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.75it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.65it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.39it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.38it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.43it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09849727153778076, 'eval_runtime': 2.5501, 'eval_samples_per_second': 12.548, 'eval_steps_per_second': 6.274, 'epoch': 4.0}\n",
      " 58%|███████████████████████▋                 | 384/665 [26:46<08:58,  1.91s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.50it/s]\u001b[A\n",
      "{'loss': 0.0635, 'grad_norm': 0.2901148200035095, 'learning_rate': 7.788152947910414e-05, 'epoch': 4.01}\n",
      "{'loss': 0.1958, 'grad_norm': 0.4018789231777191, 'learning_rate': 7.741403278887397e-05, 'epoch': 4.02}\n",
      "{'loss': 0.0508, 'grad_norm': 0.23340141773223877, 'learning_rate': 7.694705568173111e-05, 'epoch': 4.03}\n",
      "{'loss': 0.0336, 'grad_norm': 0.35617363452911377, 'learning_rate': 7.648060890033743e-05, 'epoch': 4.04}\n",
      "{'loss': 0.113, 'grad_norm': 0.2870463728904724, 'learning_rate': 7.601470317515468e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0944, 'grad_norm': 0.3657359778881073, 'learning_rate': 7.554934922419789e-05, 'epoch': 4.06}\n",
      "{'loss': 0.0763, 'grad_norm': 0.30928584933280945, 'learning_rate': 7.508455775278867e-05, 'epoch': 4.07}\n",
      "{'loss': 0.0295, 'grad_norm': 0.16359058022499084, 'learning_rate': 7.462033945330889e-05, 'epoch': 4.08}\n",
      "{'loss': 0.0079, 'grad_norm': 0.11386068910360336, 'learning_rate': 7.41567050049549e-05, 'epoch': 4.09}\n",
      "{'loss': 0.0713, 'grad_norm': 0.2601114511489868, 'learning_rate': 7.369366507349165e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0297, 'grad_norm': 0.17958015203475952, 'learning_rate': 7.32312303110074e-05, 'epoch': 4.11}\n",
      "{'loss': 0.0562, 'grad_norm': 0.3219746947288513, 'learning_rate': 7.276941135566884e-05, 'epoch': 4.13}\n",
      "{'loss': 0.0115, 'grad_norm': 0.2150416374206543, 'learning_rate': 7.230821883147598e-05, 'epoch': 4.14}\n",
      "{'loss': 0.0165, 'grad_norm': 0.14981035888195038, 'learning_rate': 7.18476633480182e-05, 'epoch': 4.15}\n",
      "{'loss': 0.0414, 'grad_norm': 0.20546485483646393, 'learning_rate': 7.13877555002298e-05, 'epoch': 4.16}\n",
      "{'loss': 0.0204, 'grad_norm': 0.2948780357837677, 'learning_rate': 7.092850586814648e-05, 'epoch': 4.17}\n",
      "{'loss': 0.0468, 'grad_norm': 0.33173054456710815, 'learning_rate': 7.046992501666195e-05, 'epoch': 4.18}\n",
      "{'loss': 0.024, 'grad_norm': 0.19943752884864807, 'learning_rate': 7.001202349528473e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0237, 'grad_norm': 0.15839217603206635, 'learning_rate': 6.955481183789558e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0124, 'grad_norm': 0.2772672176361084, 'learning_rate': 6.909830056250527e-05, 'epoch': 4.21}\n",
      "{'loss': 0.081, 'grad_norm': 0.4516109824180603, 'learning_rate': 6.864250017101235e-05, 'epoch': 4.22}\n",
      "{'loss': 0.0126, 'grad_norm': 0.21262557804584503, 'learning_rate': 6.818742114896184e-05, 'epoch': 4.23}\n",
      "{'loss': 0.0642, 'grad_norm': 0.40091201663017273, 'learning_rate': 6.773307396530378e-05, 'epoch': 4.24}\n",
      "{'loss': 0.0094, 'grad_norm': 0.15838351845741272, 'learning_rate': 6.727946907215253e-05, 'epoch': 4.25}\n",
      "{'loss': 0.0083, 'grad_norm': 0.14034637808799744, 'learning_rate': 6.682661690454633e-05, 'epoch': 4.26}\n",
      "{'loss': 0.0256, 'grad_norm': 0.32524049282073975, 'learning_rate': 6.637452788020717e-05, 'epoch': 4.27}\n",
      "{'loss': 0.0229, 'grad_norm': 0.25606757402420044, 'learning_rate': 6.592321239930112e-05, 'epoch': 4.28}\n",
      "{'loss': 0.0253, 'grad_norm': 0.2900939881801605, 'learning_rate': 6.547268084419927e-05, 'epoch': 4.29}\n",
      "{'loss': 0.0074, 'grad_norm': 0.2593039572238922, 'learning_rate': 6.502294357923855e-05, 'epoch': 4.3}\n",
      "{'loss': 0.006, 'grad_norm': 0.41398948431015015, 'learning_rate': 6.457401095048368e-05, 'epoch': 4.31}\n",
      "{'loss': 0.0121, 'grad_norm': 0.30317944288253784, 'learning_rate': 6.412589328548886e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0234, 'grad_norm': 0.2283671349287033, 'learning_rate': 6.367860089306028e-05, 'epoch': 4.33}\n",
      "{'loss': 0.0441, 'grad_norm': 0.2505195438861847, 'learning_rate': 6.323214406301918e-05, 'epoch': 4.34}\n",
      "{'loss': 0.0161, 'grad_norm': 0.17214813828468323, 'learning_rate': 6.278653306596472e-05, 'epoch': 4.36}\n",
      "{'loss': 0.0118, 'grad_norm': 0.14843066036701202, 'learning_rate': 6.234177815303804e-05, 'epoch': 4.37}\n",
      "{'loss': 0.0178, 'grad_norm': 0.21057534217834473, 'learning_rate': 6.189788955568636e-05, 'epoch': 4.38}\n",
      "{'loss': 0.0105, 'grad_norm': 0.21000109612941742, 'learning_rate': 6.145487748542753e-05, 'epoch': 4.39}\n",
      "{'loss': 0.0208, 'grad_norm': 0.3175080120563507, 'learning_rate': 6.101275213361526e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0146, 'grad_norm': 0.27157101035118103, 'learning_rate': 6.057152367120442e-05, 'epoch': 4.41}\n",
      "{'loss': 0.0161, 'grad_norm': 0.2152245193719864, 'learning_rate': 6.0131202248517336e-05, 'epoch': 4.42}\n",
      "{'loss': 0.0302, 'grad_norm': 0.2713747024536133, 'learning_rate': 5.96917979950102e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0429, 'grad_norm': 0.49317091703414917, 'learning_rate': 5.925332101903994e-05, 'epoch': 4.44}\n",
      "{'loss': 0.021, 'grad_norm': 0.3745575249195099, 'learning_rate': 5.8815781407631734e-05, 'epoch': 4.45}\n",
      "{'loss': 0.0193, 'grad_norm': 0.16306893527507782, 'learning_rate': 5.8379189226247055e-05, 'epoch': 4.46}\n",
      "{'loss': 0.0193, 'grad_norm': 0.27226561307907104, 'learning_rate': 5.794355451855198e-05, 'epoch': 4.47}\n",
      "{'loss': 0.0156, 'grad_norm': 0.45240628719329834, 'learning_rate': 5.750888730618636e-05, 'epoch': 4.48}\n",
      "{'loss': 0.0104, 'grad_norm': 0.19038337469100952, 'learning_rate': 5.707519758853288e-05, 'epoch': 4.49}\n",
      "{'loss': 0.0134, 'grad_norm': 0.1223745048046112, 'learning_rate': 5.664249534248739e-05, 'epoch': 4.5}\n",
      " 65%|██████████████████████████▋              | 432/665 [28:38<07:42,  1.99s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.97it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.29it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.34it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.97it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.72it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.62it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.35it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.36it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.39it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.42it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.29it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.29it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10750138759613037, 'eval_runtime': 2.5613, 'eval_samples_per_second': 12.494, 'eval_steps_per_second': 6.247, 'epoch': 4.5}\n",
      " 65%|██████████████████████████▋              | 432/665 [28:40<07:42,  1.99s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.48it/s]\u001b[A\n",
      "{'loss': 0.0101, 'grad_norm': 0.17932792007923126, 'learning_rate': 5.621079052222933e-05, 'epoch': 4.51}\n",
      "{'loss': 0.018, 'grad_norm': 0.34343236684799194, 'learning_rate': 5.5780093058992564e-05, 'epoch': 4.52}\n",
      "{'loss': 0.0862, 'grad_norm': 0.5383722186088562, 'learning_rate': 5.535041286083709e-05, 'epoch': 4.53}\n",
      "{'loss': 0.0445, 'grad_norm': 0.5161908268928528, 'learning_rate': 5.492175981242097e-05, 'epoch': 4.54}\n",
      "{'loss': 0.02, 'grad_norm': 0.489522784948349, 'learning_rate': 5.449414377477309e-05, 'epoch': 4.55}\n",
      "{'loss': 0.0298, 'grad_norm': 0.25667205452919006, 'learning_rate': 5.4067574585066195e-05, 'epoch': 4.56}\n",
      "{'loss': 0.0295, 'grad_norm': 0.3593071699142456, 'learning_rate': 5.36420620563906e-05, 'epoch': 4.57}\n",
      "{'loss': 0.0231, 'grad_norm': 0.23710325360298157, 'learning_rate': 5.32176159775285e-05, 'epoch': 4.58}\n",
      "{'loss': 0.0483, 'grad_norm': 0.3174028694629669, 'learning_rate': 5.279424611272873e-05, 'epoch': 4.6}\n",
      "{'loss': 0.1033, 'grad_norm': 0.3227073848247528, 'learning_rate': 5.23719622014821e-05, 'epoch': 4.61}\n",
      "{'loss': 0.0183, 'grad_norm': 0.15778788924217224, 'learning_rate': 5.1950773958297586e-05, 'epoch': 4.62}\n",
      "{'loss': 0.0161, 'grad_norm': 0.2366407960653305, 'learning_rate': 5.15306910724785e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0165, 'grad_norm': 0.2645685076713562, 'learning_rate': 5.1111723207899666e-05, 'epoch': 4.64}\n",
      "{'loss': 0.0134, 'grad_norm': 0.2578740119934082, 'learning_rate': 5.0693880002785456e-05, 'epoch': 4.65}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0669427216053009, 'learning_rate': 5.0277171069487636e-05, 'epoch': 4.66}\n",
      "{'loss': 0.0092, 'grad_norm': 0.24704474210739136, 'learning_rate': 4.986160599426443e-05, 'epoch': 4.67}\n",
      "{'loss': 0.0253, 'grad_norm': 0.23142516613006592, 'learning_rate': 4.944719433705996e-05, 'epoch': 4.68}\n",
      "{'loss': 0.0144, 'grad_norm': 0.4857201874256134, 'learning_rate': 4.9033945631284326e-05, 'epoch': 4.69}\n",
      "{'loss': 0.0183, 'grad_norm': 0.23534156382083893, 'learning_rate': 4.8621869383594406e-05, 'epoch': 4.7}\n",
      "{'loss': 0.0319, 'grad_norm': 0.22880613803863525, 'learning_rate': 4.821097507367486e-05, 'epoch': 4.71}\n",
      "{'loss': 0.0283, 'grad_norm': 0.38908660411834717, 'learning_rate': 4.780127215402031e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0137, 'grad_norm': 0.2951718866825104, 'learning_rate': 4.7392770049717936e-05, 'epoch': 4.73}\n",
      "{'loss': 0.016, 'grad_norm': 0.17876555025577545, 'learning_rate': 4.698547815823042e-05, 'epoch': 4.74}\n",
      "{'loss': 0.0405, 'grad_norm': 0.277832955121994, 'learning_rate': 4.657940584917983e-05, 'epoch': 4.75}\n",
      "{'loss': 0.0059, 'grad_norm': 0.2275446355342865, 'learning_rate': 4.6174562464132265e-05, 'epoch': 4.76}\n",
      "{'loss': 0.0445, 'grad_norm': 0.18388204276561737, 'learning_rate': 4.5770957316382725e-05, 'epoch': 4.77}\n",
      "{'loss': 0.0711, 'grad_norm': 0.4629463255405426, 'learning_rate': 4.5368599690740964e-05, 'epoch': 4.78}\n",
      "{'loss': 0.0124, 'grad_norm': 0.26108789443969727, 'learning_rate': 4.496749884331788e-05, 'epoch': 4.79}\n",
      "{'loss': 0.0086, 'grad_norm': 0.1723335087299347, 'learning_rate': 4.45676640013126e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0175, 'grad_norm': 0.8096358776092529, 'learning_rate': 4.416910436280017e-05, 'epoch': 4.81}\n",
      "{'loss': 0.0265, 'grad_norm': 0.19234168529510498, 'learning_rate': 4.377182909652e-05, 'epoch': 4.83}\n",
      "{'loss': 0.0338, 'grad_norm': 0.2576609253883362, 'learning_rate': 4.3375847341664855e-05, 'epoch': 4.84}\n",
      "{'loss': 0.0155, 'grad_norm': 0.37424907088279724, 'learning_rate': 4.298116820767086e-05, 'epoch': 4.85}\n",
      "{'loss': 0.0087, 'grad_norm': 0.1222112849354744, 'learning_rate': 4.258780077400748e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0481, 'grad_norm': 0.30226439237594604, 'learning_rate': 4.219575408996918e-05, 'epoch': 4.87}\n",
      "{'loss': 0.0094, 'grad_norm': 0.14000952243804932, 'learning_rate': 4.180503717446683e-05, 'epoch': 4.88}\n",
      "{'loss': 0.0354, 'grad_norm': 0.3549481928348541, 'learning_rate': 4.141565901582047e-05, 'epoch': 4.89}\n",
      "{'loss': 0.031, 'grad_norm': 0.2821269631385803, 'learning_rate': 4.1027628571552445e-05, 'epoch': 4.9}\n",
      "{'loss': 0.0294, 'grad_norm': 0.2699538767337799, 'learning_rate': 4.064095476818133e-05, 'epoch': 4.91}\n",
      "{'loss': 0.0767, 'grad_norm': 0.5990216732025146, 'learning_rate': 4.025564650101662e-05, 'epoch': 4.92}\n",
      "{'loss': 0.0246, 'grad_norm': 0.1627192348241806, 'learning_rate': 3.9871712633954084e-05, 'epoch': 4.93}\n",
      "{'loss': 0.0221, 'grad_norm': 0.2026364654302597, 'learning_rate': 3.9489161999271806e-05, 'epoch': 4.94}\n",
      "{'loss': 0.0209, 'grad_norm': 0.23270495235919952, 'learning_rate': 3.910800339742714e-05, 'epoch': 4.95}\n",
      "{'loss': 0.025, 'grad_norm': 0.3266077935695648, 'learning_rate': 3.87282455968541e-05, 'epoch': 4.96}\n",
      "{'loss': 0.009, 'grad_norm': 0.2178659588098526, 'learning_rate': 3.8349897333761605e-05, 'epoch': 4.97}\n",
      "{'loss': 0.0214, 'grad_norm': 0.20812666416168213, 'learning_rate': 3.797296731193282e-05, 'epoch': 4.98}\n",
      "{'loss': 0.0305, 'grad_norm': 0.315010666847229, 'learning_rate': 3.759746420252458e-05, 'epoch': 4.99}\n",
      "{'loss': 0.0097, 'grad_norm': 0.27009910345077515, 'learning_rate': 3.722339664386798e-05, 'epoch': 5.0}\n",
      " 72%|█████████████████████████████▌           | 480/665 [30:15<05:37,  1.82s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.85it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.31it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.38it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  7.03it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.76it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.66it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.34it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.36it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.38it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.43it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.28it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.30it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10779120028018951, 'eval_runtime': 2.5529, 'eval_samples_per_second': 12.535, 'eval_steps_per_second': 6.267, 'epoch': 5.0}\n",
      " 72%|█████████████████████████████▌           | 480/665 [30:18<05:37,  1.82s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.49it/s]\u001b[A\n",
      "{'loss': 0.0467, 'grad_norm': 0.23085178434848785, 'learning_rate': 3.685077324126992e-05, 'epoch': 5.01}\n",
      "{'loss': 0.099, 'grad_norm': 0.28361281752586365, 'learning_rate': 3.6479602566814785e-05, 'epoch': 5.02}\n",
      "{'loss': 0.106, 'grad_norm': 0.24959105253219604, 'learning_rate': 3.6109893159167464e-05, 'epoch': 5.03}\n",
      "{'loss': 0.0311, 'grad_norm': 0.15269404649734497, 'learning_rate': 3.574165352337684e-05, 'epoch': 5.04}\n",
      "{'loss': 0.0398, 'grad_norm': 0.28798744082450867, 'learning_rate': 3.5374892130680145e-05, 'epoch': 5.05}\n",
      "{'loss': 0.1472, 'grad_norm': 0.4799691140651703, 'learning_rate': 3.500961741830821e-05, 'epoch': 5.06}\n",
      "{'loss': 0.0763, 'grad_norm': 0.2924298346042633, 'learning_rate': 3.4645837789291044e-05, 'epoch': 5.07}\n",
      "{'loss': 0.0244, 'grad_norm': 0.1681932508945465, 'learning_rate': 3.4283561612264855e-05, 'epoch': 5.08}\n",
      "{'loss': 0.0354, 'grad_norm': 0.3273082971572876, 'learning_rate': 3.392279722127948e-05, 'epoch': 5.09}\n",
      "{'loss': 0.0123, 'grad_norm': 0.09019184857606888, 'learning_rate': 3.356355291560646e-05, 'epoch': 5.1}\n",
      "{'loss': 0.0086, 'grad_norm': 0.1454552859067917, 'learning_rate': 3.3205836959548296e-05, 'epoch': 5.11}\n",
      "{'loss': 0.0184, 'grad_norm': 0.2024359405040741, 'learning_rate': 3.2849657582248314e-05, 'epoch': 5.13}\n",
      "{'loss': 0.0071, 'grad_norm': 0.10395486652851105, 'learning_rate': 3.2495022977501297e-05, 'epoch': 5.14}\n",
      "{'loss': 0.0242, 'grad_norm': 0.30202385783195496, 'learning_rate': 3.2141941303565024e-05, 'epoch': 5.15}\n",
      "{'loss': 0.0032, 'grad_norm': 0.10365016758441925, 'learning_rate': 3.179042068297255e-05, 'epoch': 5.16}\n",
      "{'loss': 0.0037, 'grad_norm': 0.08500368893146515, 'learning_rate': 3.144046920234553e-05, 'epoch': 5.17}\n",
      "{'loss': 0.0071, 'grad_norm': 0.1870850771665573, 'learning_rate': 3.109209491220793e-05, 'epoch': 5.18}\n",
      "{'loss': 0.0118, 'grad_norm': 0.10684562474489212, 'learning_rate': 3.074530582680084e-05, 'epoch': 5.19}\n",
      "{'loss': 0.0032, 'grad_norm': 0.08629824221134186, 'learning_rate': 3.040010992389847e-05, 'epoch': 5.2}\n",
      "{'loss': 0.0245, 'grad_norm': 0.1784118115901947, 'learning_rate': 3.0056515144624208e-05, 'epoch': 5.21}\n",
      "{'loss': 0.0126, 'grad_norm': 0.14145460724830627, 'learning_rate': 2.971452939326802e-05, 'epoch': 5.22}\n",
      "{'loss': 0.0186, 'grad_norm': 0.18983863294124603, 'learning_rate': 2.93741605371049e-05, 'epoch': 5.23}\n",
      "{'loss': 0.0062, 'grad_norm': 0.10769616812467575, 'learning_rate': 2.9035416406213522e-05, 'epoch': 5.24}\n",
      "{'loss': 0.0305, 'grad_norm': 0.19638532400131226, 'learning_rate': 2.8698304793296303e-05, 'epoch': 5.25}\n",
      "{'loss': 0.0092, 'grad_norm': 0.14667430520057678, 'learning_rate': 2.8362833453500104e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0256, 'grad_norm': 0.20486676692962646, 'learning_rate': 2.8029010104237785e-05, 'epoch': 5.27}\n",
      "{'loss': 0.0159, 'grad_norm': 0.1650482416152954, 'learning_rate': 2.7696842425010806e-05, 'epoch': 5.28}\n",
      "{'loss': 0.0042, 'grad_norm': 0.0945708155632019, 'learning_rate': 2.7366338057232312e-05, 'epoch': 5.29}\n",
      "{'loss': 0.0077, 'grad_norm': 0.1659442037343979, 'learning_rate': 2.7037504604051545e-05, 'epoch': 5.3}\n",
      "{'loss': 0.0112, 'grad_norm': 0.13593937456607819, 'learning_rate': 2.6710349630178955e-05, 'epoch': 5.31}\n",
      "{'loss': 0.0142, 'grad_norm': 0.1714961677789688, 'learning_rate': 2.638488066171201e-05, 'epoch': 5.32}\n",
      "{'loss': 0.0287, 'grad_norm': 0.1905115693807602, 'learning_rate': 2.6061105185962198e-05, 'epoch': 5.33}\n",
      "{'loss': 0.0082, 'grad_norm': 0.13771063089370728, 'learning_rate': 2.5739030651282712e-05, 'epoch': 5.34}\n",
      "{'loss': 0.0118, 'grad_norm': 0.18893755972385406, 'learning_rate': 2.5418664466897147e-05, 'epoch': 5.36}\n",
      "{'loss': 0.0101, 'grad_norm': 0.10616344213485718, 'learning_rate': 2.5100014002729034e-05, 'epoch': 5.37}\n",
      "{'loss': 0.0082, 'grad_norm': 0.1950775384902954, 'learning_rate': 2.4783086589232295e-05, 'epoch': 5.38}\n",
      "{'loss': 0.0109, 'grad_norm': 0.16496436297893524, 'learning_rate': 2.446788951722262e-05, 'epoch': 5.39}\n",
      "{'loss': 0.012, 'grad_norm': 0.1785796731710434, 'learning_rate': 2.415443003770972e-05, 'epoch': 5.4}\n",
      "{'loss': 0.0079, 'grad_norm': 0.2401672601699829, 'learning_rate': 2.3842715361730518e-05, 'epoch': 5.41}\n",
      "{'loss': 0.0398, 'grad_norm': 0.3590634763240814, 'learning_rate': 2.3532752660183366e-05, 'epoch': 5.42}\n",
      "{'loss': 0.0058, 'grad_norm': 0.12971606850624084, 'learning_rate': 2.3224549063662927e-05, 'epoch': 5.43}\n",
      "{'loss': 0.0177, 'grad_norm': 0.1874096393585205, 'learning_rate': 2.291811166229615e-05, 'epoch': 5.44}\n",
      "{'loss': 0.0043, 'grad_norm': 0.12702982127666473, 'learning_rate': 2.2613447505579344e-05, 'epoch': 5.45}\n",
      "{'loss': 0.0273, 'grad_norm': 0.23585708439350128, 'learning_rate': 2.2310563602215795e-05, 'epoch': 5.46}\n",
      "{'loss': 0.0312, 'grad_norm': 0.25553005933761597, 'learning_rate': 2.2009466919954648e-05, 'epoch': 5.47}\n",
      "{'loss': 0.0095, 'grad_norm': 0.18987174332141876, 'learning_rate': 2.171016438543059e-05, 'epoch': 5.48}\n",
      "{'loss': 0.0264, 'grad_norm': 0.19189319014549255, 'learning_rate': 2.1412662884004453e-05, 'epoch': 5.49}\n",
      "{'loss': 0.0063, 'grad_norm': 0.1559721678495407, 'learning_rate': 2.1116969259605e-05, 'epoch': 5.5}\n",
      " 79%|████████████████████████████████▌        | 528/665 [32:09<04:31,  1.99s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.01it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.34it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.40it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  7.03it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.77it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.67it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.40it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.40it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.45it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:02<00:00,  4.27it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  4.79it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  5.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1102668046951294, 'eval_runtime': 2.7808, 'eval_samples_per_second': 11.507, 'eval_steps_per_second': 5.754, 'epoch': 5.5}\n",
      " 79%|████████████████████████████████▌        | 528/665 [32:12<04:31,  1.99s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  5.63it/s]\u001b[A\n",
      "{'loss': 0.0412, 'grad_norm': 0.26413458585739136, 'learning_rate': 2.082309031457118e-05, 'epoch': 5.51}\n",
      "{'loss': 0.0055, 'grad_norm': 0.18415983021259308, 'learning_rate': 2.0531032809495887e-05, 'epoch': 5.52}\n",
      "{'loss': 0.0168, 'grad_norm': 0.15600886940956116, 'learning_rate': 2.0240803463070425e-05, 'epoch': 5.53}\n",
      "{'loss': 0.0034, 'grad_norm': 0.16005270183086395, 'learning_rate': 1.9952408951929813e-05, 'epoch': 5.54}\n",
      "{'loss': 0.0058, 'grad_norm': 0.13720999658107758, 'learning_rate': 1.966585591049921e-05, 'epoch': 5.55}\n",
      "{'loss': 0.0181, 'grad_norm': 0.13247188925743103, 'learning_rate': 1.9381150930841463e-05, 'epoch': 5.56}\n",
      "{'loss': 0.0081, 'grad_norm': 0.1781330406665802, 'learning_rate': 1.9098300562505266e-05, 'epoch': 5.57}\n",
      "{'loss': 0.0039, 'grad_norm': 0.07804276794195175, 'learning_rate': 1.8817311312374564e-05, 'epoch': 5.58}\n",
      "{'loss': 0.041, 'grad_norm': 0.26729679107666016, 'learning_rate': 1.8538189644518877e-05, 'epoch': 5.6}\n",
      "{'loss': 0.0236, 'grad_norm': 0.20768192410469055, 'learning_rate': 1.8260941980044566e-05, 'epoch': 5.61}\n",
      "{'loss': 0.0302, 'grad_norm': 0.3050602078437805, 'learning_rate': 1.798557469694715e-05, 'epoch': 5.62}\n",
      "{'loss': 0.0063, 'grad_norm': 0.2559663951396942, 'learning_rate': 1.771209412996455e-05, 'epoch': 5.63}\n",
      "{'loss': 0.0069, 'grad_norm': 0.15824724733829498, 'learning_rate': 1.744050657043137e-05, 'epoch': 5.64}\n",
      "{'loss': 0.0175, 'grad_norm': 0.19043754041194916, 'learning_rate': 1.7170818266134236e-05, 'epoch': 5.65}\n",
      "{'loss': 0.0117, 'grad_norm': 0.2674832046031952, 'learning_rate': 1.6903035421167868e-05, 'epoch': 5.66}\n",
      "{'loss': 0.0039, 'grad_norm': 0.2965669631958008, 'learning_rate': 1.663716419579263e-05, 'epoch': 5.67}\n",
      "{'loss': 0.0042, 'grad_norm': 0.12180633097887039, 'learning_rate': 1.6373210706292618e-05, 'epoch': 5.68}\n",
      "{'loss': 0.0129, 'grad_norm': 0.22366109490394592, 'learning_rate': 1.6111181024835e-05, 'epoch': 5.69}\n",
      "{'loss': 0.006, 'grad_norm': 0.14179055392742157, 'learning_rate': 1.5851081179330373e-05, 'epoch': 5.7}\n",
      "{'loss': 0.0267, 'grad_norm': 0.29701435565948486, 'learning_rate': 1.5592917153294028e-05, 'epoch': 5.71}\n",
      "{'loss': 0.0108, 'grad_norm': 0.19584551453590393, 'learning_rate': 1.5336694885708436e-05, 'epoch': 5.72}\n",
      "{'loss': 0.0077, 'grad_norm': 0.1838192641735077, 'learning_rate': 1.5082420270886377e-05, 'epoch': 5.73}\n",
      "{'loss': 0.0123, 'grad_norm': 0.15637710690498352, 'learning_rate': 1.4830099158335563e-05, 'epoch': 5.74}\n",
      "{'loss': 0.0088, 'grad_norm': 0.2186667025089264, 'learning_rate': 1.4579737352624046e-05, 'epoch': 5.75}\n",
      "{'loss': 0.0095, 'grad_norm': 0.1570071280002594, 'learning_rate': 1.4331340613246591e-05, 'epoch': 5.76}\n",
      "{'loss': 0.0127, 'grad_norm': 0.22761619091033936, 'learning_rate': 1.4084914654492177e-05, 'epoch': 5.77}\n",
      "{'loss': 0.0201, 'grad_norm': 0.13440339267253876, 'learning_rate': 1.3840465145312698e-05, 'epoch': 5.78}\n",
      "{'loss': 0.0154, 'grad_norm': 0.17838482558727264, 'learning_rate': 1.3597997709192378e-05, 'epoch': 5.79}\n",
      "{'loss': 0.0073, 'grad_norm': 0.4607960879802704, 'learning_rate': 1.3357517924018481e-05, 'epoch': 5.8}\n",
      "{'loss': 0.0288, 'grad_norm': 0.24314998090267181, 'learning_rate': 1.3119031321953013e-05, 'epoch': 5.81}\n",
      "{'loss': 0.0028, 'grad_norm': 0.07393177598714828, 'learning_rate': 1.28825433893054e-05, 'epoch': 5.83}\n",
      "{'loss': 0.0451, 'grad_norm': 0.6858656406402588, 'learning_rate': 1.26480595664063e-05, 'epoch': 5.84}\n",
      "{'loss': 0.0502, 'grad_norm': 0.303724080324173, 'learning_rate': 1.2415585247482498e-05, 'epoch': 5.85}\n",
      "{'loss': 0.0759, 'grad_norm': 0.2552323043346405, 'learning_rate': 1.218512578053268e-05, 'epoch': 5.86}\n",
      "{'loss': 0.0139, 'grad_norm': 0.22759173810482025, 'learning_rate': 1.1956686467204658e-05, 'epoch': 5.87}\n",
      "{'loss': 0.0049, 'grad_norm': 0.12123020738363266, 'learning_rate': 1.173027256267304e-05, 'epoch': 5.88}\n",
      "{'loss': 0.0047, 'grad_norm': 0.11509605497121811, 'learning_rate': 1.15058892755187e-05, 'epoch': 5.89}\n",
      "{'loss': 0.0057, 'grad_norm': 0.09839969128370285, 'learning_rate': 1.128354176760873e-05, 'epoch': 5.9}\n",
      "{'loss': 0.0056, 'grad_norm': 0.12627629935741425, 'learning_rate': 1.1063235153977757e-05, 'epoch': 5.91}\n",
      "{'loss': 0.0238, 'grad_norm': 0.20127542316913605, 'learning_rate': 1.0844974502710293e-05, 'epoch': 5.92}\n",
      "{'loss': 0.0107, 'grad_norm': 0.14996179938316345, 'learning_rate': 1.0628764834824123e-05, 'epoch': 5.93}\n",
      "{'loss': 0.0114, 'grad_norm': 0.18249185383319855, 'learning_rate': 1.0414611124154805e-05, 'epoch': 5.94}\n",
      "{'loss': 0.0109, 'grad_norm': 0.2448299527168274, 'learning_rate': 1.0202518297241237e-05, 'epoch': 5.95}\n",
      "{'loss': 0.0113, 'grad_norm': 0.11806205660104752, 'learning_rate': 9.99249123321232e-06, 'epoch': 5.96}\n",
      "{'loss': 0.0075, 'grad_norm': 0.1895468831062317, 'learning_rate': 9.78453476367479e-06, 'epoch': 5.97}\n",
      "{'loss': 0.06, 'grad_norm': 0.39817923307418823, 'learning_rate': 9.578653672601967e-06, 'epoch': 5.98}\n",
      "{'loss': 0.0037, 'grad_norm': 0.10129789263010025, 'learning_rate': 9.374852696223669e-06, 'epoch': 5.99}\n",
      "{'loss': 0.0042, 'grad_norm': 0.2372794896364212, 'learning_rate': 9.173136522917457e-06, 'epoch': 6.0}\n",
      " 87%|███████████████████████████████████▌     | 576/665 [33:47<02:40,  1.80s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.03it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.32it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.40it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  7.03it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.77it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.67it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  5.43it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:01,  5.69it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  5.90it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.08it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.13it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1093740463256836, 'eval_runtime': 2.637, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 6.067, 'epoch': 6.0}\n",
      " 87%|███████████████████████████████████▌     | 576/665 [33:50<02:40,  1.80s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "{'loss': 0.0662, 'grad_norm': 0.2659040689468384, 'learning_rate': 8.973509793100577e-06, 'epoch': 6.01}\n",
      "{'loss': 0.0925, 'grad_norm': 0.23917555809020996, 'learning_rate': 8.7759770991233e-06, 'epoch': 6.02}\n",
      "{'loss': 0.0716, 'grad_norm': 0.27712294459342957, 'learning_rate': 8.580542985163254e-06, 'epoch': 6.03}\n",
      "{'loss': 0.1288, 'grad_norm': 0.3626716732978821, 'learning_rate': 8.387211947120898e-06, 'epoch': 6.04}\n",
      "{'loss': 0.0418, 'grad_norm': 0.2541711628437042, 'learning_rate': 8.195988432516078e-06, 'epoch': 6.05}\n",
      "{'loss': 0.0451, 'grad_norm': 0.250559002161026, 'learning_rate': 8.006876840385747e-06, 'epoch': 6.06}\n",
      "{'loss': 0.0395, 'grad_norm': 0.23251517117023468, 'learning_rate': 7.819881521182714e-06, 'epoch': 6.07}\n",
      "{'loss': 0.003, 'grad_norm': 0.0831698551774025, 'learning_rate': 7.635006776675647e-06, 'epoch': 6.08}\n",
      "{'loss': 0.0022, 'grad_norm': 0.035841092467308044, 'learning_rate': 7.452256859849993e-06, 'epoch': 6.09}\n",
      "{'loss': 0.0224, 'grad_norm': 0.17009788751602173, 'learning_rate': 7.27163597481022e-06, 'epoch': 6.1}\n",
      "{'loss': 0.0068, 'grad_norm': 0.09892094135284424, 'learning_rate': 7.093148276683137e-06, 'epoch': 6.11}\n",
      "{'loss': 0.0066, 'grad_norm': 0.127412810921669, 'learning_rate': 6.916797871522207e-06, 'epoch': 6.13}\n",
      "{'loss': 0.0223, 'grad_norm': 0.22128362953662872, 'learning_rate': 6.742588816213169e-06, 'epoch': 6.14}\n",
      "{'loss': 0.0186, 'grad_norm': 0.13619059324264526, 'learning_rate': 6.570525118380644e-06, 'epoch': 6.15}\n",
      "{'loss': 0.0119, 'grad_norm': 0.10235337167978287, 'learning_rate': 6.4006107362960195e-06, 'epoch': 6.16}\n",
      "{'loss': 0.005, 'grad_norm': 0.09861697256565094, 'learning_rate': 6.232849578786315e-06, 'epoch': 6.17}\n",
      "{'loss': 0.0229, 'grad_norm': 0.1817595213651657, 'learning_rate': 6.067245505144314e-06, 'epoch': 6.18}\n",
      "{'loss': 0.0601, 'grad_norm': 0.2702574133872986, 'learning_rate': 5.903802325039754e-06, 'epoch': 6.19}\n",
      "{'loss': 0.0088, 'grad_norm': 0.13104209303855896, 'learning_rate': 5.742523798431732e-06, 'epoch': 6.2}\n",
      "{'loss': 0.0217, 'grad_norm': 0.14167062938213348, 'learning_rate': 5.583413635482082e-06, 'epoch': 6.21}\n",
      "{'loss': 0.0026, 'grad_norm': 0.07545674592256546, 'learning_rate': 5.426475496470207e-06, 'epoch': 6.22}\n",
      "{'loss': 0.0162, 'grad_norm': 0.1286986917257309, 'learning_rate': 5.271712991708744e-06, 'epoch': 6.23}\n",
      "{'loss': 0.0225, 'grad_norm': 0.12368122488260269, 'learning_rate': 5.119129681460499e-06, 'epoch': 6.24}\n",
      "{'loss': 0.0076, 'grad_norm': 0.14686693251132965, 'learning_rate': 4.968729075856682e-06, 'epoch': 6.25}\n",
      "{'loss': 0.0126, 'grad_norm': 0.14126542210578918, 'learning_rate': 4.82051463481602e-06, 'epoch': 6.26}\n",
      "{'loss': 0.0045, 'grad_norm': 0.0874243900179863, 'learning_rate': 4.6744897679651955e-06, 'epoch': 6.27}\n",
      "{'loss': 0.0125, 'grad_norm': 0.09672308713197708, 'learning_rate': 4.530657834560459e-06, 'epoch': 6.28}\n",
      "{'loss': 0.0038, 'grad_norm': 0.09219139814376831, 'learning_rate': 4.389022143410282e-06, 'epoch': 6.29}\n",
      "{'loss': 0.0071, 'grad_norm': 0.14855432510375977, 'learning_rate': 4.249585952799307e-06, 'epoch': 6.3}\n",
      "{'loss': 0.0109, 'grad_norm': 0.10959924757480621, 'learning_rate': 4.112352470413328e-06, 'epoch': 6.31}\n",
      "{'loss': 0.0144, 'grad_norm': 0.14376789331436157, 'learning_rate': 3.977324853265529e-06, 'epoch': 6.32}\n",
      "{'loss': 0.0079, 'grad_norm': 0.0691186785697937, 'learning_rate': 3.8445062076238884e-06, 'epoch': 6.33}\n",
      "{'loss': 0.0024, 'grad_norm': 0.057387687265872955, 'learning_rate': 3.71389958893964e-06, 'epoch': 6.34}\n",
      "{'loss': 0.0014, 'grad_norm': 0.030423922464251518, 'learning_rate': 3.5855080017770828e-06, 'epoch': 6.36}\n",
      "{'loss': 0.0161, 'grad_norm': 0.1889125406742096, 'learning_rate': 3.459334399744374e-06, 'epoch': 6.37}\n",
      "{'loss': 0.0036, 'grad_norm': 0.09965778887271881, 'learning_rate': 3.3353816854256203e-06, 'epoch': 6.38}\n",
      "{'loss': 0.0044, 'grad_norm': 0.0895664170384407, 'learning_rate': 3.213652710314119e-06, 'epoch': 6.39}\n",
      "{'loss': 0.0036, 'grad_norm': 0.08582504093647003, 'learning_rate': 3.094150274746754e-06, 'epoch': 6.4}\n",
      "{'loss': 0.0121, 'grad_norm': 0.1401916742324829, 'learning_rate': 2.976877127839528e-06, 'epoch': 6.41}\n",
      "{'loss': 0.0039, 'grad_norm': 0.10425297170877457, 'learning_rate': 2.861835967424409e-06, 'epoch': 6.42}\n",
      "{'loss': 0.0071, 'grad_norm': 0.15732115507125854, 'learning_rate': 2.7490294399871496e-06, 'epoch': 6.43}\n",
      "{'loss': 0.0055, 'grad_norm': 0.09381498396396637, 'learning_rate': 2.638460140606547e-06, 'epoch': 6.44}\n",
      "{'loss': 0.0099, 'grad_norm': 0.19642436504364014, 'learning_rate': 2.530130612894621e-06, 'epoch': 6.45}\n",
      "{'loss': 0.0229, 'grad_norm': 0.1414458155632019, 'learning_rate': 2.4240433489381433e-06, 'epoch': 6.46}\n",
      "{'loss': 0.0207, 'grad_norm': 0.14939771592617035, 'learning_rate': 2.3202007892413447e-06, 'epoch': 6.47}\n",
      "{'loss': 0.0287, 'grad_norm': 0.1818290799856186, 'learning_rate': 2.21860532266972e-06, 'epoch': 6.48}\n",
      "{'loss': 0.0025, 'grad_norm': 0.0598619282245636, 'learning_rate': 2.1192592863950813e-06, 'epoch': 6.49}\n",
      "{'loss': 0.0096, 'grad_norm': 0.10427795350551605, 'learning_rate': 2.0221649658418328e-06, 'epoch': 6.5}\n",
      " 94%|██████████████████████████████████████▍  | 624/665 [35:40<01:21,  1.99s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.02it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.35it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:02,  4.68it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:01<00:01,  5.07it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:01<00:01,  5.41it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  5.68it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  5.74it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:01,  5.93it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.08it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.21it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:02<00:00,  6.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.20it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.11076737940311432, 'eval_runtime': 2.8006, 'eval_samples_per_second': 11.426, 'eval_steps_per_second': 5.713, 'epoch': 6.5}\n",
      " 94%|██████████████████████████████████████▍  | 624/665 [35:43<01:21,  1.99s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.45it/s]\u001b[A\n",
      "{'loss': 0.0043, 'grad_norm': 0.10880671441555023, 'learning_rate': 1.9273245946343165e-06, 'epoch': 6.51}\n",
      "{'loss': 0.0093, 'grad_norm': 0.12140055745840073, 'learning_rate': 1.83474035454555e-06, 'epoch': 6.52}\n",
      "{'loss': 0.0077, 'grad_norm': 0.15060566365718842, 'learning_rate': 1.74441437544689e-06, 'epoch': 6.53}\n",
      "{'loss': 0.0084, 'grad_norm': 0.09517170488834381, 'learning_rate': 1.656348735259128e-06, 'epoch': 6.54}\n",
      "{'loss': 0.0037, 'grad_norm': 0.06761367619037628, 'learning_rate': 1.5705454599047153e-06, 'epoch': 6.55}\n",
      "{'loss': 0.0103, 'grad_norm': 0.07165122032165527, 'learning_rate': 1.4870065232610475e-06, 'epoch': 6.56}\n",
      "{'loss': 0.0617, 'grad_norm': 0.21163564920425415, 'learning_rate': 1.4057338471151427e-06, 'epoch': 6.57}\n",
      "{'loss': 0.0156, 'grad_norm': 0.12065557390451431, 'learning_rate': 1.3267293011194226e-06, 'epoch': 6.58}\n",
      "{'loss': 0.0131, 'grad_norm': 0.14727497100830078, 'learning_rate': 1.249994702748669e-06, 'epoch': 6.6}\n",
      "{'loss': 0.0143, 'grad_norm': 0.15636877715587616, 'learning_rate': 1.1755318172582353e-06, 'epoch': 6.61}\n",
      "{'loss': 0.0152, 'grad_norm': 0.12418056279420853, 'learning_rate': 1.103342357643422e-06, 'epoch': 6.62}\n",
      "{'loss': 0.0105, 'grad_norm': 0.1835152953863144, 'learning_rate': 1.0334279846001106e-06, 'epoch': 6.63}\n",
      "{'loss': 0.0023, 'grad_norm': 0.05765455961227417, 'learning_rate': 9.657903064864916e-07, 'epoch': 6.64}\n",
      "{'loss': 0.0063, 'grad_norm': 0.11195774376392365, 'learning_rate': 9.004308792861293e-07, 'epoch': 6.65}\n",
      "{'loss': 0.0036, 'grad_norm': 0.11361780762672424, 'learning_rate': 8.373512065721456e-07, 'epoch': 6.66}\n",
      "{'loss': 0.0065, 'grad_norm': 0.12014225125312805, 'learning_rate': 7.765527394726024e-07, 'epoch': 6.67}\n",
      "{'loss': 0.0163, 'grad_norm': 0.13485923409461975, 'learning_rate': 7.180368766371515e-07, 'epoch': 6.68}\n",
      "{'loss': 0.0118, 'grad_norm': 0.106643445789814, 'learning_rate': 6.618049642048707e-07, 'epoch': 6.69}\n",
      "{'loss': 0.0169, 'grad_norm': 0.12530410289764404, 'learning_rate': 6.078582957732338e-07, 'epoch': 6.7}\n",
      "{'loss': 0.0123, 'grad_norm': 0.12395843863487244, 'learning_rate': 5.561981123684445e-07, 'epoch': 6.71}\n",
      "{'loss': 0.0015, 'grad_norm': 0.05780656635761261, 'learning_rate': 5.068256024167827e-07, 'epoch': 6.72}\n",
      "{'loss': 0.0125, 'grad_norm': 0.10512097179889679, 'learning_rate': 4.5974190171735874e-07, 'epoch': 6.73}\n",
      "{'loss': 0.0196, 'grad_norm': 0.20289795100688934, 'learning_rate': 4.1494809341593443e-07, 'epoch': 6.74}\n",
      "{'loss': 0.0102, 'grad_norm': 0.12210851907730103, 'learning_rate': 3.724452079799989e-07, 'epoch': 6.75}\n",
      "{'loss': 0.0132, 'grad_norm': 0.21068841218948364, 'learning_rate': 3.3223422317509857e-07, 'epoch': 6.76}\n",
      "{'loss': 0.016, 'grad_norm': 0.1796858012676239, 'learning_rate': 2.943160640423215e-07, 'epoch': 6.77}\n",
      "{'loss': 0.0032, 'grad_norm': 0.11044898629188538, 'learning_rate': 2.586916028770259e-07, 'epoch': 6.78}\n",
      "{'loss': 0.0141, 'grad_norm': 0.12484065443277359, 'learning_rate': 2.2536165920873376e-07, 'epoch': 6.79}\n",
      "{'loss': 0.0022, 'grad_norm': 0.05812178924679756, 'learning_rate': 1.9432699978236824e-07, 'epoch': 6.8}\n",
      "{'loss': 0.0126, 'grad_norm': 0.10741834342479706, 'learning_rate': 1.6558833854052324e-07, 'epoch': 6.81}\n",
      "{'loss': 0.0024, 'grad_norm': 0.05087347701191902, 'learning_rate': 1.3914633660706554e-07, 'epoch': 6.83}\n",
      "{'loss': 0.0145, 'grad_norm': 0.1363736391067505, 'learning_rate': 1.150016022719691e-07, 'epoch': 6.84}\n",
      "{'loss': 0.031, 'grad_norm': 0.2171260416507721, 'learning_rate': 9.31546909772596e-08, 'epoch': 6.85}\n",
      "{'loss': 0.0058, 'grad_norm': 0.11027176678180695, 'learning_rate': 7.360610530426915e-08, 'epoch': 6.86}\n",
      "{'loss': 0.0112, 'grad_norm': 0.2292954921722412, 'learning_rate': 5.635629496208994e-08, 'epoch': 6.87}\n",
      "{'loss': 0.0016, 'grad_norm': 0.05327598378062248, 'learning_rate': 4.1405656777193655e-08, 'epoch': 6.88}\n",
      "{'loss': 0.0071, 'grad_norm': 0.13396157324314117, 'learning_rate': 2.8754534684316547e-08, 'epoch': 6.89}\n",
      "{'loss': 0.005, 'grad_norm': 0.1247229129076004, 'learning_rate': 1.8403219718543528e-08, 'epoch': 6.9}\n",
      "{'loss': 0.0131, 'grad_norm': 0.15778858959674835, 'learning_rate': 1.035195000864686e-08, 'epoch': 6.91}\n",
      "{'loss': 0.0086, 'grad_norm': 0.1471760869026184, 'learning_rate': 4.600910771535017e-09, 'epoch': 6.92}\n",
      "{'loss': 0.0099, 'grad_norm': 0.09395218640565872, 'learning_rate': 1.1502343080782574e-09, 'epoch': 6.93}\n",
      "{'train_runtime': 2226.2093, 'train_samples_per_second': 2.39, 'train_steps_per_second': 0.299, 'train_loss': 0.08570530813176858, 'epoch': 6.93}\n",
      "100%|█████████████████████████████████████████| 665/665 [37:06<00:00,  3.35s/it]\n",
      "[2025-10-10 12:53:36,970] [INFO] [axolotl.train.save_trained_model:244] [PID:4136480] [RANK:0] Training completed! Saving trained model to ./out-Qwen2.5-0.5B-Instruct.\u001b[39m\n",
      "[2025-10-10 12:53:37,431] [INFO] [axolotl.train.save_trained_model:341] [PID:4136480] [RANK:0] Model successfully saved to ./out-Qwen2.5-0.5B-Instruct\u001b[39m\n",
      "\u001b[0mCPU times: user 12.1 s, sys: 2.55 s, total: 14.7 s\n",
      "Wall time: 40min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the LoRA/DoRA into the base model (for inference & quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-10 12:53:46,217] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:4140788] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 12:53:46,217] [WARNING] [axolotl.utils.schemas.config.check_sample_packing_wo_flash:482] [PID:4140788] [RANK:0] sample_packing without flash, sdp, xformers or flex attention does not handle cross sample decontamination.\u001b[39m\n",
      "\u001b[33m[2025-10-10 12:53:46,217] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:4140788] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 12:53:46,474] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:4140788] [RANK:0] cuda memory usage baseline: 0.000GB (+0.753GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 12:53:46,494] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:318] [PID:4140788] [RANK:0] loading tokenizer... Qwen/Qwen2.5-0.5B-Instruct\u001b[39m\n",
      "[2025-10-10 12:53:47,108] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:4140788] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-10-10 12:53:47,108] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:321] [PID:4140788] [RANK:0] loading model...\u001b[39m\n",
      "[2025-10-10 12:53:49,194] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:4140788] [RANK:0] cuda memory usage after model load: 0.920GB (+0.263GB cache, +1.165GB misc)\u001b[39m\n",
      "[2025-10-10 12:53:49,204] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:4140788] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 12:53:49,207] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:4140788] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "trainable params: 17,596,416 || all params: 511,629,184 || trainable%: 3.4393\n",
      "[2025-10-10 12:53:50,111] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:4140788] [RANK:0] cuda memory usage after adapters: 0.986GB (+0.838GB cache, +1.240GB misc)\u001b[39m\n",
      "[2025-10-10 12:53:50,534] [INFO] [axolotl.cli.merge_lora.do_merge_lora:31] [PID:4140788] [RANK:0] Running merge of LoRA with base model...\u001b[39m\n",
      "Unloading and merging model: 100%|██████████| 487/487 [00:00<00:00, 5163.53it/s]\n",
      "[2025-10-10 12:53:50,633] [INFO] [axolotl.cli.merge_lora.do_merge_lora:44] [PID:4140788] [RANK:0] Saving merged model to: out-Qwen2.5-0.5B-Instruct/merged...\u001b[39m\n",
      "\u001b[0mCPU times: user 131 ms, sys: 32.1 ms, total: 163 ms\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/axolotl merge-lora {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 12:54:11 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-10 12:54:14 [utils.py:328] non-default args: {'max_model_len': 8192, 'disable_log_stats': True, 'model': 'out-Qwen2.5-0.5B-Instruct/merged'}\n",
      "INFO 10-10 12:54:24 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO 10-10 12:54:24 [__init__.py:1815] Using max model len 8192\n",
      "INFO 10-10 12:54:24 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:25 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:25 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='out-Qwen2.5-0.5B-Instruct/merged', speculative_config=None, tokenizer='out-Qwen2.5-0.5B-Instruct/merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=out-Qwen2.5-0.5B-Instruct/merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[W1010 12:54:28.306153894 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:28 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m WARNING 10-10 12:54:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:28 [gpu_model_runner.py:2338] Starting to load model out-Qwen2.5-0.5B-Instruct/merged...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:28 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:28 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:29 [default_loader.py:268] Loading weights took 0.69 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:30 [gpu_model_runner.py:2392] Model loading took 0.9277 GiB and 0.959574 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:36 [backends.py:539] Using cache directory: /home/oisuomin/.cache/vllm/torch_compile_cache/25357a56fc/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:36 [backends.py:550] Dynamo bytecode transform time: 6.68 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.414 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:40 [monitor.py:34] torch.compile takes 6.68 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:41 [gpu_worker.py:298] Available KV cache memory: 68.98 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:41 [kv_cache_utils.py:864] GPU KV cache size: 6,027,600 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:41 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 735.79x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:01<00\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:43 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.37 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:43 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 0.93 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.37 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=73517804953` to fit into requested memory, or `--kv-cache-memory=81500771328` to fully utilize gpu memory. Current kv cache memory in use is 74067258777 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4140933)\u001b[0;0m INFO 10-10 12:54:43 [core.py:218] init engine (profile, create kv cache, warmup model) took 13.75 seconds\n",
      "INFO 10-10 12:54:44 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-10 12:54:44 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Adding requests: 100%|███████████████████████| 377/377 [00:01<00:00, 298.93it/s]\n",
      "Processed prompts: 100%|█| 377/377 [00:11<00:00, 33.62it/s, est. speed input: 82\n",
      "Errors: 1 out of 377 records (0.27%)\n",
      "ERROR 10-10 12:54:57 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n",
      "| language   | field     |   mean |   size |\n",
      "|------------|-----------|--------|--------|\n",
      "| en         | alt_title | 0.8741 |    135 |\n",
      "| en         | creator   | 0.8176 |    135 |\n",
      "| en         | doi       | 0.9630 |    135 |\n",
      "| en         | e-isbn    | 0.8938 |    135 |\n",
      "| en         | e-issn    | 0.9407 |    135 |\n",
      "| en         | language  | 0.9852 |    135 |\n",
      "| en         | p-isbn    | 0.9111 |    135 |\n",
      "| en         | p-issn    | 0.9481 |    135 |\n",
      "| en         | publisher | 0.7580 |    135 |\n",
      "| en         | title     | 0.8370 |    135 |\n",
      "| en         | type_coar | 0.8074 |    135 |\n",
      "| en         | year      | 0.9481 |    135 |\n",
      "| fi         | alt_title | 0.8785 |    181 |\n",
      "| fi         | creator   | 0.7410 |    181 |\n",
      "| fi         | doi       | 1.0000 |    181 |\n",
      "| fi         | e-isbn    | 0.9098 |    181 |\n",
      "| fi         | e-issn    | 0.9116 |    181 |\n",
      "| fi         | language  | 0.9945 |    181 |\n",
      "| fi         | p-isbn    | 0.9724 |    181 |\n",
      "| fi         | p-issn    | 0.9834 |    181 |\n",
      "| fi         | publisher | 0.7735 |    181 |\n",
      "| fi         | title     | 0.7072 |    181 |\n",
      "| fi         | type_coar | 0.7072 |    181 |\n",
      "| fi         | year      | 0.8398 |    181 |\n",
      "| se         | alt_title | 1.0000 |      3 |\n",
      "| se         | creator   | 0.6667 |      3 |\n",
      "| se         | doi       | 1.0000 |      3 |\n",
      "| se         | e-isbn    | 1.0000 |      3 |\n",
      "| se         | e-issn    | 1.0000 |      3 |\n",
      "| se         | language  | 1.0000 |      3 |\n",
      "| se         | p-isbn    | 1.0000 |      3 |\n",
      "| se         | p-issn    | 1.0000 |      3 |\n",
      "| se         | publisher | 0.0000 |      3 |\n",
      "| se         | title     | 0.3333 |      3 |\n",
      "| se         | type_coar | 0.3333 |      3 |\n",
      "| se         | year      | 0.6667 |      3 |\n",
      "| sv         | alt_title | 0.7414 |     58 |\n",
      "| sv         | creator   | 0.8064 |     58 |\n",
      "| sv         | doi       | 1.0000 |     58 |\n",
      "| sv         | e-isbn    | 0.8793 |     58 |\n",
      "| sv         | e-issn    | 0.9138 |     58 |\n",
      "| sv         | language  | 1.0000 |     58 |\n",
      "| sv         | p-isbn    | 0.8621 |     58 |\n",
      "| sv         | p-issn    | 0.9483 |     58 |\n",
      "| sv         | publisher | 0.7414 |     58 |\n",
      "| sv         | title     | 0.6897 |     58 |\n",
      "| sv         | type_coar | 0.8793 |     58 |\n",
      "| sv         | year      | 0.8793 |     58 |\n",
      "CPU times: user 267 ms, sys: 79.7 ms, total: 347 ms\n",
      "Wall time: 55.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultfile = f\"../../eval/results-{MODEL_SHORT_NAME.replace('.','_')}.md\"\n",
    "\n",
    "# evaluate using the evaluate-model script, which needs venv with vLLM installed\n",
    "!../dspy/venv/bin/python evaluate-model.py out-{MODEL_SHORT_NAME}/merged axolotl-test.jsonl {resultfile}\n",
    "!cat {resultfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greylitlm-axolotl",
   "language": "python",
   "name": "greylitlm-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
