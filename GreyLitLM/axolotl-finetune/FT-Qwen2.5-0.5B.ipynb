{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Qwen2.5-0.5B-Instruct model using Axolotl framework\n",
    "\n",
    "How to install dependencies (in HPC environment):\n",
    "\n",
    "- load Python and cuDNN modules\n",
    "- create a Python venv and activate it\n",
    "- install dependencies from requirements.txt (e.g. torch)\n",
    "- install Axolotl from git clone (pip won't work, see [this issue](https://github.com/OpenAccess-AI-Collective/axolotl/issues/945)):\n",
    "\n",
    "```\n",
    "git clone git@github.com:OpenAccess-AI-Collective/axolotl.git\n",
    "cd axolotl\n",
    "pip install -e '.[flash-attn,deepspeed]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/appl/easybuild/opt/CUDA/12.6.0\n"
     ]
    }
   ],
   "source": [
    "!printenv CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1224 train records\n",
      "Wrote 377 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'assistant', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"axolotl-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"axolotl-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"axolotl-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: axolotl-train.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "test_datasets:\n",
    "  - path: axolotl-eval.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "#chat_template: chatml\n",
    "\n",
    "adapter: lora\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 8\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-10-10 10:21:49,904] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3628323] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 10:21:49,905] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3628323] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 10:21:50,409] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3628323] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 10:21:51,569] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:3628323] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-10-10 10:21:51,574] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3628323] [RANK:0] Unable to find prepared dataset in last_run_prepared/17cf1644cd1cdae47e6acfaa84bdba96\u001b[39m\n",
      "[2025-10-10 10:21:51,574] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3628323] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 10:21:51,574] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3628323] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 1224 examples [00:00, 17812.25 examples/s]\n",
      "[2025-10-10 10:21:52,451] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3628323] [RANK:0] Loading dataset: axolotl-train.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 10:21:52,488] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3628323] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|█| 1224/1224 [00:05<00:00, 217.67 example\n",
      "[2025-10-10 10:21:58,516] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3628323] [RANK:0] min_input_len: 163\u001b[39m\n",
      "[2025-10-10 10:21:58,516] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3628323] [RANK:0] max_input_len: 8780\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 1224/1224 [00:00<00:00, 2914.72 e\n",
      "\u001b[33m[2025-10-10 10:21:59,594] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:201] [PID:3628323] [RANK:0] Dropped 6 long samples from dataset\u001b[39m\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 1218/1218 [00:00<\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 1218/1218 [00:00<\n",
      "Saving the dataset (1/1 shards): 100%|█| 1218/1218 [00:00<00:00, 10353.10 exampl\n",
      "[2025-10-10 10:22:02,239] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3628323] [RANK:0] Unable to find prepared dataset in last_run_prepared/d1a000016f7de3936e986d6cb1ba7681\u001b[39m\n",
      "[2025-10-10 10:22:02,239] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3628323] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 10:22:02,239] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3628323] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 2563.07 examples/s]\n",
      "[2025-10-10 10:22:02,673] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3628323] [RANK:0] Loading dataset: axolotl-eval.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 10:22:02,701] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3628323] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:03<00:00,  9.30 examples/s]\n",
      "[2025-10-10 10:22:06,617] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3628323] [RANK:0] min_input_len: 389\u001b[39m\n",
      "[2025-10-10 10:22:06,617] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3628323] [RANK:0] max_input_len: 3304\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:02<00:00, 13.07 example\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 32/32 [00:04<00:0\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Saving the dataset (1/1 shards): 100%|██| 32/32 [00:00<00:00, 828.73 examples/s]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 10:23:48,026] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3628323] [RANK:0] gather_len_batches: [382]\u001b[39m\n",
      "[2025-10-10 10:23:48,028] [INFO] [axolotl.utils.data.sft._prepare_standard_dataset:123] [PID:3628323] [RANK:0] Maximum number of steps set at 760\u001b[39m\n",
      "[2025-10-10 10:23:48,676] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:3628323] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[2025-10-10 10:23:58,414] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3628323] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 10:23:58,532] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:3628323] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
      "[2025-10-10 10:23:59,025] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3628323] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n",
      "[2025-10-10 10:24:05,266] [WARNING] [accelerate.utils.other.check_os_kernel:441] [PID:3628323] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[2025-10-10 10:24:26,404] [INFO] [axolotl.train.save_initial_configs:403] [PID:3628323] [RANK:0] Pre-saving adapter config to ./out-Qwen2.5-0.5B-Instruct...\u001b[39m\n",
      "[2025-10-10 10:24:26,406] [INFO] [axolotl.train.save_initial_configs:407] [PID:3628323] [RANK:0] Pre-saving tokenizer to ./out-Qwen2.5-0.5B-Instruct...\u001b[39m\n",
      "[2025-10-10 10:24:26,712] [INFO] [axolotl.train.save_initial_configs:410] [PID:3628323] [RANK:0] Pre-saving model config to ./out-Qwen2.5-0.5B-Instruct...\u001b[39m\n",
      "[2025-10-10 10:24:26,719] [INFO] [axolotl.train.execute_training:225] [PID:3628323] [RANK:0] Starting trainer...\u001b[39m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 10:25:38,602] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3628323] [RANK:0] gather_len_batches: [383]\u001b[39m\n",
      "  0%|                                                   | 0/760 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.73it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.26it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.23it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.95it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.73it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.59it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.29it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.32it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.36it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.08it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.20it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.22it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.1820948123931885, 'eval_runtime': 3.8453, 'eval_samples_per_second': 8.322, 'eval_steps_per_second': 4.161, 'epoch': 0}\n",
      "  0%|                                                   | 0/760 [00:03<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "{'loss': 1.4292, 'grad_norm': 3.8089258670806885, 'learning_rate': 0.0, 'epoch': 0.01}m\u001b[0m\n",
      "  0%|                                         | 1/760 [00:23<4:58:58, 23.63s/it][2025-10-10 10:26:04,682] [INFO] [axolotl.utils.callbacks.log_gpu_memory_usage:107] [PID:3628323] [RANK:0] cuda memory usage while training: 0.995GB (+17.204GB cache, +1.326GB misc)\u001b[39m\n",
      "{'loss': 1.5269, 'grad_norm': 4.134901523590088, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4315, 'grad_norm': 3.3962361812591553, 'learning_rate': 4e-05, 'epoch': 0.03}\n",
      "{'loss': 1.3784, 'grad_norm': 3.9029974937438965, 'learning_rate': 6e-05, 'epoch': 0.04}\n",
      "{'loss': 1.172, 'grad_norm': 2.275164842605591, 'learning_rate': 8e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9115, 'grad_norm': 2.025872230529785, 'learning_rate': 0.0001, 'epoch': 0.06}\n",
      "{'loss': 0.9126, 'grad_norm': 1.996006727218628, 'learning_rate': 0.00012, 'epoch': 0.07}\n",
      "{'loss': 0.7307, 'grad_norm': 1.9187082052230835, 'learning_rate': 0.00014, 'epoch': 0.08}\n",
      "{'loss': 0.5388, 'grad_norm': 1.8367317914962769, 'learning_rate': 0.00016, 'epoch': 0.09}\n",
      "{'loss': 0.4658, 'grad_norm': 1.6806458234786987, 'learning_rate': 0.00018, 'epoch': 0.1}\n",
      "{'loss': 0.4795, 'grad_norm': 1.632903814315796, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.2766, 'grad_norm': 1.3143415451049805, 'learning_rate': 0.00019999912270311375, 'epoch': 0.13}\n",
      "{'loss': 0.3114, 'grad_norm': 1.4714688062667847, 'learning_rate': 0.0001999964908278481, 'epoch': 0.14}\n",
      "{'loss': 0.3913, 'grad_norm': 1.5110872983932495, 'learning_rate': 0.00019999210442038162, 'epoch': 0.15}\n",
      "{'loss': 0.436, 'grad_norm': 1.3655341863632202, 'learning_rate': 0.00019998596355767805, 'epoch': 0.16}\n",
      "{'loss': 0.209, 'grad_norm': 1.357399821281433, 'learning_rate': 0.00019997806834748456, 'epoch': 0.17}\n",
      "{'loss': 0.22, 'grad_norm': 1.216844081878662, 'learning_rate': 0.00019996841892833, 'epoch': 0.18}\n",
      "{'loss': 0.1798, 'grad_norm': 0.90640789270401, 'learning_rate': 0.0001999570154695225, 'epoch': 0.19}\n",
      "{'loss': 0.2776, 'grad_norm': 1.3221975564956665, 'learning_rate': 0.00019994385817114646, 'epoch': 0.2}\n",
      "{'loss': 0.3033, 'grad_norm': 1.4941390752792358, 'learning_rate': 0.00019992894726405893, 'epoch': 0.21}\n",
      "{'loss': 0.1431, 'grad_norm': 1.0335993766784668, 'learning_rate': 0.00019991228300988585, 'epoch': 0.22}\n",
      "{'loss': 0.2378, 'grad_norm': 1.1726435422897339, 'learning_rate': 0.00019989386570101714, 'epoch': 0.23}\n",
      "{'loss': 0.1664, 'grad_norm': 0.917375385761261, 'learning_rate': 0.00019987369566060176, 'epoch': 0.24}\n",
      "{'loss': 0.2039, 'grad_norm': 0.9864804148674011, 'learning_rate': 0.000199851773242542, 'epoch': 0.25}\n",
      "{'loss': 0.1719, 'grad_norm': 0.8077904582023621, 'learning_rate': 0.00019982809883148722, 'epoch': 0.26}\n",
      "{'loss': 0.1821, 'grad_norm': 1.1507560014724731, 'learning_rate': 0.00019980267284282717, 'epoch': 0.27}\n",
      "{'loss': 0.1441, 'grad_norm': 0.980277955532074, 'learning_rate': 0.00019977549572268468, 'epoch': 0.28}\n",
      "{'loss': 0.1896, 'grad_norm': 0.901980996131897, 'learning_rate': 0.00019974656794790775, 'epoch': 0.29}\n",
      "{'loss': 0.1296, 'grad_norm': 0.7313044667243958, 'learning_rate': 0.0001997158900260614, 'epoch': 0.3}\n",
      "{'loss': 0.1563, 'grad_norm': 0.7132395505905151, 'learning_rate': 0.00019968346249541846, 'epoch': 0.31}\n",
      "{'loss': 0.2967, 'grad_norm': 0.8721889853477478, 'learning_rate': 0.00019964928592495045, 'epoch': 0.32}\n",
      "{'loss': 0.1983, 'grad_norm': 1.0617610216140747, 'learning_rate': 0.00019961336091431727, 'epoch': 0.33}\n",
      "{'loss': 0.1303, 'grad_norm': 0.7091017365455627, 'learning_rate': 0.00019957568809385694, 'epoch': 0.34}\n",
      "{'loss': 0.2098, 'grad_norm': 1.3419400453567505, 'learning_rate': 0.0001995362681245744, 'epoch': 0.36}\n",
      "{'loss': 0.1392, 'grad_norm': 0.709459125995636, 'learning_rate': 0.00019949510169813003, 'epoch': 0.37}\n",
      "{'loss': 0.2142, 'grad_norm': 0.9040849208831787, 'learning_rate': 0.00019945218953682734, 'epoch': 0.38}\n",
      "{'loss': 0.1918, 'grad_norm': 0.7194731831550598, 'learning_rate': 0.00019940753239360047, 'epoch': 0.39}\n",
      "{'loss': 0.2282, 'grad_norm': 0.8819513320922852, 'learning_rate': 0.00019936113105200085, 'epoch': 0.4}\n",
      "{'loss': 0.5337, 'grad_norm': 1.2775949239730835, 'learning_rate': 0.00019931298632618356, 'epoch': 0.41}\n",
      "{'loss': 0.2387, 'grad_norm': 0.9731531739234924, 'learning_rate': 0.0001992630990608929, 'epoch': 0.42}\n",
      "{'loss': 0.2651, 'grad_norm': 0.746812105178833, 'learning_rate': 0.0001992114701314478, 'epoch': 0.43}\n",
      "{'loss': 0.1843, 'grad_norm': 0.7536473870277405, 'learning_rate': 0.00019915810044372618, 'epoch': 0.44}\n",
      "{'loss': 0.2336, 'grad_norm': 0.7775750160217285, 'learning_rate': 0.0001991029909341493, 'epoch': 0.45}\n",
      "{'loss': 0.1894, 'grad_norm': 0.8455029129981995, 'learning_rate': 0.00019904614256966512, 'epoch': 0.46}\n",
      "{'loss': 0.1595, 'grad_norm': 0.8409152030944824, 'learning_rate': 0.00019898755634773158, 'epoch': 0.47}\n",
      "{'loss': 0.1118, 'grad_norm': 0.5934069752693176, 'learning_rate': 0.00019892723329629887, 'epoch': 0.48}\n",
      "{'loss': 0.1326, 'grad_norm': 0.5961267352104187, 'learning_rate': 0.0001988651744737914, 'epoch': 0.49}\n",
      "{'loss': 0.3386, 'grad_norm': 0.9456781148910522, 'learning_rate': 0.00019880138096908952, 'epoch': 0.5}\n",
      "  6%|██▋                                       | 48/760 [02:01<24:56,  2.10s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.90it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.29it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.96it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.70it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.60it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.32it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.35it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.39it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.24it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.22it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.15771961212158203, 'eval_runtime': 2.5938, 'eval_samples_per_second': 12.337, 'eval_steps_per_second': 6.169, 'epoch': 0.5}\n",
      "  6%|██▋                                       | 48/760 [02:04<24:56,  2.10s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.42it/s]\u001b[A\n",
      "{'loss': 0.2434, 'grad_norm': 0.824844241142273, 'learning_rate': 0.00019873585390151003, 'epoch': 0.51}\n",
      "{'loss': 0.1056, 'grad_norm': 0.5497947931289673, 'learning_rate': 0.0001986685944207868, 'epoch': 0.52}\n",
      "{'loss': 0.0963, 'grad_norm': 0.577057421207428, 'learning_rate': 0.0001985996037070505, 'epoch': 0.53}\n",
      "{'loss': 0.2066, 'grad_norm': 0.7403032779693604, 'learning_rate': 0.00019852888297080786, 'epoch': 0.54}\n",
      "{'loss': 0.178, 'grad_norm': 0.8194671869277954, 'learning_rate': 0.00019845643345292054, 'epoch': 0.55}\n",
      "{'loss': 0.1227, 'grad_norm': 0.5217846632003784, 'learning_rate': 0.00019838225642458327, 'epoch': 0.56}\n",
      "{'loss': 0.1235, 'grad_norm': 0.7278236746788025, 'learning_rate': 0.00019830635318730154, 'epoch': 0.57}\n",
      "{'loss': 0.3518, 'grad_norm': 0.9519531726837158, 'learning_rate': 0.0001982287250728689, 'epoch': 0.58}\n",
      "{'loss': 0.0868, 'grad_norm': 0.745684027671814, 'learning_rate': 0.0001981493734433433, 'epoch': 0.6}\n",
      "{'loss': 0.2165, 'grad_norm': 0.6499689817428589, 'learning_rate': 0.00019806829969102357, 'epoch': 0.61}\n",
      "{'loss': 0.281, 'grad_norm': 0.7562832832336426, 'learning_rate': 0.0001979855052384247, 'epoch': 0.62}\n",
      "{'loss': 0.1912, 'grad_norm': 0.7346369624137878, 'learning_rate': 0.000197900991538253, 'epoch': 0.63}\n",
      "{'loss': 0.2468, 'grad_norm': 0.8449017405509949, 'learning_rate': 0.00019781476007338058, 'epoch': 0.64}\n",
      "{'loss': 0.1149, 'grad_norm': 0.5338783860206604, 'learning_rate': 0.00019772681235681936, 'epoch': 0.65}\n",
      "{'loss': 0.1304, 'grad_norm': 0.5629681348800659, 'learning_rate': 0.00019763714993169452, 'epoch': 0.66}\n",
      "{'loss': 0.0889, 'grad_norm': 0.5569332838058472, 'learning_rate': 0.00019754577437121733, 'epoch': 0.67}\n",
      "{'loss': 0.0884, 'grad_norm': 0.49444061517715454, 'learning_rate': 0.00019745268727865774, 'epoch': 0.68}\n",
      "{'loss': 0.1584, 'grad_norm': 0.6601732969284058, 'learning_rate': 0.00019735789028731604, 'epoch': 0.69}\n",
      "{'loss': 0.2388, 'grad_norm': 0.837704062461853, 'learning_rate': 0.00019726138506049438, 'epoch': 0.7}\n",
      "{'loss': 0.0907, 'grad_norm': 0.5772764086723328, 'learning_rate': 0.0001971631732914674, 'epoch': 0.71}\n",
      "{'loss': 0.113, 'grad_norm': 0.6105923652648926, 'learning_rate': 0.00019706325670345275, 'epoch': 0.72}\n",
      "{'loss': 0.1105, 'grad_norm': 0.5843368172645569, 'learning_rate': 0.0001969616370495806, 'epoch': 0.73}\n",
      "{'loss': 0.148, 'grad_norm': 0.6556480526924133, 'learning_rate': 0.0001968583161128631, 'epoch': 0.74}\n",
      "{'loss': 0.1893, 'grad_norm': 0.5344923138618469, 'learning_rate': 0.00019675329570616298, 'epoch': 0.75}\n",
      "{'loss': 0.1144, 'grad_norm': 0.4831055700778961, 'learning_rate': 0.00019664657767216176, 'epoch': 0.76}\n",
      "{'loss': 0.1365, 'grad_norm': 0.5581430792808533, 'learning_rate': 0.0001965381638833274, 'epoch': 0.77}\n",
      "{'loss': 0.1676, 'grad_norm': 0.6541418433189392, 'learning_rate': 0.00019642805624188147, 'epoch': 0.78}\n",
      "{'loss': 0.1657, 'grad_norm': 0.668156087398529, 'learning_rate': 0.00019631625667976583, 'epoch': 0.79}\n",
      "{'loss': 0.1302, 'grad_norm': 0.5765981078147888, 'learning_rate': 0.0001962027671586086, 'epoch': 0.8}\n",
      "{'loss': 0.1166, 'grad_norm': 0.8252591490745544, 'learning_rate': 0.00019608758966968988, 'epoch': 0.81}\n",
      "{'loss': 0.2237, 'grad_norm': 0.8661592602729797, 'learning_rate': 0.00019597072623390668, 'epoch': 0.83}\n",
      "{'loss': 0.1839, 'grad_norm': 0.773309588432312, 'learning_rate': 0.0001958521789017376, 'epoch': 0.84}\n",
      "{'loss': 0.0936, 'grad_norm': 0.5216185450553894, 'learning_rate': 0.00019573194975320673, 'epoch': 0.85}\n",
      "{'loss': 0.1373, 'grad_norm': 0.5475468039512634, 'learning_rate': 0.00019561004089784723, 'epoch': 0.86}\n",
      "{'loss': 0.2477, 'grad_norm': 0.9481291770935059, 'learning_rate': 0.00019548645447466431, 'epoch': 0.87}\n",
      "{'loss': 0.1827, 'grad_norm': 0.7263553738594055, 'learning_rate': 0.0001953611926520976, 'epoch': 0.88}\n",
      "{'loss': 0.1178, 'grad_norm': 0.6095473766326904, 'learning_rate': 0.00019523425762798329, 'epoch': 0.89}\n",
      "{'loss': 0.2349, 'grad_norm': 0.8036404848098755, 'learning_rate': 0.00019510565162951537, 'epoch': 0.9}\n",
      "{'loss': 0.1024, 'grad_norm': 0.5719877481460571, 'learning_rate': 0.00019497537691320668, 'epoch': 0.91}\n",
      "{'loss': 0.1098, 'grad_norm': 0.49770981073379517, 'learning_rate': 0.00019484343576484933, 'epoch': 0.92}\n",
      "{'loss': 0.1177, 'grad_norm': 0.6805661916732788, 'learning_rate': 0.00019470983049947444, 'epoch': 0.93}\n",
      "{'loss': 0.0791, 'grad_norm': 0.4403361678123474, 'learning_rate': 0.0001945745634613117, 'epoch': 0.94}\n",
      "{'loss': 0.5315, 'grad_norm': 1.0302762985229492, 'learning_rate': 0.00019443763702374812, 'epoch': 0.95}\n",
      "{'loss': 0.1116, 'grad_norm': 0.48169833421707153, 'learning_rate': 0.00019429905358928646, 'epoch': 0.96}\n",
      "{'loss': 0.1216, 'grad_norm': 0.5498560667037964, 'learning_rate': 0.00019415881558950302, 'epoch': 0.97}\n",
      "{'loss': 0.1457, 'grad_norm': 0.7739796042442322, 'learning_rate': 0.00019401692548500502, 'epoch': 0.98}\n",
      "{'loss': 0.1306, 'grad_norm': 0.5450989007949829, 'learning_rate': 0.00019387338576538744, 'epoch': 0.99}\n",
      "{'loss': 0.1602, 'grad_norm': 0.908982515335083, 'learning_rate': 0.00019372819894918915, 'epoch': 1.0}\n",
      " 13%|█████▎                                    | 96/760 [03:45<23:32,  2.13s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.08it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:02,  5.94it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  5.67it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  5.81it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:01<00:01,  5.85it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.01it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  5.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.15it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.25it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:02<00:00,  6.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.17it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.12837478518486023, 'eval_runtime': 2.7763, 'eval_samples_per_second': 11.526, 'eval_steps_per_second': 5.763, 'epoch': 1.0}\n",
      " 13%|█████▎                                    | 96/760 [03:48<23:32,  2.13s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.38it/s]\u001b[A\n",
      "{'loss': 0.407, 'grad_norm': 0.5299391746520996, 'learning_rate': 0.00019358136758384912, 'epoch': 1.01}\n",
      "{'loss': 0.5898, 'grad_norm': 0.7409521341323853, 'learning_rate': 0.00019343289424566122, 'epoch': 1.02}\n",
      "{'loss': 0.5073, 'grad_norm': 0.6393389105796814, 'learning_rate': 0.00019328278153972947, 'epoch': 1.03}\n",
      "{'loss': 0.3797, 'grad_norm': 0.5426569581031799, 'learning_rate': 0.00019313103209992204, 'epoch': 1.04}\n",
      "{'loss': 0.2153, 'grad_norm': 0.47432732582092285, 'learning_rate': 0.00019297764858882514, 'epoch': 1.05}\n",
      "{'loss': 0.1547, 'grad_norm': 0.5581537485122681, 'learning_rate': 0.00019282263369769633, 'epoch': 1.06}\n",
      "{'loss': 0.1904, 'grad_norm': 0.4780898988246918, 'learning_rate': 0.0001926659901464172, 'epoch': 1.07}\n",
      "{'loss': 0.0984, 'grad_norm': 0.45619744062423706, 'learning_rate': 0.0001925077206834458, 'epoch': 1.08}\n",
      "{'loss': 0.0842, 'grad_norm': 0.3912423849105835, 'learning_rate': 0.00019234782808576824, 'epoch': 1.09}\n",
      "{'loss': 0.0629, 'grad_norm': 0.40937092900276184, 'learning_rate': 0.00019218631515885006, 'epoch': 1.1}\n",
      "{'loss': 0.2398, 'grad_norm': 0.643585205078125, 'learning_rate': 0.00019202318473658705, 'epoch': 1.11}\n",
      "{'loss': 0.0917, 'grad_norm': 0.49466586112976074, 'learning_rate': 0.0001918584396812554, 'epoch': 1.13}\n",
      "{'loss': 0.0804, 'grad_norm': 0.5090150237083435, 'learning_rate': 0.00019169208288346166, 'epoch': 1.14}\n",
      "{'loss': 0.0333, 'grad_norm': 0.31742143630981445, 'learning_rate': 0.00019152411726209176, 'epoch': 1.15}\n",
      "{'loss': 0.1413, 'grad_norm': 0.5290130972862244, 'learning_rate': 0.0001913545457642601, 'epoch': 1.16}\n",
      "{'loss': 0.1225, 'grad_norm': 0.4709445536136627, 'learning_rate': 0.0001911833713652576, 'epoch': 1.17}\n",
      "{'loss': 0.0782, 'grad_norm': 0.4509527385234833, 'learning_rate': 0.00019101059706849957, 'epoch': 1.18}\n",
      "{'loss': 0.092, 'grad_norm': 0.5624393224716187, 'learning_rate': 0.00019083622590547312, 'epoch': 1.19}\n",
      "{'loss': 0.0699, 'grad_norm': 0.348675012588501, 'learning_rate': 0.00019066026093568378, 'epoch': 1.2}\n",
      "{'loss': 0.1244, 'grad_norm': 0.4916304349899292, 'learning_rate': 0.00019048270524660196, 'epoch': 1.21}\n",
      "{'loss': 0.1179, 'grad_norm': 0.5802679061889648, 'learning_rate': 0.00019030356195360874, 'epoch': 1.22}\n",
      "{'loss': 0.0785, 'grad_norm': 0.5915049910545349, 'learning_rate': 0.00019012283419994115, 'epoch': 1.23}\n",
      "{'loss': 0.0628, 'grad_norm': 0.4642912745475769, 'learning_rate': 0.0001899405251566371, 'epoch': 1.24}\n",
      "{'loss': 0.0939, 'grad_norm': 0.6406937837600708, 'learning_rate': 0.00018975663802247976, 'epoch': 1.25}\n",
      "{'loss': 0.0423, 'grad_norm': 0.3494308590888977, 'learning_rate': 0.0001895711760239413, 'epoch': 1.26}\n",
      "{'loss': 0.1354, 'grad_norm': 0.5641918182373047, 'learning_rate': 0.0001893841424151264, 'epoch': 1.27}\n",
      "{'loss': 0.2795, 'grad_norm': 0.8570969700813293, 'learning_rate': 0.0001891955404777151, 'epoch': 1.28}\n",
      "{'loss': 0.054, 'grad_norm': 0.3993394374847412, 'learning_rate': 0.00018900537352090524, 'epoch': 1.29}\n",
      "{'loss': 0.0901, 'grad_norm': 0.5145037174224854, 'learning_rate': 0.00018881364488135448, 'epoch': 1.3}\n",
      "{'loss': 0.0984, 'grad_norm': 0.38860952854156494, 'learning_rate': 0.00018862035792312147, 'epoch': 1.31}\n",
      "{'loss': 0.1332, 'grad_norm': 0.5503110289573669, 'learning_rate': 0.00018842551603760724, 'epoch': 1.32}\n",
      "{'loss': 0.1362, 'grad_norm': 0.6042898893356323, 'learning_rate': 0.00018822912264349534, 'epoch': 1.33}\n",
      "{'loss': 0.1182, 'grad_norm': 0.5214242935180664, 'learning_rate': 0.00018803118118669202, 'epoch': 1.34}\n",
      "{'loss': 0.0457, 'grad_norm': 0.37878936529159546, 'learning_rate': 0.00018783169514026578, 'epoch': 1.36}\n",
      "{'loss': 0.0446, 'grad_norm': 0.36775872111320496, 'learning_rate': 0.00018763066800438636, 'epoch': 1.37}\n",
      "{'loss': 0.0732, 'grad_norm': 0.49241796135902405, 'learning_rate': 0.00018742810330626337, 'epoch': 1.38}\n",
      "{'loss': 0.0704, 'grad_norm': 0.4633628726005554, 'learning_rate': 0.0001872240046000844, 'epoch': 1.39}\n",
      "{'loss': 0.1119, 'grad_norm': 0.4352385699748993, 'learning_rate': 0.0001870183754669526, 'epoch': 1.4}\n",
      "{'loss': 0.1142, 'grad_norm': 0.4822078049182892, 'learning_rate': 0.00018681121951482393, 'epoch': 1.41}\n",
      "{'loss': 0.1411, 'grad_norm': 0.5534571409225464, 'learning_rate': 0.00018660254037844388, 'epoch': 1.42}\n",
      "{'loss': 0.0915, 'grad_norm': 0.4359314739704132, 'learning_rate': 0.00018639234171928353, 'epoch': 1.43}\n",
      "{'loss': 0.0519, 'grad_norm': 0.46617451310157776, 'learning_rate': 0.0001861806272254755, 'epoch': 1.44}\n",
      "{'loss': 0.0861, 'grad_norm': 0.36469608545303345, 'learning_rate': 0.0001859674006117491, 'epoch': 1.45}\n",
      "{'loss': 0.0857, 'grad_norm': 0.5001779198646545, 'learning_rate': 0.00018575266561936523, 'epoch': 1.46}\n",
      "{'loss': 0.1318, 'grad_norm': 0.5568540692329407, 'learning_rate': 0.00018553642601605068, 'epoch': 1.47}\n",
      "{'loss': 0.098, 'grad_norm': 0.49333101511001587, 'learning_rate': 0.00018531868559593204, 'epoch': 1.48}\n",
      "{'loss': 0.0931, 'grad_norm': 0.3836285471916199, 'learning_rate': 0.00018509944817946922, 'epoch': 1.49}\n",
      "{'loss': 0.1017, 'grad_norm': 0.40221989154815674, 'learning_rate': 0.0001848787176133882, 'epoch': 1.5}\n",
      " 19%|███████▊                                 | 144/760 [05:54<21:21,  2.08s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.93it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.14it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.91it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.66it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.56it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.16it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.28it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.30it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.36it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.22it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.19it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1086203008890152, 'eval_runtime': 2.625, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 6.095, 'epoch': 1.5}\n",
      " 19%|███████▊                                 | 144/760 [05:56<21:21,  2.08s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.40it/s]\u001b[A\n",
      "{'loss': 0.13, 'grad_norm': 0.6318656802177429, 'learning_rate': 0.0001846564977706138, 'epoch': 1.51}\n",
      "{'loss': 0.088, 'grad_norm': 0.5472437739372253, 'learning_rate': 0.00018443279255020152, 'epoch': 1.52}\n",
      "{'loss': 0.0912, 'grad_norm': 0.43607035279273987, 'learning_rate': 0.00018420760587726923, 'epoch': 1.53}\n",
      "{'loss': 0.1071, 'grad_norm': 0.5193798542022705, 'learning_rate': 0.0001839809417029283, 'epoch': 1.54}\n",
      "{'loss': 0.1067, 'grad_norm': 0.5528654456138611, 'learning_rate': 0.0001837528040042142, 'epoch': 1.55}\n",
      "{'loss': 0.0585, 'grad_norm': 0.5343217253684998, 'learning_rate': 0.00018352319678401676, 'epoch': 1.56}\n",
      "{'loss': 0.0614, 'grad_norm': 0.5008694529533386, 'learning_rate': 0.00018329212407100994, 'epoch': 1.57}\n",
      "{'loss': 0.0917, 'grad_norm': 0.3959421217441559, 'learning_rate': 0.00018305958991958127, 'epoch': 1.58}\n",
      "{'loss': 0.118, 'grad_norm': 0.48491355776786804, 'learning_rate': 0.00018282559840976042, 'epoch': 1.6}\n",
      "{'loss': 0.0464, 'grad_norm': 0.4123397171497345, 'learning_rate': 0.00018259015364714787, 'epoch': 1.61}\n",
      "{'loss': 0.1265, 'grad_norm': 0.5779452919960022, 'learning_rate': 0.00018235325976284275, 'epoch': 1.62}\n",
      "{'loss': 0.0808, 'grad_norm': 0.3299538791179657, 'learning_rate': 0.00018211492091337042, 'epoch': 1.63}\n",
      "{'loss': 0.1148, 'grad_norm': 0.5745647549629211, 'learning_rate': 0.00018187514128060946, 'epoch': 1.64}\n",
      "{'loss': 0.102, 'grad_norm': 0.44837281107902527, 'learning_rate': 0.00018163392507171842, 'epoch': 1.65}\n",
      "{'loss': 0.0712, 'grad_norm': 0.5895638465881348, 'learning_rate': 0.00018139127651906184, 'epoch': 1.66}\n",
      "{'loss': 0.0491, 'grad_norm': 0.3603937327861786, 'learning_rate': 0.00018114719988013612, 'epoch': 1.67}\n",
      "{'loss': 0.0913, 'grad_norm': 0.5459515452384949, 'learning_rate': 0.00018090169943749476, 'epoch': 1.68}\n",
      "{'loss': 0.0609, 'grad_norm': 0.3798031210899353, 'learning_rate': 0.00018065477949867327, 'epoch': 1.69}\n",
      "{'loss': 0.051, 'grad_norm': 0.4233115613460541, 'learning_rate': 0.00018040644439611348, 'epoch': 1.7}\n",
      "{'loss': 0.0405, 'grad_norm': 0.36653462052345276, 'learning_rate': 0.00018015669848708767, 'epoch': 1.71}\n",
      "{'loss': 0.1333, 'grad_norm': 0.7382522225379944, 'learning_rate': 0.00017990554615362198, 'epoch': 1.72}\n",
      "{'loss': 0.1013, 'grad_norm': 0.4894152581691742, 'learning_rate': 0.00017965299180241963, 'epoch': 1.73}\n",
      "{'loss': 0.1249, 'grad_norm': 0.49108773469924927, 'learning_rate': 0.00017939903986478355, 'epoch': 1.74}\n",
      "{'loss': 0.3108, 'grad_norm': 0.8734948039054871, 'learning_rate': 0.0001791436947965386, 'epoch': 1.75}\n",
      "{'loss': 0.0739, 'grad_norm': 0.4712584912776947, 'learning_rate': 0.00017888696107795342, 'epoch': 1.76}\n",
      "{'loss': 0.137, 'grad_norm': 0.703666090965271, 'learning_rate': 0.00017862884321366188, 'epoch': 1.77}\n",
      "{'loss': 0.0945, 'grad_norm': 0.643358051776886, 'learning_rate': 0.000178369345732584, 'epoch': 1.78}\n",
      "{'loss': 0.0676, 'grad_norm': 0.4752543568611145, 'learning_rate': 0.0001781084731878463, 'epoch': 1.79}\n",
      "{'loss': 0.1101, 'grad_norm': 0.5616304874420166, 'learning_rate': 0.00017784623015670238, 'epoch': 1.8}\n",
      "{'loss': 0.0738, 'grad_norm': 0.4237021207809448, 'learning_rate': 0.00017758262124045195, 'epoch': 1.81}\n",
      "{'loss': 0.0755, 'grad_norm': 0.4441133737564087, 'learning_rate': 0.00017731765106436073, 'epoch': 1.83}\n",
      "{'loss': 0.0871, 'grad_norm': 0.503032922744751, 'learning_rate': 0.00017705132427757895, 'epoch': 1.84}\n",
      "{'loss': 0.1055, 'grad_norm': 0.7550344467163086, 'learning_rate': 0.00017678364555305978, 'epoch': 1.85}\n",
      "{'loss': 0.056, 'grad_norm': 0.4479670226573944, 'learning_rate': 0.00017651461958747745, 'epoch': 1.86}\n",
      "{'loss': 0.1495, 'grad_norm': 0.4944733679294586, 'learning_rate': 0.0001762442511011448, 'epoch': 1.87}\n",
      "{'loss': 0.0448, 'grad_norm': 0.37694019079208374, 'learning_rate': 0.00017597254483793048, 'epoch': 1.88}\n",
      "{'loss': 0.3694, 'grad_norm': 0.8170204758644104, 'learning_rate': 0.00017569950556517566, 'epoch': 1.89}\n",
      "{'loss': 0.1816, 'grad_norm': 0.6307899951934814, 'learning_rate': 0.00017542513807361037, 'epoch': 1.9}\n",
      "{'loss': 0.061, 'grad_norm': 0.35615164041519165, 'learning_rate': 0.00017514944717726962, 'epoch': 1.91}\n",
      "{'loss': 0.1068, 'grad_norm': 0.4863624572753906, 'learning_rate': 0.0001748724377134086, 'epoch': 1.92}\n",
      "{'loss': 0.0414, 'grad_norm': 0.3620626926422119, 'learning_rate': 0.00017459411454241822, 'epoch': 1.93}\n",
      "{'loss': 0.0758, 'grad_norm': 0.5131049156188965, 'learning_rate': 0.00017431448254773944, 'epoch': 1.94}\n",
      "{'loss': 0.1275, 'grad_norm': 0.529458224773407, 'learning_rate': 0.00017403354663577783, 'epoch': 1.95}\n",
      "{'loss': 0.1486, 'grad_norm': 0.5112460851669312, 'learning_rate': 0.0001737513117358174, 'epoch': 1.96}\n",
      "{'loss': 0.1074, 'grad_norm': 0.48970791697502136, 'learning_rate': 0.00017346778279993415, 'epoch': 1.97}\n",
      "{'loss': 0.0272, 'grad_norm': 0.2621384561061859, 'learning_rate': 0.0001731829648029091, 'epoch': 1.98}\n",
      "{'loss': 0.1583, 'grad_norm': 0.6916561722755432, 'learning_rate': 0.00017289686274214118, 'epoch': 1.99}\n",
      "{'loss': 0.154, 'grad_norm': 1.0630264282226562, 'learning_rate': 0.00017260948163755918, 'epoch': 2.0}\n",
      " 25%|██████████▎                              | 192/760 [07:32<19:11,  2.03s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.03it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.35it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.27it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  7.01it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.75it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.65it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.35it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.37it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.42it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.15it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.29it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.27it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1062622144818306, 'eval_runtime': 2.5675, 'eval_samples_per_second': 12.463, 'eval_steps_per_second': 6.232, 'epoch': 2.0}\n",
      " 25%|██████████▎                              | 192/760 [07:35<19:11,  2.03s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.46it/s]\u001b[A\n",
      "{'loss': 0.4248, 'grad_norm': 0.5937719345092773, 'learning_rate': 0.00017232082653153422, 'epoch': 2.01}\n",
      "{'loss': 0.2501, 'grad_norm': 0.45523253083229065, 'learning_rate': 0.0001720309024887907, 'epoch': 2.02}\n",
      "{'loss': 0.1052, 'grad_norm': 0.4304550588130951, 'learning_rate': 0.00017173971459631787, 'epoch': 2.03}\n",
      "{'loss': 0.3297, 'grad_norm': 0.6727988123893738, 'learning_rate': 0.00017144726796328034, 'epoch': 2.04}\n",
      "{'loss': 0.1701, 'grad_norm': 0.41119271516799927, 'learning_rate': 0.00017115356772092857, 'epoch': 2.05}\n",
      "{'loss': 0.1264, 'grad_norm': 0.7414469122886658, 'learning_rate': 0.00017085861902250865, 'epoch': 2.06}\n",
      "{'loss': 0.0498, 'grad_norm': 0.3146101236343384, 'learning_rate': 0.0001705624270431721, 'epoch': 2.07}\n",
      "{'loss': 0.0203, 'grad_norm': 0.1996113508939743, 'learning_rate': 0.00017026499697988493, 'epoch': 2.08}\n",
      "{'loss': 0.0849, 'grad_norm': 0.513508141040802, 'learning_rate': 0.00016996633405133655, 'epoch': 2.09}\n",
      "{'loss': 0.067, 'grad_norm': 0.4534582793712616, 'learning_rate': 0.00016966644349784808, 'epoch': 2.1}\n",
      "{'loss': 0.047, 'grad_norm': 0.2723972797393799, 'learning_rate': 0.0001693653305812805, 'epoch': 2.11}\n",
      "{'loss': 0.0486, 'grad_norm': 0.34659048914909363, 'learning_rate': 0.00016906300058494228, 'epoch': 2.13}\n",
      "{'loss': 0.1115, 'grad_norm': 0.47669294476509094, 'learning_rate': 0.00016875945881349676, 'epoch': 2.14}\n",
      "{'loss': 0.0765, 'grad_norm': 0.45737314224243164, 'learning_rate': 0.00016845471059286887, 'epoch': 2.15}\n",
      "{'loss': 0.0309, 'grad_norm': 0.39774826169013977, 'learning_rate': 0.000168148761270152, 'epoch': 2.16}\n",
      "{'loss': 0.0311, 'grad_norm': 0.5139020681381226, 'learning_rate': 0.00016784161621351382, 'epoch': 2.17}\n",
      "{'loss': 0.06, 'grad_norm': 0.4717119038105011, 'learning_rate': 0.00016753328081210245, 'epoch': 2.18}\n",
      "{'loss': 0.0713, 'grad_norm': 0.5574057698249817, 'learning_rate': 0.00016722376047595164, 'epoch': 2.19}\n",
      "{'loss': 0.0417, 'grad_norm': 0.463164746761322, 'learning_rate': 0.00016691306063588583, 'epoch': 2.2}\n",
      "{'loss': 0.1044, 'grad_norm': 0.5004167556762695, 'learning_rate': 0.00016660118674342517, 'epoch': 2.21}\n",
      "{'loss': 0.082, 'grad_norm': 0.48185402154922485, 'learning_rate': 0.00016628814427068953, 'epoch': 2.22}\n",
      "{'loss': 0.0463, 'grad_norm': 0.3016021251678467, 'learning_rate': 0.00016597393871030264, 'epoch': 2.23}\n",
      "{'loss': 0.0866, 'grad_norm': 0.4714953601360321, 'learning_rate': 0.00016565857557529566, 'epoch': 2.24}\n",
      "{'loss': 0.0259, 'grad_norm': 0.4482111632823944, 'learning_rate': 0.00016534206039901057, 'epoch': 2.25}\n",
      "{'loss': 0.0462, 'grad_norm': 0.47274380922317505, 'learning_rate': 0.00016502439873500289, 'epoch': 2.26}\n",
      "{'loss': 0.1922, 'grad_norm': 0.6607533693313599, 'learning_rate': 0.00016470559615694446, 'epoch': 2.27}\n",
      "{'loss': 0.0693, 'grad_norm': 0.6200143694877625, 'learning_rate': 0.0001643856582585254, 'epoch': 2.28}\n",
      "{'loss': 0.0664, 'grad_norm': 0.6128303408622742, 'learning_rate': 0.00016406459065335615, 'epoch': 2.29}\n",
      "{'loss': 0.0274, 'grad_norm': 0.35316869616508484, 'learning_rate': 0.000163742398974869, 'epoch': 2.3}\n",
      "{'loss': 0.0395, 'grad_norm': 0.34272530674934387, 'learning_rate': 0.00016341908887621895, 'epoch': 2.31}\n",
      "{'loss': 0.0951, 'grad_norm': 0.4258058965206146, 'learning_rate': 0.00016309466603018496, 'epoch': 2.32}\n",
      "{'loss': 0.0667, 'grad_norm': 0.4719950258731842, 'learning_rate': 0.00016276913612907007, 'epoch': 2.33}\n",
      "{'loss': 0.0656, 'grad_norm': 0.5414919853210449, 'learning_rate': 0.00016244250488460158, 'epoch': 2.34}\n",
      "{'loss': 0.0307, 'grad_norm': 0.3153011202812195, 'learning_rate': 0.00016211477802783103, 'epoch': 2.36}\n",
      "{'loss': 0.0546, 'grad_norm': 0.45321381092071533, 'learning_rate': 0.00016178596130903344, 'epoch': 2.37}\n",
      "{'loss': 0.0856, 'grad_norm': 0.5435059666633606, 'learning_rate': 0.00016145606049760644, 'epoch': 2.38}\n",
      "{'loss': 0.0359, 'grad_norm': 0.317963570356369, 'learning_rate': 0.00016112508138196917, 'epoch': 2.39}\n",
      "{'loss': 0.06, 'grad_norm': 0.376630961894989, 'learning_rate': 0.00016079302976946055, 'epoch': 2.4}\n",
      "{'loss': 0.051, 'grad_norm': 0.6148827075958252, 'learning_rate': 0.0001604599114862375, 'epoch': 2.41}\n",
      "{'loss': 0.0324, 'grad_norm': 0.3342263996601105, 'learning_rate': 0.0001601257323771727, 'epoch': 2.42}\n",
      "{'loss': 0.0498, 'grad_norm': 0.4424358904361725, 'learning_rate': 0.0001597904983057519, 'epoch': 2.43}\n",
      "{'loss': 0.0603, 'grad_norm': 0.40964990854263306, 'learning_rate': 0.00015945421515397133, 'epoch': 2.44}\n",
      "{'loss': 0.0491, 'grad_norm': 0.4563571512699127, 'learning_rate': 0.0001591168888222342, 'epoch': 2.45}\n",
      "{'loss': 0.0634, 'grad_norm': 0.3820624351501465, 'learning_rate': 0.00015877852522924732, 'epoch': 2.46}\n",
      "{'loss': 0.0448, 'grad_norm': 0.3968484699726105, 'learning_rate': 0.00015843913031191723, 'epoch': 2.47}\n",
      "{'loss': 0.0445, 'grad_norm': 0.4760454297065735, 'learning_rate': 0.000158098710025246, 'epoch': 2.48}\n",
      "{'loss': 0.095, 'grad_norm': 0.4497348368167877, 'learning_rate': 0.00015775727034222675, 'epoch': 2.49}\n",
      "{'loss': 0.0655, 'grad_norm': 0.602937638759613, 'learning_rate': 0.000157414817253739, 'epoch': 2.5}\n",
      " 32%|████████████▉                            | 240/760 [09:36<17:30,  2.02s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.11it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.33it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.94it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.69it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.59it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.18it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.30it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.32it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.37it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.10it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1083919107913971, 'eval_runtime': 2.5992, 'eval_samples_per_second': 12.311, 'eval_steps_per_second': 6.156, 'epoch': 2.5}\n",
      " 32%|████████████▉                            | 240/760 [09:39<17:30,  2.02s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "{'loss': 0.0402, 'grad_norm': 0.4748958349227905, 'learning_rate': 0.0001570713567684432, 'epoch': 2.51}\n",
      "{'loss': 0.1109, 'grad_norm': 0.5401780605316162, 'learning_rate': 0.00015672689491267567, 'epoch': 2.52}\n",
      "{'loss': 0.0393, 'grad_norm': 0.34113097190856934, 'learning_rate': 0.00015638143773034267, 'epoch': 2.53}\n",
      "{'loss': 0.0931, 'grad_norm': 0.350072979927063, 'learning_rate': 0.00015603499128281448, 'epoch': 2.54}\n",
      "{'loss': 0.0264, 'grad_norm': 0.34326252341270447, 'learning_rate': 0.00015568756164881882, 'epoch': 2.55}\n",
      "{'loss': 0.2605, 'grad_norm': 0.5633964538574219, 'learning_rate': 0.00015533915492433443, 'epoch': 2.56}\n",
      "{'loss': 0.1299, 'grad_norm': 0.6471573710441589, 'learning_rate': 0.000154989777222484, 'epoch': 2.57}\n",
      "{'loss': 0.0721, 'grad_norm': 0.46454918384552, 'learning_rate': 0.00015463943467342693, 'epoch': 2.58}\n",
      "{'loss': 0.0557, 'grad_norm': 0.2895015478134155, 'learning_rate': 0.00015428813342425177, 'epoch': 2.6}\n",
      "{'loss': 0.0748, 'grad_norm': 0.36378738284111023, 'learning_rate': 0.00015393587963886835, 'epoch': 2.61}\n",
      "{'loss': 0.1411, 'grad_norm': 0.6071217060089111, 'learning_rate': 0.00015358267949789966, 'epoch': 2.62}\n",
      "{'loss': 0.1066, 'grad_norm': 0.5787434577941895, 'learning_rate': 0.0001532285391985734, 'epoch': 2.63}\n",
      "{'loss': 0.0246, 'grad_norm': 0.22228381037712097, 'learning_rate': 0.00015287346495461315, 'epoch': 2.64}\n",
      "{'loss': 0.0571, 'grad_norm': 0.41456738114356995, 'learning_rate': 0.0001525174629961296, 'epoch': 2.65}\n",
      "{'loss': 0.0411, 'grad_norm': 0.39367014169692993, 'learning_rate': 0.0001521605395695108, 'epoch': 2.66}\n",
      "{'loss': 0.0554, 'grad_norm': 0.47688794136047363, 'learning_rate': 0.00015180270093731303, 'epoch': 2.67}\n",
      "{'loss': 0.0528, 'grad_norm': 0.38956716656684875, 'learning_rate': 0.00015144395337815064, 'epoch': 2.68}\n",
      "{'loss': 0.0789, 'grad_norm': 0.6103975772857666, 'learning_rate': 0.000151084303186586, 'epoch': 2.69}\n",
      "{'loss': 0.031, 'grad_norm': 0.4098359942436218, 'learning_rate': 0.00015072375667301893, 'epoch': 2.7}\n",
      "{'loss': 0.0498, 'grad_norm': 0.5847111344337463, 'learning_rate': 0.0001503623201635761, 'epoch': 2.71}\n",
      "{'loss': 0.0488, 'grad_norm': 0.4570818543434143, 'learning_rate': 0.00015000000000000001, 'epoch': 2.72}\n",
      "{'loss': 0.0427, 'grad_norm': 0.3720187842845917, 'learning_rate': 0.0001496368025395377, 'epoch': 2.73}\n",
      "{'loss': 0.0396, 'grad_norm': 0.5749357342720032, 'learning_rate': 0.00014927273415482915, 'epoch': 2.74}\n",
      "{'loss': 0.0729, 'grad_norm': 0.4954891800880432, 'learning_rate': 0.00014890780123379564, 'epoch': 2.75}\n",
      "{'loss': 0.0429, 'grad_norm': 0.34747809171676636, 'learning_rate': 0.0001485420101795274, 'epoch': 2.76}\n",
      "{'loss': 0.0441, 'grad_norm': 0.42749619483947754, 'learning_rate': 0.00014817536741017152, 'epoch': 2.77}\n",
      "{'loss': 0.0404, 'grad_norm': 0.33614787459373474, 'learning_rate': 0.00014780787935881923, 'epoch': 2.78}\n",
      "{'loss': 0.0291, 'grad_norm': 0.3732527792453766, 'learning_rate': 0.00014743955247339293, 'epoch': 2.79}\n",
      "{'loss': 0.0227, 'grad_norm': 0.2688620388507843, 'learning_rate': 0.0001470703932165333, 'epoch': 2.8}\n",
      "{'loss': 0.064, 'grad_norm': 0.48440465331077576, 'learning_rate': 0.00014670040806548555, 'epoch': 2.81}\n",
      "{'loss': 0.1009, 'grad_norm': 0.4190366268157959, 'learning_rate': 0.00014632960351198618, 'epoch': 2.83}\n",
      "{'loss': 0.0807, 'grad_norm': 0.4654383659362793, 'learning_rate': 0.00014595798606214882, 'epoch': 2.84}\n",
      "{'loss': 0.0393, 'grad_norm': 0.4062231481075287, 'learning_rate': 0.00014558556223635003, 'epoch': 2.85}\n",
      "{'loss': 0.077, 'grad_norm': 0.7316527366638184, 'learning_rate': 0.00014521233856911508, 'epoch': 2.86}\n",
      "{'loss': 0.0385, 'grad_norm': 0.38054999709129333, 'learning_rate': 0.00014483832160900326, 'epoch': 2.87}\n",
      "{'loss': 0.0416, 'grad_norm': 0.4135943353176117, 'learning_rate': 0.00014446351791849276, 'epoch': 2.88}\n",
      "{'loss': 0.074, 'grad_norm': 0.5256775617599487, 'learning_rate': 0.00014408793407386588, 'epoch': 2.89}\n",
      "{'loss': 0.0297, 'grad_norm': 0.4670177698135376, 'learning_rate': 0.0001437115766650933, 'epoch': 2.9}\n",
      "{'loss': 0.0293, 'grad_norm': 0.39253419637680054, 'learning_rate': 0.00014333445229571873, 'epoch': 2.91}\n",
      "{'loss': 0.0418, 'grad_norm': 0.4351385831832886, 'learning_rate': 0.00014295656758274284, 'epoch': 2.92}\n",
      "{'loss': 0.1939, 'grad_norm': 0.8385515809059143, 'learning_rate': 0.00014257792915650728, 'epoch': 2.93}\n",
      "{'loss': 0.0244, 'grad_norm': 0.20972691476345062, 'learning_rate': 0.0001421985436605783, 'epoch': 2.94}\n",
      "{'loss': 0.0347, 'grad_norm': 0.3899412155151367, 'learning_rate': 0.00014181841775163013, 'epoch': 2.95}\n",
      "{'loss': 0.0752, 'grad_norm': 0.4142437279224396, 'learning_rate': 0.00014143755809932845, 'epoch': 2.96}\n",
      "{'loss': 0.0313, 'grad_norm': 0.3645007908344269, 'learning_rate': 0.0001410559713862128, 'epoch': 2.97}\n",
      "{'loss': 0.245, 'grad_norm': 0.8084869980812073, 'learning_rate': 0.00014067366430758004, 'epoch': 2.98}\n",
      "{'loss': 0.0285, 'grad_norm': 0.49633562564849854, 'learning_rate': 0.00014029064357136628, 'epoch': 2.99}\n",
      "{'loss': 0.0342, 'grad_norm': 0.5031834244728088, 'learning_rate': 0.00013990691589802954, 'epoch': 3.0}\n",
      " 38%|███████████████▌                         | 288/760 [11:17<15:31,  1.97s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.05it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.35it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.21it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.95it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.70it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.61it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.33it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.35it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.40it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.25it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.23it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09982678294181824, 'eval_runtime': 2.5847, 'eval_samples_per_second': 12.381, 'eval_steps_per_second': 6.19, 'epoch': 3.0}\n",
      " 38%|███████████████▌                         | 288/760 [11:19<15:31,  1.97s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "{'loss': 0.2165, 'grad_norm': 0.571257472038269, 'learning_rate': 0.00013952248802043165, 'epoch': 3.01}\n",
      "{'loss': 0.2535, 'grad_norm': 0.5496947765350342, 'learning_rate': 0.00013913736668372026, 'epoch': 3.02}\n",
      "{'loss': 0.152, 'grad_norm': 0.42332473397254944, 'learning_rate': 0.0001387515586452103, 'epoch': 3.03}\n",
      "{'loss': 0.1198, 'grad_norm': 0.44660669565200806, 'learning_rate': 0.00013836507067426564, 'epoch': 3.04}\n",
      "{'loss': 0.1657, 'grad_norm': 0.550270676612854, 'learning_rate': 0.00013797790955218014, 'epoch': 3.05}\n",
      "{'loss': 0.0645, 'grad_norm': 0.4472406506538391, 'learning_rate': 0.0001375900820720587, 'epoch': 3.06}\n",
      "{'loss': 0.0181, 'grad_norm': 0.45634153485298157, 'learning_rate': 0.00013720159503869815, 'epoch': 3.07}\n",
      "{'loss': 0.0722, 'grad_norm': 0.4572370946407318, 'learning_rate': 0.00013681245526846783, 'epoch': 3.08}\n",
      "{'loss': 0.0141, 'grad_norm': 0.2281263917684555, 'learning_rate': 0.00013642266958918984, 'epoch': 3.09}\n",
      "{'loss': 0.0309, 'grad_norm': 0.29340770840644836, 'learning_rate': 0.00013603224484001948, 'epoch': 3.1}\n",
      "{'loss': 0.0249, 'grad_norm': 0.39899536967277527, 'learning_rate': 0.00013564118787132506, 'epoch': 3.11}\n",
      "{'loss': 0.0254, 'grad_norm': 0.36151912808418274, 'learning_rate': 0.00013524950554456784, 'epoch': 3.13}\n",
      "{'loss': 0.0196, 'grad_norm': 0.37217262387275696, 'learning_rate': 0.00013485720473218154, 'epoch': 3.14}\n",
      "{'loss': 0.044, 'grad_norm': 0.5350500345230103, 'learning_rate': 0.0001344642923174517, 'epoch': 3.15}\n",
      "{'loss': 0.0077, 'grad_norm': 0.22294846177101135, 'learning_rate': 0.0001340707751943952, 'epoch': 3.16}\n",
      "{'loss': 0.0391, 'grad_norm': 0.5466324090957642, 'learning_rate': 0.00013367666026763882, 'epoch': 3.17}\n",
      "{'loss': 0.0308, 'grad_norm': 0.47291573882102966, 'learning_rate': 0.00013328195445229868, 'epoch': 3.18}\n",
      "{'loss': 0.0113, 'grad_norm': 0.23628951609134674, 'learning_rate': 0.00013288666467385833, 'epoch': 3.19}\n",
      "{'loss': 0.166, 'grad_norm': 0.6776010990142822, 'learning_rate': 0.00013249079786804765, 'epoch': 3.2}\n",
      "{'loss': 0.041, 'grad_norm': 0.419681191444397, 'learning_rate': 0.00013209436098072095, 'epoch': 3.21}\n",
      "{'loss': 0.0292, 'grad_norm': 0.4681847393512726, 'learning_rate': 0.0001316973609677352, 'epoch': 3.22}\n",
      "{'loss': 0.0307, 'grad_norm': 0.2931773066520691, 'learning_rate': 0.00013129980479482782, 'epoch': 3.23}\n",
      "{'loss': 0.1321, 'grad_norm': 0.6312546133995056, 'learning_rate': 0.00013090169943749476, 'epoch': 3.24}\n",
      "{'loss': 0.0165, 'grad_norm': 0.31292155385017395, 'learning_rate': 0.0001305030518808678, 'epoch': 3.25}\n",
      "{'loss': 0.0251, 'grad_norm': 0.24948154389858246, 'learning_rate': 0.00013010386911959206, 'epoch': 3.26}\n",
      "{'loss': 0.0199, 'grad_norm': 0.3085591793060303, 'learning_rate': 0.0001297041581577035, 'epoch': 3.27}\n",
      "{'loss': 0.0731, 'grad_norm': 0.5295810103416443, 'learning_rate': 0.00012930392600850573, 'epoch': 3.28}\n",
      "{'loss': 0.0876, 'grad_norm': 0.614830493927002, 'learning_rate': 0.00012890317969444716, 'epoch': 3.29}\n",
      "{'loss': 0.0425, 'grad_norm': 0.48150917887687683, 'learning_rate': 0.0001285019262469976, 'epoch': 3.3}\n",
      "{'loss': 0.0267, 'grad_norm': 0.4778255224227905, 'learning_rate': 0.00012810017270652513, 'epoch': 3.31}\n",
      "{'loss': 0.0305, 'grad_norm': 0.44926342368125916, 'learning_rate': 0.00012769792612217224, 'epoch': 3.32}\n",
      "{'loss': 0.042, 'grad_norm': 0.8816819787025452, 'learning_rate': 0.00012729519355173254, 'epoch': 3.33}\n",
      "{'loss': 0.0966, 'grad_norm': 0.5255540013313293, 'learning_rate': 0.00012689198206152657, 'epoch': 3.34}\n",
      "{'loss': 0.0668, 'grad_norm': 0.5418737530708313, 'learning_rate': 0.00012648829872627807, 'epoch': 3.36}\n",
      "{'loss': 0.0514, 'grad_norm': 0.5693055987358093, 'learning_rate': 0.00012608415062898972, 'epoch': 3.37}\n",
      "{'loss': 0.0482, 'grad_norm': 0.49927273392677307, 'learning_rate': 0.00012567954486081878, 'epoch': 3.38}\n",
      "{'loss': 0.0579, 'grad_norm': 0.32889324426651, 'learning_rate': 0.00012527448852095295, 'epoch': 3.39}\n",
      "{'loss': 0.0415, 'grad_norm': 0.32753241062164307, 'learning_rate': 0.0001248689887164855, 'epoch': 3.4}\n",
      "{'loss': 0.0347, 'grad_norm': 0.3117850124835968, 'learning_rate': 0.00012446305256229073, 'epoch': 3.41}\n",
      "{'loss': 0.0136, 'grad_norm': 0.1834673136472702, 'learning_rate': 0.00012405668718089917, 'epoch': 3.42}\n",
      "{'loss': 0.0139, 'grad_norm': 0.21614620089530945, 'learning_rate': 0.00012364989970237248, 'epoch': 3.43}\n",
      "{'loss': 0.0548, 'grad_norm': 0.8722354173660278, 'learning_rate': 0.0001232426972641784, 'epoch': 3.44}\n",
      "{'loss': 0.0325, 'grad_norm': 0.4773963391780853, 'learning_rate': 0.00012283508701106557, 'epoch': 3.45}\n",
      "{'loss': 0.0263, 'grad_norm': 0.3217516839504242, 'learning_rate': 0.00012242707609493814, 'epoch': 3.46}\n",
      "{'loss': 0.0332, 'grad_norm': 0.36221495270729065, 'learning_rate': 0.00012201867167473015, 'epoch': 3.47}\n",
      "{'loss': 0.0302, 'grad_norm': 0.38443252444267273, 'learning_rate': 0.00012160988091628022, 'epoch': 3.48}\n",
      "{'loss': 0.0484, 'grad_norm': 0.5345597863197327, 'learning_rate': 0.00012120071099220549, 'epoch': 3.49}\n",
      "{'loss': 0.0141, 'grad_norm': 0.3872898817062378, 'learning_rate': 0.00012079116908177593, 'epoch': 3.5}\n",
      " 44%|██████████████████▏                      | 336/760 [13:20<14:31,  2.06s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.85it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.27it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.17it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.94it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.68it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.58it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.19it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.30it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.32it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.37it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.22it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.11529067158699036, 'eval_runtime': 2.6031, 'eval_samples_per_second': 12.293, 'eval_steps_per_second': 6.147, 'epoch': 3.5}\n",
      " 44%|██████████████████▏                      | 336/760 [13:23<14:31,  2.06s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "{'loss': 0.0245, 'grad_norm': 0.24742697179317474, 'learning_rate': 0.0001203812623707885, 'epoch': 3.51}\n",
      "{'loss': 0.0387, 'grad_norm': 0.38930246233940125, 'learning_rate': 0.00011997099805144069, 'epoch': 3.52}\n",
      "{'loss': 0.0329, 'grad_norm': 0.4678325653076172, 'learning_rate': 0.00011956038332220483, 'epoch': 3.53}\n",
      "{'loss': 0.0526, 'grad_norm': 0.7922792434692383, 'learning_rate': 0.00011914942538770131, 'epoch': 3.54}\n",
      "{'loss': 0.0222, 'grad_norm': 0.37113699316978455, 'learning_rate': 0.00011873813145857249, 'epoch': 3.55}\n",
      "{'loss': 0.0226, 'grad_norm': 0.3811202049255371, 'learning_rate': 0.00011832650875135598, 'epoch': 3.56}\n",
      "{'loss': 0.0302, 'grad_norm': 0.6834125518798828, 'learning_rate': 0.00011791456448835825, 'epoch': 3.57}\n",
      "{'loss': 0.0349, 'grad_norm': 0.413993239402771, 'learning_rate': 0.00011750230589752762, 'epoch': 3.58}\n",
      "{'loss': 0.0304, 'grad_norm': 0.47917452454566956, 'learning_rate': 0.00011708974021232769, 'epoch': 3.6}\n",
      "{'loss': 0.0368, 'grad_norm': 0.36654725670814514, 'learning_rate': 0.00011667687467161024, 'epoch': 3.61}\n",
      "{'loss': 0.024, 'grad_norm': 0.38119712471961975, 'learning_rate': 0.00011626371651948838, 'epoch': 3.62}\n",
      "{'loss': 0.0432, 'grad_norm': 0.44954928755760193, 'learning_rate': 0.0001158502730052093, 'epoch': 3.63}\n",
      "{'loss': 0.0296, 'grad_norm': 0.30606240034103394, 'learning_rate': 0.00011543655138302714, 'epoch': 3.64}\n",
      "{'loss': 0.0278, 'grad_norm': 0.4982143044471741, 'learning_rate': 0.00011502255891207572, 'epoch': 3.65}\n",
      "{'loss': 0.0501, 'grad_norm': 0.48772379755973816, 'learning_rate': 0.00011460830285624118, 'epoch': 3.66}\n",
      "{'loss': 0.0168, 'grad_norm': 0.4696011245250702, 'learning_rate': 0.00011419379048403444, 'epoch': 3.67}\n",
      "{'loss': 0.025, 'grad_norm': 0.35387012362480164, 'learning_rate': 0.0001137790290684638, 'epoch': 3.68}\n",
      "{'loss': 0.0303, 'grad_norm': 0.39181023836135864, 'learning_rate': 0.00011336402588690726, 'epoch': 3.69}\n",
      "{'loss': 0.0183, 'grad_norm': 0.26061558723449707, 'learning_rate': 0.00011294878822098469, 'epoch': 3.7}\n",
      "{'loss': 0.0163, 'grad_norm': 0.26992034912109375, 'learning_rate': 0.00011253332335643043, 'epoch': 3.71}\n",
      "{'loss': 0.0348, 'grad_norm': 0.34049418568611145, 'learning_rate': 0.00011211763858296507, 'epoch': 3.72}\n",
      "{'loss': 0.0196, 'grad_norm': 0.5301587581634521, 'learning_rate': 0.00011170174119416776, 'epoch': 3.73}\n",
      "{'loss': 0.0292, 'grad_norm': 0.40919023752212524, 'learning_rate': 0.00011128563848734816, 'epoch': 3.74}\n",
      "{'loss': 0.0383, 'grad_norm': 0.4917561411857605, 'learning_rate': 0.00011086933776341852, 'epoch': 3.75}\n",
      "{'loss': 0.0426, 'grad_norm': 0.5824217200279236, 'learning_rate': 0.00011045284632676536, 'epoch': 3.76}\n",
      "{'loss': 0.0203, 'grad_norm': 0.3460635840892792, 'learning_rate': 0.00011003617148512149, 'epoch': 3.77}\n",
      "{'loss': 0.0434, 'grad_norm': 0.4930019974708557, 'learning_rate': 0.00010961932054943778, 'epoch': 3.78}\n",
      "{'loss': 0.0277, 'grad_norm': 0.24856793880462646, 'learning_rate': 0.00010920230083375473, 'epoch': 3.79}\n",
      "{'loss': 0.0756, 'grad_norm': 3.3223297595977783, 'learning_rate': 0.00010878511965507434, 'epoch': 3.8}\n",
      "{'loss': 0.0591, 'grad_norm': 0.474475622177124, 'learning_rate': 0.00010836778433323158, 'epoch': 3.81}\n",
      "{'loss': 0.0499, 'grad_norm': 0.6939396262168884, 'learning_rate': 0.00010795030219076599, 'epoch': 3.83}\n",
      "{'loss': 0.0472, 'grad_norm': 0.5662484169006348, 'learning_rate': 0.00010753268055279329, 'epoch': 3.84}\n",
      "{'loss': 0.0481, 'grad_norm': 0.5600312352180481, 'learning_rate': 0.00010711492674687671, 'epoch': 3.85}\n",
      "{'loss': 0.0586, 'grad_norm': 0.43463853001594543, 'learning_rate': 0.00010669704810289851, 'epoch': 3.86}\n",
      "{'loss': 0.0176, 'grad_norm': 0.3190420866012573, 'learning_rate': 0.00010627905195293135, 'epoch': 3.87}\n",
      "{'loss': 0.0601, 'grad_norm': 0.4754108786582947, 'learning_rate': 0.00010586094563110964, 'epoch': 3.88}\n",
      "{'loss': 0.0786, 'grad_norm': 1.4979757070541382, 'learning_rate': 0.00010544273647350092, 'epoch': 3.89}\n",
      "{'loss': 0.0321, 'grad_norm': 0.5250207781791687, 'learning_rate': 0.00010502443181797697, 'epoch': 3.9}\n",
      "{'loss': 0.097, 'grad_norm': 0.6439546346664429, 'learning_rate': 0.00010460603900408523, 'epoch': 3.91}\n",
      "{'loss': 0.0164, 'grad_norm': 0.2946867048740387, 'learning_rate': 0.00010418756537291996, 'epoch': 3.92}\n",
      "{'loss': 0.1049, 'grad_norm': 0.5478954315185547, 'learning_rate': 0.00010376901826699348, 'epoch': 3.93}\n",
      "{'loss': 0.0193, 'grad_norm': 0.27278658747673035, 'learning_rate': 0.00010335040503010716, 'epoch': 3.94}\n",
      "{'loss': 0.0452, 'grad_norm': 0.5429567098617554, 'learning_rate': 0.00010293173300722285, 'epoch': 3.95}\n",
      "{'loss': 0.0113, 'grad_norm': 0.22446705400943756, 'learning_rate': 0.00010251300954433376, 'epoch': 3.96}\n",
      "{'loss': 0.0166, 'grad_norm': 0.2452102154493332, 'learning_rate': 0.0001020942419883357, 'epoch': 3.97}\n",
      "{'loss': 0.0728, 'grad_norm': 0.6493418216705322, 'learning_rate': 0.00010167543768689815, 'epoch': 3.98}\n",
      "{'loss': 0.0493, 'grad_norm': 0.5167670249938965, 'learning_rate': 0.00010125660398833528, 'epoch': 3.99}\n",
      "{'loss': 0.0226, 'grad_norm': 0.4105488657951355, 'learning_rate': 0.00010083774824147708, 'epoch': 4.0}\n",
      " 51%|████████████████████▋                    | 384/760 [15:01<12:11,  1.94s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.04it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.32it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.18it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.93it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.68it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.59it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.18it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.30it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.32it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.37it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10814209282398224, 'eval_runtime': 2.5967, 'eval_samples_per_second': 12.323, 'eval_steps_per_second': 6.162, 'epoch': 4.0}\n",
      " 51%|████████████████████▋                    | 384/760 [15:04<12:11,  1.94s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "{'loss': 0.0635, 'grad_norm': 0.3795362114906311, 'learning_rate': 0.0001004188777955404, 'epoch': 4.01}\n",
      "{'loss': 0.1909, 'grad_norm': 0.5010929107666016, 'learning_rate': 0.0001, 'epoch': 4.02}\n",
      "{'loss': 0.0439, 'grad_norm': 0.22145673632621765, 'learning_rate': 9.958112220445963e-05, 'epoch': 4.03}\n",
      "{'loss': 0.0283, 'grad_norm': 0.2921319901943207, 'learning_rate': 9.916225175852293e-05, 'epoch': 4.04}\n",
      "{'loss': 0.1103, 'grad_norm': 0.39878982305526733, 'learning_rate': 9.874339601166473e-05, 'epoch': 4.05}\n",
      "{'loss': 0.09, 'grad_norm': 0.44368699193000793, 'learning_rate': 9.832456231310189e-05, 'epoch': 4.06}\n",
      "{'loss': 0.0741, 'grad_norm': 0.41275614500045776, 'learning_rate': 9.790575801166432e-05, 'epoch': 4.07}\n",
      "{'loss': 0.0292, 'grad_norm': 0.24393701553344727, 'learning_rate': 9.748699045566626e-05, 'epoch': 4.08}\n",
      "{'loss': 0.0072, 'grad_norm': 0.13550807535648346, 'learning_rate': 9.706826699277718e-05, 'epoch': 4.09}\n",
      "{'loss': 0.0723, 'grad_norm': 0.3820574879646301, 'learning_rate': 9.664959496989285e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0311, 'grad_norm': 0.2534860074520111, 'learning_rate': 9.623098173300654e-05, 'epoch': 4.11}\n",
      "{'loss': 0.0531, 'grad_norm': 0.5207120776176453, 'learning_rate': 9.581243462708006e-05, 'epoch': 4.13}\n",
      "{'loss': 0.0089, 'grad_norm': 0.22971941530704498, 'learning_rate': 9.539396099591476e-05, 'epoch': 4.14}\n",
      "{'loss': 0.0165, 'grad_norm': 0.20982575416564941, 'learning_rate': 9.497556818202306e-05, 'epoch': 4.15}\n",
      "{'loss': 0.0409, 'grad_norm': 0.3265868127346039, 'learning_rate': 9.455726352649911e-05, 'epoch': 4.16}\n",
      "{'loss': 0.0153, 'grad_norm': 0.2884172201156616, 'learning_rate': 9.413905436889035e-05, 'epoch': 4.17}\n",
      "{'loss': 0.0439, 'grad_norm': 0.46912580728530884, 'learning_rate': 9.372094804706867e-05, 'epoch': 4.18}\n",
      "{'loss': 0.0284, 'grad_norm': 0.42501625418663025, 'learning_rate': 9.330295189710152e-05, 'epoch': 4.19}\n",
      "{'loss': 0.025, 'grad_norm': 0.35619819164276123, 'learning_rate': 9.288507325312335e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0144, 'grad_norm': 0.40256771445274353, 'learning_rate': 9.246731944720675e-05, 'epoch': 4.21}\n",
      "{'loss': 0.0742, 'grad_norm': 0.515624463558197, 'learning_rate': 9.204969780923403e-05, 'epoch': 4.22}\n",
      "{'loss': 0.0117, 'grad_norm': 0.2664960026741028, 'learning_rate': 9.163221566676847e-05, 'epoch': 4.23}\n",
      "{'loss': 0.0659, 'grad_norm': 0.5706647038459778, 'learning_rate': 9.121488034492569e-05, 'epoch': 4.24}\n",
      "{'loss': 0.0106, 'grad_norm': 0.2696962058544159, 'learning_rate': 9.07976991662453e-05, 'epoch': 4.25}\n",
      "{'loss': 0.0112, 'grad_norm': 0.5332798361778259, 'learning_rate': 9.038067945056227e-05, 'epoch': 4.26}\n",
      "{'loss': 0.0278, 'grad_norm': 0.39645159244537354, 'learning_rate': 8.99638285148785e-05, 'epoch': 4.27}\n",
      "{'loss': 0.0224, 'grad_norm': 0.34935036301612854, 'learning_rate': 8.954715367323468e-05, 'epoch': 4.28}\n",
      "{'loss': 0.0228, 'grad_norm': 0.395913690328598, 'learning_rate': 8.913066223658151e-05, 'epoch': 4.29}\n",
      "{'loss': 0.0072, 'grad_norm': 0.40538743138313293, 'learning_rate': 8.871436151265184e-05, 'epoch': 4.3}\n",
      "{'loss': 0.007, 'grad_norm': 0.2383628785610199, 'learning_rate': 8.829825880583226e-05, 'epoch': 4.31}\n",
      "{'loss': 0.011, 'grad_norm': 0.8390543460845947, 'learning_rate': 8.788236141703498e-05, 'epoch': 4.32}\n",
      "{'loss': 0.021, 'grad_norm': 0.3858746290206909, 'learning_rate': 8.746667664356956e-05, 'epoch': 4.33}\n",
      "{'loss': 0.0417, 'grad_norm': 0.3437962234020233, 'learning_rate': 8.705121177901532e-05, 'epoch': 4.34}\n",
      "{'loss': 0.0174, 'grad_norm': 0.24113847315311432, 'learning_rate': 8.663597411309279e-05, 'epoch': 4.36}\n",
      "{'loss': 0.0125, 'grad_norm': 0.19921176135540009, 'learning_rate': 8.62209709315362e-05, 'epoch': 4.37}\n",
      "{'loss': 0.0152, 'grad_norm': 0.3283408582210541, 'learning_rate': 8.580620951596557e-05, 'epoch': 4.38}\n",
      "{'loss': 0.0117, 'grad_norm': 0.2934439182281494, 'learning_rate': 8.539169714375885e-05, 'epoch': 4.39}\n",
      "{'loss': 0.0145, 'grad_norm': 0.2784571349620819, 'learning_rate': 8.497744108792429e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0135, 'grad_norm': 0.3069188892841339, 'learning_rate': 8.456344861697289e-05, 'epoch': 4.41}\n",
      "{'loss': 0.014, 'grad_norm': 0.33296895027160645, 'learning_rate': 8.414972699479075e-05, 'epoch': 4.42}\n",
      "{'loss': 0.0292, 'grad_norm': 0.35027945041656494, 'learning_rate': 8.373628348051165e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0363, 'grad_norm': 0.69944167137146, 'learning_rate': 8.332312532838978e-05, 'epoch': 4.44}\n",
      "{'loss': 0.0157, 'grad_norm': 0.424594908952713, 'learning_rate': 8.291025978767235e-05, 'epoch': 4.45}\n",
      "{'loss': 0.0186, 'grad_norm': 0.3607359826564789, 'learning_rate': 8.249769410247239e-05, 'epoch': 4.46}\n",
      "{'loss': 0.0264, 'grad_norm': 0.3903181254863739, 'learning_rate': 8.208543551164178e-05, 'epoch': 4.47}\n",
      "{'loss': 0.0131, 'grad_norm': 0.2897704839706421, 'learning_rate': 8.167349124864405e-05, 'epoch': 4.48}\n",
      "{'loss': 0.0085, 'grad_norm': 0.23460887372493744, 'learning_rate': 8.126186854142752e-05, 'epoch': 4.49}\n",
      "{'loss': 0.0135, 'grad_norm': 0.29304108023643494, 'learning_rate': 8.085057461229872e-05, 'epoch': 4.5}\n",
      " 57%|███████████████████████▎                 | 432/760 [17:04<11:05,  2.03s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.88it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.29it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.94it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.69it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.58it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.19it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.31it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.34it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.39it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.24it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.11823832988739014, 'eval_runtime': 2.5914, 'eval_samples_per_second': 12.348, 'eval_steps_per_second': 6.174, 'epoch': 4.5}\n",
      " 57%|███████████████████████▎                 | 432/760 [17:06<11:05,  2.03s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "{'loss': 0.0109, 'grad_norm': 0.25664812326431274, 'learning_rate': 8.04396166777952e-05, 'epoch': 4.51}\n",
      "{'loss': 0.0149, 'grad_norm': 0.30163314938545227, 'learning_rate': 8.002900194855932e-05, 'epoch': 4.52}\n",
      "{'loss': 0.0859, 'grad_norm': 0.6241722106933594, 'learning_rate': 7.961873762921153e-05, 'epoch': 4.53}\n",
      "{'loss': 0.0538, 'grad_norm': 0.6200727820396423, 'learning_rate': 7.920883091822408e-05, 'epoch': 4.54}\n",
      "{'loss': 0.0166, 'grad_norm': 0.42209529876708984, 'learning_rate': 7.879928900779456e-05, 'epoch': 4.55}\n",
      "{'loss': 0.0297, 'grad_norm': 0.3154030442237854, 'learning_rate': 7.83901190837198e-05, 'epoch': 4.56}\n",
      "{'loss': 0.0314, 'grad_norm': 0.49460819363594055, 'learning_rate': 7.798132832526986e-05, 'epoch': 4.57}\n",
      "{'loss': 0.0252, 'grad_norm': 0.334072470664978, 'learning_rate': 7.75729239050619e-05, 'epoch': 4.58}\n",
      "{'loss': 0.0435, 'grad_norm': 0.3987768590450287, 'learning_rate': 7.716491298893442e-05, 'epoch': 4.6}\n",
      "{'loss': 0.0993, 'grad_norm': 0.43797802925109863, 'learning_rate': 7.67573027358216e-05, 'epoch': 4.61}\n",
      "{'loss': 0.0168, 'grad_norm': 0.216530904173851, 'learning_rate': 7.635010029762756e-05, 'epoch': 4.62}\n",
      "{'loss': 0.02, 'grad_norm': 0.41008460521698, 'learning_rate': 7.594331281910082e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0244, 'grad_norm': 0.4779978096485138, 'learning_rate': 7.553694743770928e-05, 'epoch': 4.64}\n",
      "{'loss': 0.0141, 'grad_norm': 0.3268566131591797, 'learning_rate': 7.513101128351454e-05, 'epoch': 4.65}\n",
      "{'loss': 0.0053, 'grad_norm': 0.2531861960887909, 'learning_rate': 7.472551147904708e-05, 'epoch': 4.66}\n",
      "{'loss': 0.0099, 'grad_norm': 0.26435112953186035, 'learning_rate': 7.432045513918122e-05, 'epoch': 4.67}\n",
      "{'loss': 0.0272, 'grad_norm': 0.27863380312919617, 'learning_rate': 7.391584937101033e-05, 'epoch': 4.68}\n",
      "{'loss': 0.009, 'grad_norm': 0.2349340170621872, 'learning_rate': 7.351170127372191e-05, 'epoch': 4.69}\n",
      "{'loss': 0.0188, 'grad_norm': 0.35394808650016785, 'learning_rate': 7.310801793847344e-05, 'epoch': 4.7}\n",
      "{'loss': 0.0313, 'grad_norm': 0.3736225664615631, 'learning_rate': 7.270480644826749e-05, 'epoch': 4.71}\n",
      "{'loss': 0.0343, 'grad_norm': 0.5089965462684631, 'learning_rate': 7.230207387782776e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0136, 'grad_norm': 0.3652070462703705, 'learning_rate': 7.18998272934749e-05, 'epoch': 4.73}\n",
      "{'loss': 0.0148, 'grad_norm': 0.23997043073177338, 'learning_rate': 7.149807375300239e-05, 'epoch': 4.74}\n",
      "{'loss': 0.0376, 'grad_norm': 0.32326382398605347, 'learning_rate': 7.109682030555283e-05, 'epoch': 4.75}\n",
      "{'loss': 0.0053, 'grad_norm': 0.19690677523612976, 'learning_rate': 7.069607399149428e-05, 'epoch': 4.76}\n",
      "{'loss': 0.0433, 'grad_norm': 0.37864887714385986, 'learning_rate': 7.029584184229653e-05, 'epoch': 4.77}\n",
      "{'loss': 0.0752, 'grad_norm': 0.6794359087944031, 'learning_rate': 6.989613088040796e-05, 'epoch': 4.78}\n",
      "{'loss': 0.0126, 'grad_norm': 0.3021010756492615, 'learning_rate': 6.949694811913225e-05, 'epoch': 4.79}\n",
      "{'loss': 0.0088, 'grad_norm': 0.23437485098838806, 'learning_rate': 6.909830056250527e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0123, 'grad_norm': 0.25218629837036133, 'learning_rate': 6.870019520517217e-05, 'epoch': 4.81}\n",
      "{'loss': 0.0257, 'grad_norm': 0.22770747542381287, 'learning_rate': 6.830263903226483e-05, 'epoch': 4.83}\n",
      "{'loss': 0.0373, 'grad_norm': 0.5052791833877563, 'learning_rate': 6.790563901927907e-05, 'epoch': 4.84}\n",
      "{'loss': 0.0183, 'grad_norm': 0.44487860798835754, 'learning_rate': 6.750920213195238e-05, 'epoch': 4.85}\n",
      "{'loss': 0.0115, 'grad_norm': 0.34452927112579346, 'learning_rate': 6.711333532614168e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0461, 'grad_norm': 0.51466965675354, 'learning_rate': 6.671804554770135e-05, 'epoch': 4.87}\n",
      "{'loss': 0.0101, 'grad_norm': 0.2523607611656189, 'learning_rate': 6.63233397323612e-05, 'epoch': 4.88}\n",
      "{'loss': 0.0483, 'grad_norm': 0.6990631818771362, 'learning_rate': 6.592922480560483e-05, 'epoch': 4.89}\n",
      "{'loss': 0.0298, 'grad_norm': 0.34661492705345154, 'learning_rate': 6.55357076825483e-05, 'epoch': 4.9}\n",
      "{'loss': 0.0311, 'grad_norm': 0.4105773866176605, 'learning_rate': 6.51427952678185e-05, 'epoch': 4.91}\n",
      "{'loss': 0.0701, 'grad_norm': 0.5974554419517517, 'learning_rate': 6.475049445543215e-05, 'epoch': 4.92}\n",
      "{'loss': 0.0257, 'grad_norm': 0.24789860844612122, 'learning_rate': 6.435881212867493e-05, 'epoch': 4.93}\n",
      "{'loss': 0.0185, 'grad_norm': 0.22391466796398163, 'learning_rate': 6.396775515998055e-05, 'epoch': 4.94}\n",
      "{'loss': 0.0244, 'grad_norm': 0.39656659960746765, 'learning_rate': 6.357733041081018e-05, 'epoch': 4.95}\n",
      "{'loss': 0.022, 'grad_norm': 0.336032509803772, 'learning_rate': 6.318754473153221e-05, 'epoch': 4.96}\n",
      "{'loss': 0.0084, 'grad_norm': 0.2000938206911087, 'learning_rate': 6.27984049613019e-05, 'epoch': 4.97}\n",
      "{'loss': 0.023, 'grad_norm': 0.3100849390029907, 'learning_rate': 6.240991792794133e-05, 'epoch': 4.98}\n",
      "{'loss': 0.029, 'grad_norm': 0.5359669327735901, 'learning_rate': 6.20220904478199e-05, 'epoch': 4.99}\n",
      "{'loss': 0.0141, 'grad_norm': 0.6505672335624695, 'learning_rate': 6.163492932573438e-05, 'epoch': 5.0}\n",
      " 63%|█████████████████████████▉               | 480/760 [18:45<08:51,  1.90s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.05it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.33it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.22it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.98it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.72it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.62it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.21it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.34it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.36it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.40it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.26it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.23it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.11651933193206787, 'eval_runtime': 2.5829, 'eval_samples_per_second': 12.389, 'eval_steps_per_second': 6.195, 'epoch': 5.0}\n",
      " 63%|█████████████████████████▉               | 480/760 [18:47<08:51,  1.90s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "{'loss': 0.0431, 'grad_norm': 0.32771602272987366, 'learning_rate': 6.12484413547897e-05, 'epoch': 5.01}\n",
      "{'loss': 0.0883, 'grad_norm': 0.3239237070083618, 'learning_rate': 6.086263331627976e-05, 'epoch': 5.02}\n",
      "{'loss': 0.1063, 'grad_norm': 0.367118775844574, 'learning_rate': 6.047751197956838e-05, 'epoch': 5.03}\n",
      "{'loss': 0.0278, 'grad_norm': 0.1757706254720688, 'learning_rate': 6.009308410197047e-05, 'epoch': 5.04}\n",
      "{'loss': 0.0388, 'grad_norm': 0.40997007489204407, 'learning_rate': 5.9709356428633746e-05, 'epoch': 5.05}\n",
      "{'loss': 0.1387, 'grad_norm': 0.7056753039360046, 'learning_rate': 5.9326335692419995e-05, 'epoch': 5.06}\n",
      "{'loss': 0.0716, 'grad_norm': 0.4368675947189331, 'learning_rate': 5.8944028613787206e-05, 'epoch': 5.07}\n",
      "{'loss': 0.0201, 'grad_norm': 0.18588203191757202, 'learning_rate': 5.856244190067159e-05, 'epoch': 5.08}\n",
      "{'loss': 0.0392, 'grad_norm': 0.430826336145401, 'learning_rate': 5.818158224836987e-05, 'epoch': 5.09}\n",
      "{'loss': 0.0115, 'grad_norm': 0.14296534657478333, 'learning_rate': 5.780145633942173e-05, 'epoch': 5.1}\n",
      "{'loss': 0.0065, 'grad_norm': 0.18863767385482788, 'learning_rate': 5.7422070843492734e-05, 'epoch': 5.11}\n",
      "{'loss': 0.0139, 'grad_norm': 0.26080888509750366, 'learning_rate': 5.704343241725719e-05, 'epoch': 5.13}\n",
      "{'loss': 0.006, 'grad_norm': 0.1463232934474945, 'learning_rate': 5.666554770428129e-05, 'epoch': 5.14}\n",
      "{'loss': 0.0181, 'grad_norm': 0.3162533640861511, 'learning_rate': 5.6288423334906735e-05, 'epoch': 5.15}\n",
      "{'loss': 0.0022, 'grad_norm': 0.07146722823381424, 'learning_rate': 5.591206592613416e-05, 'epoch': 5.16}\n",
      "{'loss': 0.0032, 'grad_norm': 0.1252775937318802, 'learning_rate': 5.553648208150728e-05, 'epoch': 5.17}\n",
      "{'loss': 0.0042, 'grad_norm': 0.13214613497257233, 'learning_rate': 5.5161678390996796e-05, 'epoch': 5.18}\n",
      "{'loss': 0.0087, 'grad_norm': 0.15181826055049896, 'learning_rate': 5.478766143088492e-05, 'epoch': 5.19}\n",
      "{'loss': 0.0033, 'grad_norm': 0.2440393567085266, 'learning_rate': 5.441443776365003e-05, 'epoch': 5.2}\n",
      "{'loss': 0.0212, 'grad_norm': 0.42809921503067017, 'learning_rate': 5.404201393785122e-05, 'epoch': 5.21}\n",
      "{'loss': 0.0115, 'grad_norm': 0.15910851955413818, 'learning_rate': 5.3670396488013854e-05, 'epoch': 5.22}\n",
      "{'loss': 0.0194, 'grad_norm': 0.3072102665901184, 'learning_rate': 5.329959193451448e-05, 'epoch': 5.23}\n",
      "{'loss': 0.0065, 'grad_norm': 0.32705408334732056, 'learning_rate': 5.292960678346675e-05, 'epoch': 5.24}\n",
      "{'loss': 0.0278, 'grad_norm': 0.33739104866981506, 'learning_rate': 5.256044752660709e-05, 'epoch': 5.25}\n",
      "{'loss': 0.005, 'grad_norm': 0.12126118689775467, 'learning_rate': 5.2192120641180786e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0205, 'grad_norm': 0.24525509774684906, 'learning_rate': 5.182463258982846e-05, 'epoch': 5.27}\n",
      "{'loss': 0.014, 'grad_norm': 0.234189972281456, 'learning_rate': 5.145798982047261e-05, 'epoch': 5.28}\n",
      "{'loss': 0.0065, 'grad_norm': 0.46331408619880676, 'learning_rate': 5.1092198766204415e-05, 'epoch': 5.29}\n",
      "{'loss': 0.0053, 'grad_norm': 0.1919422447681427, 'learning_rate': 5.072726584517086e-05, 'epoch': 5.3}\n",
      "{'loss': 0.0116, 'grad_norm': 0.22945140302181244, 'learning_rate': 5.036319746046232e-05, 'epoch': 5.31}\n",
      "{'loss': 0.0165, 'grad_norm': 0.32577183842658997, 'learning_rate': 5.000000000000002e-05, 'epoch': 5.32}\n",
      "{'loss': 0.0253, 'grad_norm': 0.31743013858795166, 'learning_rate': 4.9637679836423924e-05, 'epoch': 5.33}\n",
      "{'loss': 0.0084, 'grad_norm': 0.20590034127235413, 'learning_rate': 4.927624332698109e-05, 'epoch': 5.34}\n",
      "{'loss': 0.0111, 'grad_norm': 0.2928810715675354, 'learning_rate': 4.8915696813414026e-05, 'epoch': 5.36}\n",
      "{'loss': 0.009, 'grad_norm': 0.1858905404806137, 'learning_rate': 4.8556046621849346e-05, 'epoch': 5.37}\n",
      "{'loss': 0.0089, 'grad_norm': 0.31801360845565796, 'learning_rate': 4.8197299062686995e-05, 'epoch': 5.38}\n",
      "{'loss': 0.0092, 'grad_norm': 0.2357792854309082, 'learning_rate': 4.783946043048923e-05, 'epoch': 5.39}\n",
      "{'loss': 0.0092, 'grad_norm': 0.19891782104969025, 'learning_rate': 4.748253700387042e-05, 'epoch': 5.4}\n",
      "{'loss': 0.0082, 'grad_norm': 0.39526432752609253, 'learning_rate': 4.712653504538683e-05, 'epoch': 5.41}\n",
      "{'loss': 0.0312, 'grad_norm': 0.4498332142829895, 'learning_rate': 4.6771460801426635e-05, 'epoch': 5.42}\n",
      "{'loss': 0.0049, 'grad_norm': 0.1928764134645462, 'learning_rate': 4.6417320502100316e-05, 'epoch': 5.43}\n",
      "{'loss': 0.0175, 'grad_norm': 0.2777620553970337, 'learning_rate': 4.6064120361131656e-05, 'epoch': 5.44}\n",
      "{'loss': 0.0062, 'grad_norm': 1.7778263092041016, 'learning_rate': 4.5711866575748276e-05, 'epoch': 5.45}\n",
      "{'loss': 0.0249, 'grad_norm': 0.3636281490325928, 'learning_rate': 4.5360565326573104e-05, 'epoch': 5.46}\n",
      "{'loss': 0.0286, 'grad_norm': 0.2564079165458679, 'learning_rate': 4.501022277751602e-05, 'epoch': 5.47}\n",
      "{'loss': 0.0086, 'grad_norm': 0.2486756294965744, 'learning_rate': 4.46608450756656e-05, 'epoch': 5.48}\n",
      "{'loss': 0.0229, 'grad_norm': 0.2929864227771759, 'learning_rate': 4.431243835118124e-05, 'epoch': 5.49}\n",
      "{'loss': 0.0056, 'grad_norm': 0.19093595445156097, 'learning_rate': 4.396500871718555e-05, 'epoch': 5.5}\n",
      " 69%|████████████████████████████▍            | 528/760 [20:48<07:55,  2.05s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.26it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.35it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.20it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.95it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.71it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.62it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.21it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.33it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.34it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.39it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1312233805656433, 'eval_runtime': 2.5943, 'eval_samples_per_second': 12.335, 'eval_steps_per_second': 6.167, 'epoch': 5.5}\n",
      " 69%|████████████████████████████▍            | 528/760 [20:51<07:55,  2.05s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "{'loss': 0.0454, 'grad_norm': 0.468344509601593, 'learning_rate': 4.361856226965733e-05, 'epoch': 5.51}\n",
      "{'loss': 0.0033, 'grad_norm': 0.13404667377471924, 'learning_rate': 4.327310508732437e-05, 'epoch': 5.52}\n",
      "{'loss': 0.0156, 'grad_norm': 0.22297640144824982, 'learning_rate': 4.2928643231556844e-05, 'epoch': 5.53}\n",
      "{'loss': 0.0037, 'grad_norm': 0.19472187757492065, 'learning_rate': 4.2585182746261035e-05, 'epoch': 5.54}\n",
      "{'loss': 0.0058, 'grad_norm': 0.1448320597410202, 'learning_rate': 4.224272965777326e-05, 'epoch': 5.55}\n",
      "{'loss': 0.015, 'grad_norm': 0.1622803956270218, 'learning_rate': 4.190128997475402e-05, 'epoch': 5.56}\n",
      "{'loss': 0.0083, 'grad_norm': 0.31444910168647766, 'learning_rate': 4.15608696880828e-05, 'epoch': 5.57}\n",
      "{'loss': 0.0036, 'grad_norm': 0.10936303436756134, 'learning_rate': 4.12214747707527e-05, 'epoch': 5.58}\n",
      "{'loss': 0.0371, 'grad_norm': 0.3685127794742584, 'learning_rate': 4.08831111777658e-05, 'epoch': 5.6}\n",
      "{'loss': 0.0182, 'grad_norm': 0.2648613154888153, 'learning_rate': 4.05457848460287e-05, 'epoch': 5.61}\n",
      "{'loss': 0.0297, 'grad_norm': 0.8503837585449219, 'learning_rate': 4.020950169424815e-05, 'epoch': 5.62}\n",
      "{'loss': 0.0036, 'grad_norm': 0.1371777206659317, 'learning_rate': 3.987426762282733e-05, 'epoch': 5.63}\n",
      "{'loss': 0.0056, 'grad_norm': 0.21092356741428375, 'learning_rate': 3.954008851376252e-05, 'epoch': 5.64}\n",
      "{'loss': 0.0176, 'grad_norm': 0.262574166059494, 'learning_rate': 3.920697023053949e-05, 'epoch': 5.65}\n",
      "{'loss': 0.0114, 'grad_norm': 0.4861820638179779, 'learning_rate': 3.887491861803085e-05, 'epoch': 5.66}\n",
      "{'loss': 0.0032, 'grad_norm': 0.35456591844558716, 'learning_rate': 3.854393950239355e-05, 'epoch': 5.67}\n",
      "{'loss': 0.0042, 'grad_norm': 0.18159645795822144, 'learning_rate': 3.821403869096658e-05, 'epoch': 5.68}\n",
      "{'loss': 0.0106, 'grad_norm': 0.277922123670578, 'learning_rate': 3.788522197216897e-05, 'epoch': 5.69}\n",
      "{'loss': 0.006, 'grad_norm': 0.21141867339611053, 'learning_rate': 3.755749511539845e-05, 'epoch': 5.7}\n",
      "{'loss': 0.0255, 'grad_norm': 0.34585198760032654, 'learning_rate': 3.7230863870929964e-05, 'epoch': 5.71}\n",
      "{'loss': 0.0102, 'grad_norm': 0.3528015911579132, 'learning_rate': 3.690533396981504e-05, 'epoch': 5.72}\n",
      "{'loss': 0.0082, 'grad_norm': 0.37781423330307007, 'learning_rate': 3.6580911123781056e-05, 'epoch': 5.73}\n",
      "{'loss': 0.0126, 'grad_norm': 0.19631563127040863, 'learning_rate': 3.6257601025131026e-05, 'epoch': 5.74}\n",
      "{'loss': 0.0087, 'grad_norm': 0.23892660439014435, 'learning_rate': 3.5935409346643835e-05, 'epoch': 5.75}\n",
      "{'loss': 0.0094, 'grad_norm': 0.2399459183216095, 'learning_rate': 3.561434174147463e-05, 'epoch': 5.76}\n",
      "{'loss': 0.0157, 'grad_norm': 0.39057520031929016, 'learning_rate': 3.52944038430556e-05, 'epoch': 5.77}\n",
      "{'loss': 0.0188, 'grad_norm': 0.1793043166399002, 'learning_rate': 3.497560126499709e-05, 'epoch': 5.78}\n",
      "{'loss': 0.0127, 'grad_norm': 0.20952680706977844, 'learning_rate': 3.465793960098945e-05, 'epoch': 5.79}\n",
      "{'loss': 0.0052, 'grad_norm': 0.17843638360500336, 'learning_rate': 3.4341424424704375e-05, 'epoch': 5.8}\n",
      "{'loss': 0.0312, 'grad_norm': 0.6840955018997192, 'learning_rate': 3.40260612896974e-05, 'epoch': 5.81}\n",
      "{'loss': 0.002, 'grad_norm': 0.08387085795402527, 'learning_rate': 3.371185572931048e-05, 'epoch': 5.83}\n",
      "{'loss': 0.0455, 'grad_norm': 0.7639410495758057, 'learning_rate': 3.339881325657484e-05, 'epoch': 5.84}\n",
      "{'loss': 0.0454, 'grad_norm': 0.39319178462028503, 'learning_rate': 3.308693936411421e-05, 'epoch': 5.85}\n",
      "{'loss': 0.0663, 'grad_norm': 0.3107658326625824, 'learning_rate': 3.277623952404842e-05, 'epoch': 5.86}\n",
      "{'loss': 0.0126, 'grad_norm': 0.18889714777469635, 'learning_rate': 3.246671918789755e-05, 'epoch': 5.87}\n",
      "{'loss': 0.0044, 'grad_norm': 0.169070303440094, 'learning_rate': 3.21583837864862e-05, 'epoch': 5.88}\n",
      "{'loss': 0.0037, 'grad_norm': 0.13926267623901367, 'learning_rate': 3.1851238729848034e-05, 'epoch': 5.89}\n",
      "{'loss': 0.0035, 'grad_norm': 0.09546536207199097, 'learning_rate': 3.154528940713113e-05, 'epoch': 5.9}\n",
      "{'loss': 0.0046, 'grad_norm': 0.22891852259635925, 'learning_rate': 3.124054118650327e-05, 'epoch': 5.91}\n",
      "{'loss': 0.0237, 'grad_norm': 0.3444746732711792, 'learning_rate': 3.093699941505771e-05, 'epoch': 5.92}\n",
      "{'loss': 0.0106, 'grad_norm': 0.19889502227306366, 'learning_rate': 3.063466941871952e-05, 'epoch': 5.93}\n",
      "{'loss': 0.0108, 'grad_norm': 0.26382631063461304, 'learning_rate': 3.0333556502151926e-05, 'epoch': 5.94}\n",
      "{'loss': 0.0093, 'grad_norm': 0.24077467620372772, 'learning_rate': 3.0033665948663448e-05, 'epoch': 5.95}\n",
      "{'loss': 0.0111, 'grad_norm': 0.1451781690120697, 'learning_rate': 2.9735003020115092e-05, 'epoch': 5.96}\n",
      "{'loss': 0.0055, 'grad_norm': 0.24732375144958496, 'learning_rate': 2.9437572956827964e-05, 'epoch': 5.97}\n",
      "{'loss': 0.0485, 'grad_norm': 0.5806713104248047, 'learning_rate': 2.9141380977491373e-05, 'epoch': 5.98}\n",
      "{'loss': 0.0027, 'grad_norm': 0.1906282752752304, 'learning_rate': 2.8846432279071467e-05, 'epoch': 5.99}\n",
      "{'loss': 0.0023, 'grad_norm': 0.12016003578901291, 'learning_rate': 2.8552732036719687e-05, 'epoch': 6.0}\n",
      " 76%|███████████████████████████████          | 576/760 [22:29<05:41,  1.85s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.96it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.31it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.22it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.98it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.72it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.60it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.21it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.33it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.36it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.41it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.12it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.26it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.23it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.12506011128425598, 'eval_runtime': 2.5804, 'eval_samples_per_second': 12.401, 'eval_steps_per_second': 6.201, 'epoch': 6.0}\n",
      " 76%|███████████████████████████████          | 576/760 [22:31<05:41,  1.85s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "{'loss': 0.0542, 'grad_norm': 0.36268121004104614, 'learning_rate': 2.826028540368215e-05, 'epoch': 6.01}\n",
      "{'loss': 0.0751, 'grad_norm': 0.3171575665473938, 'learning_rate': 2.7969097511209308e-05, 'epoch': 6.02}\n",
      "{'loss': 0.0607, 'grad_norm': 0.4059193432331085, 'learning_rate': 2.7679173468465812e-05, 'epoch': 6.03}\n",
      "{'loss': 0.0982, 'grad_norm': 0.4933318495750427, 'learning_rate': 2.7390518362440808e-05, 'epoch': 6.04}\n",
      "{'loss': 0.0357, 'grad_norm': 0.3054932951927185, 'learning_rate': 2.7103137257858868e-05, 'epoch': 6.05}\n",
      "{'loss': 0.0369, 'grad_norm': 0.3793582022190094, 'learning_rate': 2.681703519709089e-05, 'epoch': 6.06}\n",
      "{'loss': 0.0303, 'grad_norm': 0.26980242133140564, 'learning_rate': 2.6532217200065858e-05, 'epoch': 6.07}\n",
      "{'loss': 0.0022, 'grad_norm': 0.09217172116041183, 'learning_rate': 2.624868826418262e-05, 'epoch': 6.08}\n",
      "{'loss': 0.0017, 'grad_norm': 0.048476703464984894, 'learning_rate': 2.5966453364222186e-05, 'epoch': 6.09}\n",
      "{'loss': 0.0172, 'grad_norm': 0.17674648761749268, 'learning_rate': 2.5685517452260567e-05, 'epoch': 6.1}\n",
      "{'loss': 0.0045, 'grad_norm': 0.12620532512664795, 'learning_rate': 2.540588545758179e-05, 'epoch': 6.11}\n",
      "{'loss': 0.0042, 'grad_norm': 0.15594202280044556, 'learning_rate': 2.512756228659141e-05, 'epoch': 6.13}\n",
      "{'loss': 0.0212, 'grad_norm': 0.36936479806900024, 'learning_rate': 2.48505528227304e-05, 'epoch': 6.14}\n",
      "{'loss': 0.0164, 'grad_norm': 0.2207775115966797, 'learning_rate': 2.4574861926389615e-05, 'epoch': 6.15}\n",
      "{'loss': 0.0108, 'grad_norm': 0.1562609076499939, 'learning_rate': 2.4300494434824373e-05, 'epoch': 6.16}\n",
      "{'loss': 0.005, 'grad_norm': 0.18210221827030182, 'learning_rate': 2.4027455162069567e-05, 'epoch': 6.17}\n",
      "{'loss': 0.0164, 'grad_norm': 0.22869712114334106, 'learning_rate': 2.37557488988552e-05, 'epoch': 6.18}\n",
      "{'loss': 0.0497, 'grad_norm': 0.3760044574737549, 'learning_rate': 2.3485380412522585e-05, 'epoch': 6.19}\n",
      "{'loss': 0.0058, 'grad_norm': 0.268698126077652, 'learning_rate': 2.321635444694028e-05, 'epoch': 6.2}\n",
      "{'loss': 0.0132, 'grad_norm': 0.2056495100259781, 'learning_rate': 2.2948675722421086e-05, 'epoch': 6.21}\n",
      "{'loss': 0.0021, 'grad_norm': 0.2464718520641327, 'learning_rate': 2.2682348935639274e-05, 'epoch': 6.22}\n",
      "{'loss': 0.0131, 'grad_norm': 0.14258818328380585, 'learning_rate': 2.241737875954808e-05, 'epoch': 6.23}\n",
      "{'loss': 0.0204, 'grad_norm': 0.1621377319097519, 'learning_rate': 2.2153769843297667e-05, 'epoch': 6.24}\n",
      "{'loss': 0.0038, 'grad_norm': 0.14930187165737152, 'learning_rate': 2.1891526812153672e-05, 'epoch': 6.25}\n",
      "{'loss': 0.0097, 'grad_norm': 0.13637861609458923, 'learning_rate': 2.163065426741603e-05, 'epoch': 6.26}\n",
      "{'loss': 0.0025, 'grad_norm': 0.08534826338291168, 'learning_rate': 2.137115678633811e-05, 'epoch': 6.27}\n",
      "{'loss': 0.0093, 'grad_norm': 0.1316729187965393, 'learning_rate': 2.1113038922046602e-05, 'epoch': 6.28}\n",
      "{'loss': 0.0021, 'grad_norm': 0.06964140385389328, 'learning_rate': 2.0856305203461436e-05, 'epoch': 6.29}\n",
      "{'loss': 0.0049, 'grad_norm': 0.2698250710964203, 'learning_rate': 2.0600960135216462e-05, 'epoch': 6.3}\n",
      "{'loss': 0.01, 'grad_norm': 0.2040054351091385, 'learning_rate': 2.0347008197580374e-05, 'epoch': 6.31}\n",
      "{'loss': 0.0091, 'grad_norm': 0.1366085261106491, 'learning_rate': 2.009445384637805e-05, 'epoch': 6.32}\n",
      "{'loss': 0.0054, 'grad_norm': 0.11843543499708176, 'learning_rate': 1.9843301512912327e-05, 'epoch': 6.33}\n",
      "{'loss': 0.0017, 'grad_norm': 0.06228170916438103, 'learning_rate': 1.9593555603886538e-05, 'epoch': 6.34}\n",
      "{'loss': 0.0011, 'grad_norm': 0.03905496001243591, 'learning_rate': 1.9345220501326777e-05, 'epoch': 6.36}\n",
      "{'loss': 0.0118, 'grad_norm': 0.23792679607868195, 'learning_rate': 1.9098300562505266e-05, 'epoch': 6.37}\n",
      "{'loss': 0.0029, 'grad_norm': 0.3346891701221466, 'learning_rate': 1.885280011986391e-05, 'epoch': 6.38}\n",
      "{'loss': 0.0024, 'grad_norm': 0.06949459761381149, 'learning_rate': 1.8608723480938206e-05, 'epoch': 6.39}\n",
      "{'loss': 0.0036, 'grad_norm': 0.28481805324554443, 'learning_rate': 1.8366074928281607e-05, 'epoch': 6.4}\n",
      "{'loss': 0.0083, 'grad_norm': 0.2589883804321289, 'learning_rate': 1.812485871939056e-05, 'epoch': 6.41}\n",
      "{'loss': 0.0027, 'grad_norm': 0.09664390981197357, 'learning_rate': 1.78850790866296e-05, 'epoch': 6.42}\n",
      "{'loss': 0.0045, 'grad_norm': 0.24821260571479797, 'learning_rate': 1.7646740237157256e-05, 'epoch': 6.43}\n",
      "{'loss': 0.0036, 'grad_norm': 0.10486159473657608, 'learning_rate': 1.7409846352852143e-05, 'epoch': 6.44}\n",
      "{'loss': 0.008, 'grad_norm': 0.3739469051361084, 'learning_rate': 1.7174401590239587e-05, 'epoch': 6.45}\n",
      "{'loss': 0.018, 'grad_norm': 0.20662471652030945, 'learning_rate': 1.6940410080418723e-05, 'epoch': 6.46}\n",
      "{'loss': 0.0175, 'grad_norm': 0.17368671298027039, 'learning_rate': 1.6707875928990058e-05, 'epoch': 6.47}\n",
      "{'loss': 0.0225, 'grad_norm': 0.2773002088069916, 'learning_rate': 1.6476803215983294e-05, 'epoch': 6.48}\n",
      "{'loss': 0.0019, 'grad_norm': 0.07287110388278961, 'learning_rate': 1.6247195995785837e-05, 'epoch': 6.49}\n",
      "{'loss': 0.0084, 'grad_norm': 0.12341891974210739, 'learning_rate': 1.601905829707171e-05, 'epoch': 6.5}\n",
      " 82%|█████████████████████████████████▋       | 624/760 [24:32<04:43,  2.08s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.05it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.31it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.95it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.69it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.60it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  5.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.19it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.28it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.18it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.13582447171211243, 'eval_runtime': 2.6312, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 6.081, 'epoch': 6.5}\n",
      " 82%|█████████████████████████████████▋       | 624/760 [24:35<04:43,  2.08s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.38it/s]\u001b[A\n",
      "{'loss': 0.0029, 'grad_norm': 0.12880027294158936, 'learning_rate': 1.579239412273078e-05, 'epoch': 6.51}\n",
      "{'loss': 0.0102, 'grad_norm': 0.25577273964881897, 'learning_rate': 1.5567207449798515e-05, 'epoch': 6.52}\n",
      "{'loss': 0.0054, 'grad_norm': 0.20792889595031738, 'learning_rate': 1.5343502229386207e-05, 'epoch': 6.53}\n",
      "{'loss': 0.0076, 'grad_norm': 0.15837036073207855, 'learning_rate': 1.5121282386611824e-05, 'epoch': 6.54}\n",
      "{'loss': 0.0033, 'grad_norm': 0.09539451450109482, 'learning_rate': 1.4900551820530828e-05, 'epoch': 6.55}\n",
      "{'loss': 0.0094, 'grad_norm': 0.1114104613661766, 'learning_rate': 1.468131440406798e-05, 'epoch': 6.56}\n",
      "{'loss': 0.0478, 'grad_norm': 0.30011969804763794, 'learning_rate': 1.4463573983949341e-05, 'epoch': 6.57}\n",
      "{'loss': 0.011, 'grad_norm': 0.19236719608306885, 'learning_rate': 1.4247334380634792e-05, 'epoch': 6.58}\n",
      "{'loss': 0.0092, 'grad_norm': 0.18055498600006104, 'learning_rate': 1.40325993882509e-05, 'epoch': 6.6}\n",
      "{'loss': 0.0116, 'grad_norm': 0.18531836569309235, 'learning_rate': 1.3819372774524508e-05, 'epoch': 6.61}\n",
      "{'loss': 0.0135, 'grad_norm': 0.2531220018863678, 'learning_rate': 1.3607658280716473e-05, 'epoch': 6.62}\n",
      "{'loss': 0.0043, 'grad_norm': 0.13314688205718994, 'learning_rate': 1.339745962155613e-05, 'epoch': 6.63}\n",
      "{'loss': 0.0017, 'grad_norm': 0.05050952360033989, 'learning_rate': 1.3188780485176088e-05, 'epoch': 6.64}\n",
      "{'loss': 0.0046, 'grad_norm': 0.14483760297298431, 'learning_rate': 1.2981624533047432e-05, 'epoch': 6.65}\n",
      "{'loss': 0.0017, 'grad_norm': 0.05965028330683708, 'learning_rate': 1.2775995399915631e-05, 'epoch': 6.66}\n",
      "{'loss': 0.0048, 'grad_norm': 0.1452692449092865, 'learning_rate': 1.257189669373664e-05, 'epoch': 6.67}\n",
      "{'loss': 0.0132, 'grad_norm': 0.2442743480205536, 'learning_rate': 1.2369331995613665e-05, 'epoch': 6.68}\n",
      "{'loss': 0.0087, 'grad_norm': 0.1466926634311676, 'learning_rate': 1.2168304859734226e-05, 'epoch': 6.69}\n",
      "{'loss': 0.0149, 'grad_norm': 0.15964466333389282, 'learning_rate': 1.196881881330798e-05, 'epoch': 6.7}\n",
      "{'loss': 0.0097, 'grad_norm': 0.2556782364845276, 'learning_rate': 1.1770877356504683e-05, 'epoch': 6.71}\n",
      "{'loss': 0.0013, 'grad_norm': 0.08626535534858704, 'learning_rate': 1.1574483962392767e-05, 'epoch': 6.72}\n",
      "{'loss': 0.0094, 'grad_norm': 0.1120123341679573, 'learning_rate': 1.1379642076878527e-05, 'epoch': 6.73}\n",
      "{'loss': 0.0179, 'grad_norm': 0.3880700469017029, 'learning_rate': 1.1186355118645554e-05, 'epoch': 6.74}\n",
      "{'loss': 0.0083, 'grad_norm': 0.152547225356102, 'learning_rate': 1.099462647909475e-05, 'epoch': 6.75}\n",
      "{'loss': 0.0095, 'grad_norm': 0.21727481484413147, 'learning_rate': 1.0804459522284926e-05, 'epoch': 6.76}\n",
      "{'loss': 0.0124, 'grad_norm': 0.27079910039901733, 'learning_rate': 1.0615857584873623e-05, 'epoch': 6.77}\n",
      "{'loss': 0.0027, 'grad_norm': 0.15444406867027283, 'learning_rate': 1.042882397605871e-05, 'epoch': 6.78}\n",
      "{'loss': 0.011, 'grad_norm': 0.1469441056251526, 'learning_rate': 1.0243361977520249e-05, 'epoch': 6.79}\n",
      "{'loss': 0.0016, 'grad_norm': 0.08174693584442139, 'learning_rate': 1.0059474843362892e-05, 'epoch': 6.8}\n",
      "{'loss': 0.0108, 'grad_norm': 0.19114121794700623, 'learning_rate': 9.877165800058874e-06, 'epoch': 6.81}\n",
      "{'loss': 0.0015, 'grad_norm': 0.05900418013334274, 'learning_rate': 9.696438046391288e-06, 'epoch': 6.83}\n",
      "{'loss': 0.0098, 'grad_norm': 0.12379857897758484, 'learning_rate': 9.517294753398064e-06, 'epoch': 6.84}\n",
      "{'loss': 0.0232, 'grad_norm': 0.28320756554603577, 'learning_rate': 9.339739064316233e-06, 'epoch': 6.85}\n",
      "{'loss': 0.0048, 'grad_norm': 0.3456208407878876, 'learning_rate': 9.163774094526889e-06, 'epoch': 6.86}\n",
      "{'loss': 0.0049, 'grad_norm': 0.19737190008163452, 'learning_rate': 8.989402931500434e-06, 'epoch': 6.87}\n",
      "{'loss': 0.001, 'grad_norm': 0.036924201995134354, 'learning_rate': 8.816628634742441e-06, 'epoch': 6.88}\n",
      "{'loss': 0.0048, 'grad_norm': 0.17786051332950592, 'learning_rate': 8.645454235739903e-06, 'epoch': 6.89}\n",
      "{'loss': 0.0025, 'grad_norm': 0.11626952886581421, 'learning_rate': 8.475882737908248e-06, 'epoch': 6.9}\n",
      "{'loss': 0.0118, 'grad_norm': 0.2065410017967224, 'learning_rate': 8.307917116538378e-06, 'epoch': 6.91}\n",
      "{'loss': 0.0056, 'grad_norm': 0.13060228526592255, 'learning_rate': 8.1415603187446e-06, 'epoch': 6.92}\n",
      "{'loss': 0.0086, 'grad_norm': 0.19013762474060059, 'learning_rate': 7.976815263412963e-06, 'epoch': 6.93}\n",
      "{'loss': 0.0015, 'grad_norm': 0.05133756995201111, 'learning_rate': 7.81368484114996e-06, 'epoch': 6.94}\n",
      "{'loss': 0.0107, 'grad_norm': 0.2045932561159134, 'learning_rate': 7.652171914231776e-06, 'epoch': 6.95}\n",
      "{'loss': 0.0087, 'grad_norm': 0.16702137887477875, 'learning_rate': 7.492279316554207e-06, 'epoch': 6.96}\n",
      "{'loss': 0.006, 'grad_norm': 0.2655309736728668, 'learning_rate': 7.3340098535827905e-06, 'epoch': 6.97}\n",
      "{'loss': 0.0019, 'grad_norm': 0.08597064763307571, 'learning_rate': 7.177366302303667e-06, 'epoch': 6.98}\n",
      "{'loss': 0.0082, 'grad_norm': 0.1756902039051056, 'learning_rate': 7.022351411174866e-06, 'epoch': 6.99}\n",
      "{'loss': 0.0021, 'grad_norm': 0.08517833054065704, 'learning_rate': 6.868967900077972e-06, 'epoch': 7.0}\n",
      " 88%|████████████████████████████████████▎    | 672/760 [26:14<02:45,  1.88s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 13.22it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.38it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.22it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.97it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.72it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.62it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.33it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.35it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.40it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.26it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.23it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.13465473055839539, 'eval_runtime': 2.5858, 'eval_samples_per_second': 12.375, 'eval_steps_per_second': 6.188, 'epoch': 7.0}\n",
      " 88%|████████████████████████████████████▎    | 672/760 [26:16<02:45,  1.88s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "{'loss': 0.0386, 'grad_norm': 0.3114256262779236, 'learning_rate': 6.717218460270536e-06, 'epoch': 7.01}\n",
      "{'loss': 0.0473, 'grad_norm': 0.40816739201545715, 'learning_rate': 6.5671057543387985e-06, 'epoch': 7.02}\n",
      "{'loss': 0.0145, 'grad_norm': 0.18760795891284943, 'learning_rate': 6.418632416150927e-06, 'epoch': 7.03}\n",
      "{'loss': 0.1135, 'grad_norm': 0.4408634901046753, 'learning_rate': 6.2718010508108545e-06, 'epoch': 7.04}\n",
      "{'loss': 0.0048, 'grad_norm': 0.1197180300951004, 'learning_rate': 6.126614234612593e-06, 'epoch': 7.05}\n",
      "{'loss': 0.058, 'grad_norm': 0.4091624915599823, 'learning_rate': 5.98307451499498e-06, 'epoch': 7.06}\n",
      "{'loss': 0.042, 'grad_norm': 0.2541452944278717, 'learning_rate': 5.8411844104969916e-06, 'epoch': 7.07}\n",
      "{'loss': 0.0021, 'grad_norm': 0.08697552233934402, 'learning_rate': 5.700946410713548e-06, 'epoch': 7.08}\n",
      "{'loss': 0.0337, 'grad_norm': 0.41935163736343384, 'learning_rate': 5.562362976251901e-06, 'epoch': 7.09}\n",
      "{'loss': 0.0023, 'grad_norm': 0.10263185948133469, 'learning_rate': 5.425436538688322e-06, 'epoch': 7.1}\n",
      "{'loss': 0.0173, 'grad_norm': 0.20659120380878448, 'learning_rate': 5.290169500525577e-06, 'epoch': 7.11}\n",
      "{'loss': 0.0061, 'grad_norm': 0.11036378890275955, 'learning_rate': 5.1565642351506845e-06, 'epoch': 7.13}\n",
      "{'loss': 0.0065, 'grad_norm': 0.15315081179141998, 'learning_rate': 5.024623086793323e-06, 'epoch': 7.14}\n",
      "{'loss': 0.0038, 'grad_norm': 0.10505345463752747, 'learning_rate': 4.8943483704846475e-06, 'epoch': 7.15}\n",
      "{'loss': 0.0046, 'grad_norm': 0.18634609878063202, 'learning_rate': 4.765742372016735e-06, 'epoch': 7.16}\n",
      "{'loss': 0.0073, 'grad_norm': 0.1353108435869217, 'learning_rate': 4.638807347902408e-06, 'epoch': 7.17}\n",
      "{'loss': 0.0104, 'grad_norm': 0.12786811590194702, 'learning_rate': 4.513545525335705e-06, 'epoch': 7.18}\n",
      "{'loss': 0.0055, 'grad_norm': 0.15565474331378937, 'learning_rate': 4.389959102152774e-06, 'epoch': 7.19}\n",
      "{'loss': 0.0026, 'grad_norm': 0.09751083701848984, 'learning_rate': 4.268050246793276e-06, 'epoch': 7.2}\n",
      "{'loss': 0.0133, 'grad_norm': 0.15568611025810242, 'learning_rate': 4.147821098262405e-06, 'epoch': 7.21}\n",
      "{'loss': 0.0014, 'grad_norm': 0.043706510215997696, 'learning_rate': 4.029273766093333e-06, 'epoch': 7.22}\n",
      "{'loss': 0.0013, 'grad_norm': 0.05946275591850281, 'learning_rate': 3.912410330310156e-06, 'epoch': 7.23}\n",
      "{'loss': 0.0016, 'grad_norm': 0.06122426688671112, 'learning_rate': 3.797232841391407e-06, 'epoch': 7.24}\n",
      "{'loss': 0.002, 'grad_norm': 0.08477184176445007, 'learning_rate': 3.68374332023419e-06, 'epoch': 7.25}\n",
      "{'loss': 0.008, 'grad_norm': 0.15330766141414642, 'learning_rate': 3.5719437581185454e-06, 'epoch': 7.26}\n",
      "{'loss': 0.0021, 'grad_norm': 0.06161525845527649, 'learning_rate': 3.461836116672612e-06, 'epoch': 7.27}\n",
      "{'loss': 0.0032, 'grad_norm': 0.09545811265707016, 'learning_rate': 3.3534223278382405e-06, 'epoch': 7.28}\n",
      "{'loss': 0.003, 'grad_norm': 0.09110086411237717, 'learning_rate': 3.246704293837011e-06, 'epoch': 7.29}\n",
      "{'loss': 0.0014, 'grad_norm': 0.042435839772224426, 'learning_rate': 3.1416838871368924e-06, 'epoch': 7.3}\n",
      "{'loss': 0.0024, 'grad_norm': 0.1189589723944664, 'learning_rate': 3.0383629504194046e-06, 'epoch': 7.31}\n",
      "{'loss': 0.0017, 'grad_norm': 0.11187068372964859, 'learning_rate': 2.936743296547273e-06, 'epoch': 7.32}\n",
      "{'loss': 0.0155, 'grad_norm': 0.23416335880756378, 'learning_rate': 2.836826708532603e-06, 'epoch': 7.33}\n",
      "{'loss': 0.0073, 'grad_norm': 0.17887850105762482, 'learning_rate': 2.738614939505646e-06, 'epoch': 7.34}\n",
      "{'loss': 0.0139, 'grad_norm': 0.13912194967269897, 'learning_rate': 2.6421097126839712e-06, 'epoch': 7.36}\n",
      "{'loss': 0.0042, 'grad_norm': 0.09798655658960342, 'learning_rate': 2.5473127213422763e-06, 'epoch': 7.37}\n",
      "{'loss': 0.0064, 'grad_norm': 0.10353674739599228, 'learning_rate': 2.4542256287826914e-06, 'epoch': 7.38}\n",
      "{'loss': 0.0016, 'grad_norm': 0.05575662851333618, 'learning_rate': 2.3628500683055222e-06, 'epoch': 7.39}\n",
      "{'loss': 0.0018, 'grad_norm': 0.057508643716573715, 'learning_rate': 2.273187643180652e-06, 'epoch': 7.4}\n",
      "{'loss': 0.0106, 'grad_norm': 0.12334007024765015, 'learning_rate': 2.1852399266194314e-06, 'epoch': 7.41}\n",
      "{'loss': 0.0057, 'grad_norm': 0.15555477142333984, 'learning_rate': 2.0990084617470206e-06, 'epoch': 7.42}\n",
      "{'loss': 0.0272, 'grad_norm': 0.26660868525505066, 'learning_rate': 2.014494761575314e-06, 'epoch': 7.43}\n",
      "{'loss': 0.003, 'grad_norm': 0.10505358129739761, 'learning_rate': 1.931700308976436e-06, 'epoch': 7.44}\n",
      "{'loss': 0.0033, 'grad_norm': 0.1509777307510376, 'learning_rate': 1.8506265566567094e-06, 'epoch': 7.45}\n",
      "{'loss': 0.0018, 'grad_norm': 0.08387341350317001, 'learning_rate': 1.771274927131139e-06, 'epoch': 7.46}\n",
      "{'loss': 0.0047, 'grad_norm': 0.15692850947380066, 'learning_rate': 1.6936468126984572e-06, 'epoch': 7.47}\n",
      "{'loss': 0.0084, 'grad_norm': 0.14327189326286316, 'learning_rate': 1.6177435754167415e-06, 'epoch': 7.48}\n",
      "{'loss': 0.0111, 'grad_norm': 0.2507287561893463, 'learning_rate': 1.543566547079467e-06, 'epoch': 7.49}\n",
      "{'loss': 0.0022, 'grad_norm': 0.06888259947299957, 'learning_rate': 1.4711170291921484e-06, 'epoch': 7.5}\n",
      " 95%|██████████████████████████████████████▊  | 720/760 [28:17<01:21,  2.04s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:00<00:01, 12.91it/s]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:00<00:01,  8.28it/s]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:00<00:01,  6.93it/s]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:00<00:01,  6.68it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:01<00:01,  6.59it/s]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  6.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:01<00:00,  6.31it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:01<00:00,  6.33it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:01<00:00,  6.38it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:01<00:00,  6.10it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  6.24it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.13417837023735046, 'eval_runtime': 2.5923, 'eval_samples_per_second': 12.344, 'eval_steps_per_second': 6.172, 'epoch': 7.5}\n",
      " 95%|██████████████████████████████████████▊  | 720/760 [28:20<01:21,  2.04s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  6.42it/s]\u001b[A\n",
      "{'loss': 0.003, 'grad_norm': 0.09260901808738708, 'learning_rate': 1.400396292949513e-06, 'epoch': 7.51}\n",
      "{'loss': 0.002, 'grad_norm': 0.11103509366512299, 'learning_rate': 1.3314055792131964e-06, 'epoch': 7.52}\n",
      "{'loss': 0.0015, 'grad_norm': 0.06247953698039055, 'learning_rate': 1.26414609848996e-06, 'epoch': 7.53}\n",
      "{'loss': 0.0054, 'grad_norm': 0.12732073664665222, 'learning_rate': 1.1986190309104861e-06, 'epoch': 7.54}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0673082247376442, 'learning_rate': 1.134825526208605e-06, 'epoch': 7.55}\n",
      "{'loss': 0.002, 'grad_norm': 0.06640744209289551, 'learning_rate': 1.0727667037011668e-06, 'epoch': 7.56}\n",
      "{'loss': 0.003, 'grad_norm': 0.07661660015583038, 'learning_rate': 1.0124436522684243e-06, 'epoch': 7.57}\n",
      "{'loss': 0.0025, 'grad_norm': 0.08476044982671738, 'learning_rate': 9.538574303348813e-07, 'epoch': 7.58}\n",
      "{'loss': 0.0014, 'grad_norm': 0.04548582062125206, 'learning_rate': 8.970090658507291e-07, 'epoch': 7.6}\n",
      "{'loss': 0.0033, 'grad_norm': 0.13446949422359467, 'learning_rate': 8.418995562738285e-07, 'epoch': 7.61}\n",
      "{'loss': 0.0034, 'grad_norm': 0.13153740763664246, 'learning_rate': 7.885298685522235e-07, 'epoch': 7.62}\n",
      "{'loss': 0.0241, 'grad_norm': 0.2567020654678345, 'learning_rate': 7.369009391070992e-07, 'epoch': 7.63}\n",
      "{'loss': 0.0181, 'grad_norm': 0.16289037466049194, 'learning_rate': 6.870136738164612e-07, 'epoch': 7.64}\n",
      "{'loss': 0.003, 'grad_norm': 0.10241642594337463, 'learning_rate': 6.388689479991605e-07, 'epoch': 7.65}\n",
      "{'loss': 0.0189, 'grad_norm': 0.2807376980781555, 'learning_rate': 5.924676063995382e-07, 'epoch': 7.66}\n",
      "{'loss': 0.0032, 'grad_norm': 0.1168028712272644, 'learning_rate': 5.478104631726711e-07, 'epoch': 7.67}\n",
      "{'loss': 0.0018, 'grad_norm': 0.07033491134643555, 'learning_rate': 5.048983018699827e-07, 'epoch': 7.68}\n",
      "{'loss': 0.0033, 'grad_norm': 0.1515670269727707, 'learning_rate': 4.6373187542561035e-07, 'epoch': 7.69}\n",
      "{'loss': 0.012, 'grad_norm': 0.23056279122829437, 'learning_rate': 4.2431190614309335e-07, 'epoch': 7.7}\n",
      "{'loss': 0.0131, 'grad_norm': 0.19696325063705444, 'learning_rate': 3.866390856827495e-07, 'epoch': 7.71}\n",
      "{'loss': 0.0089, 'grad_norm': 0.14182867109775543, 'learning_rate': 3.50714075049563e-07, 'epoch': 7.72}\n",
      "{'loss': 0.0169, 'grad_norm': 0.1988362967967987, 'learning_rate': 3.1653750458152666e-07, 'epoch': 7.73}\n",
      "{'loss': 0.0091, 'grad_norm': 0.1561334878206253, 'learning_rate': 2.841099739386066e-07, 'epoch': 7.74}\n",
      "{'loss': 0.0327, 'grad_norm': 0.3096162676811218, 'learning_rate': 2.534320520922506e-07, 'epoch': 7.75}\n",
      "{'loss': 0.0021, 'grad_norm': 0.06607908755540848, 'learning_rate': 2.2450427731534053e-07, 'epoch': 7.76}\n",
      "{'loss': 0.0028, 'grad_norm': 0.07638555765151978, 'learning_rate': 1.973271571728441e-07, 'epoch': 7.77}\n",
      "{'loss': 0.0048, 'grad_norm': 0.22279946506023407, 'learning_rate': 1.7190116851280026e-07, 'epoch': 7.78}\n",
      "{'loss': 0.0274, 'grad_norm': 0.31813883781433105, 'learning_rate': 1.4822675745801429e-07, 'epoch': 7.79}\n",
      "{'loss': 0.0012, 'grad_norm': 0.05908902361989021, 'learning_rate': 1.2630433939825327e-07, 'epoch': 7.8}\n",
      "{'loss': 0.0373, 'grad_norm': 0.3144456446170807, 'learning_rate': 1.0613429898287398e-07, 'epoch': 7.81}\n",
      "{'loss': 0.0013, 'grad_norm': 0.03522561118006706, 'learning_rate': 8.771699011416168e-08, 'epoch': 7.83}\n",
      "{'loss': 0.0189, 'grad_norm': 0.1990056335926056, 'learning_rate': 7.105273594107953e-08, 'epoch': 7.84}\n",
      "{'loss': 0.0071, 'grad_norm': 0.08687062561511993, 'learning_rate': 5.6141828853573106e-08, 'epoch': 7.85}\n",
      "{'loss': 0.0049, 'grad_norm': 0.12564994394779205, 'learning_rate': 4.298453047749673e-08, 'epoch': 7.86}\n",
      "{'loss': 0.0081, 'grad_norm': 0.0994255393743515, 'learning_rate': 3.1581071670006015e-08, 'epoch': 7.87}\n",
      "{'loss': 0.0026, 'grad_norm': 0.09552478790283203, 'learning_rate': 2.193165251545004e-08, 'epoch': 7.88}\n",
      "{'loss': 0.0113, 'grad_norm': 0.08950307965278625, 'learning_rate': 1.4036442321962995e-08, 'epoch': 7.89}\n",
      "{'loss': 0.0134, 'grad_norm': 0.24224001169204712, 'learning_rate': 7.895579618388827e-09, 'epoch': 7.9}\n",
      "{'loss': 0.002, 'grad_norm': 0.06592006236314774, 'learning_rate': 3.509172151938689e-09, 'epoch': 7.91}\n",
      "{'loss': 0.0132, 'grad_norm': 0.2529037296772003, 'learning_rate': 8.772968862369446e-10, 'epoch': 7.92}\n",
      "{'train_runtime': 1783.4076, 'train_samples_per_second': 3.409, 'train_steps_per_second': 0.426, 'train_loss': 0.07527080466373406, 'epoch': 7.92}\n",
      "100%|█████████████████████████████████████████| 760/760 [29:43<00:00,  2.35s/it]\n",
      "[2025-10-10 10:55:22,064] [INFO] [axolotl.train.save_trained_model:244] [PID:3628323] [RANK:0] Training completed! Saving trained model to ./out-Qwen2.5-0.5B-Instruct.\u001b[39m\n",
      "[2025-10-10 10:55:22,487] [INFO] [axolotl.train.save_trained_model:341] [PID:3628323] [RANK:0] Model successfully saved to ./out-Qwen2.5-0.5B-Instruct\u001b[39m\n",
      "\u001b[0mCPU times: user 12.7 s, sys: 1.86 s, total: 14.6 s\n",
      "Wall time: 34min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the LoRA/DoRA into the base model (for inference & quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-10 10:55:37,951] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3632280] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 10:55:37,952] [WARNING] [axolotl.utils.schemas.config.check_sample_packing_wo_flash:482] [PID:3632280] [RANK:0] sample_packing without flash, sdp, xformers or flex attention does not handle cross sample decontamination.\u001b[39m\n",
      "\u001b[33m[2025-10-10 10:55:37,952] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3632280] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 10:55:38,214] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3632280] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 10:55:38,228] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:318] [PID:3632280] [RANK:0] loading tokenizer... Qwen/Qwen2.5-0.5B-Instruct\u001b[39m\n",
      "[2025-10-10 10:55:38,858] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:3632280] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-10-10 10:55:38,858] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:321] [PID:3632280] [RANK:0] loading model...\u001b[39m\n",
      "[2025-10-10 10:55:40,586] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3632280] [RANK:0] cuda memory usage after model load: 0.920GB (+0.263GB cache, +1.223GB misc)\u001b[39m\n",
      "[2025-10-10 10:55:40,606] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3632280] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 10:55:40,608] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:3632280] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
      "[2025-10-10 10:55:41,205] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3632280] [RANK:0] cuda memory usage after adapters: 0.953GB (+0.805GB cache, +1.301GB misc)\u001b[39m\n",
      "[2025-10-10 10:55:41,627] [INFO] [axolotl.cli.merge_lora.do_merge_lora:31] [PID:3632280] [RANK:0] Running merge of LoRA with base model...\u001b[39m\n",
      "Unloading and merging model: 100%|██████████| 487/487 [00:00<00:00, 2714.22it/s]\n",
      "[2025-10-10 10:55:41,811] [INFO] [axolotl.cli.merge_lora.do_merge_lora:44] [PID:3632280] [RANK:0] Saving merged model to: out-Qwen2.5-0.5B-Instruct/merged...\u001b[39m\n",
      "\u001b[0mCPU times: user 180 ms, sys: 48.6 ms, total: 228 ms\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/axolotl merge-lora {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 10:56:59 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-10 10:57:10 [utils.py:328] non-default args: {'max_model_len': 8192, 'disable_log_stats': True, 'model': 'out-Qwen2.5-0.5B-Instruct/merged'}\n",
      "INFO 10-10 10:57:32 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO 10-10 10:57:32 [__init__.py:1815] Using max model len 8192\n",
      "INFO 10-10 10:57:34 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:35 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:35 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='out-Qwen2.5-0.5B-Instruct/merged', speculative_config=None, tokenizer='out-Qwen2.5-0.5B-Instruct/merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=out-Qwen2.5-0.5B-Instruct/merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[W1010 10:57:41.929828980 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:41 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m WARNING 10-10 10:57:41 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:41 [gpu_model_runner.py:2338] Starting to load model out-Qwen2.5-0.5B-Instruct/merged...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:42 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:43 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.47it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:43 [default_loader.py:268] Loading weights took 0.31 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:43 [gpu_model_runner.py:2392] Model loading took 0.9277 GiB and 1.322359 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:56 [backends.py:539] Using cache directory: /home/oisuomin/.cache/vllm/torch_compile_cache/25357a56fc/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:57:56 [backends.py:550] Dynamo bytecode transform time: 12.00 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 14.955 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:12 [monitor.py:34] torch.compile takes 12.00 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:14 [gpu_worker.py:298] Available KV cache memory: 68.92 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:14 [kv_cache_utils.py:864] GPU KV cache size: 6,022,416 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:14 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 735.16x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:02<00\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:17 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 1.37 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:17 [gpu_worker.py:391] Free memory on device (78.7/79.18 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 0.93 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.37 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=72378323968` to fit into requested memory, or `--kv-cache-memory=80359134208` to fully utilize gpu memory. Current kv cache memory in use is 74003616768 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3632452)\u001b[0;0m INFO 10-10 10:58:17 [core.py:218] init engine (profile, create kv cache, warmup model) took 33.48 seconds\n",
      "INFO 10-10 10:58:18 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-10 10:58:18 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Adding requests: 100%|███████████████████████| 377/377 [00:01<00:00, 297.18it/s]\n",
      "Processed prompts: 100%|█| 377/377 [00:19<00:00, 19.60it/s, est. speed input: 48\n",
      "Errors: 4 out of 377 records (1.06%)\n",
      "ERROR 10-10 10:58:39 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n",
      "| language   | field     |   mean |   size |\n",
      "|------------|-----------|--------|--------|\n",
      "| en         | alt_title | 0.8593 |    135 |\n",
      "| en         | creator   | 0.8278 |    135 |\n",
      "| en         | doi       | 0.9704 |    135 |\n",
      "| en         | e-isbn    | 0.8617 |    135 |\n",
      "| en         | e-issn    | 0.9556 |    135 |\n",
      "| en         | language  | 0.9778 |    135 |\n",
      "| en         | p-isbn    | 0.8963 |    135 |\n",
      "| en         | p-issn    | 0.9556 |    135 |\n",
      "| en         | publisher | 0.7136 |    135 |\n",
      "| en         | title     | 0.8074 |    135 |\n",
      "| en         | type_coar | 0.7926 |    135 |\n",
      "| en         | year      | 0.9407 |    135 |\n",
      "| fi         | alt_title | 0.8840 |    181 |\n",
      "| fi         | creator   | 0.7135 |    181 |\n",
      "| fi         | doi       | 1.0000 |    181 |\n",
      "| fi         | e-isbn    | 0.9190 |    181 |\n",
      "| fi         | e-issn    | 0.9171 |    181 |\n",
      "| fi         | language  | 0.9834 |    181 |\n",
      "| fi         | p-isbn    | 0.9669 |    181 |\n",
      "| fi         | p-issn    | 0.9669 |    181 |\n",
      "| fi         | publisher | 0.7772 |    181 |\n",
      "| fi         | title     | 0.7293 |    181 |\n",
      "| fi         | type_coar | 0.7182 |    181 |\n",
      "| fi         | year      | 0.8674 |    181 |\n",
      "| se         | alt_title | 1.0000 |      3 |\n",
      "| se         | creator   | 0.6667 |      3 |\n",
      "| se         | doi       | 1.0000 |      3 |\n",
      "| se         | e-isbn    | 1.0000 |      3 |\n",
      "| se         | e-issn    | 1.0000 |      3 |\n",
      "| se         | language  | 1.0000 |      3 |\n",
      "| se         | p-isbn    | 1.0000 |      3 |\n",
      "| se         | p-issn    | 1.0000 |      3 |\n",
      "| se         | publisher | 0.0000 |      3 |\n",
      "| se         | title     | 0.0000 |      3 |\n",
      "| se         | type_coar | 0.6667 |      3 |\n",
      "| se         | year      | 1.0000 |      3 |\n",
      "| sv         | alt_title | 0.7586 |     58 |\n",
      "| sv         | creator   | 0.7522 |     58 |\n",
      "| sv         | doi       | 1.0000 |     58 |\n",
      "| sv         | e-isbn    | 0.8966 |     58 |\n",
      "| sv         | e-issn    | 0.9310 |     58 |\n",
      "| sv         | language  | 1.0000 |     58 |\n",
      "| sv         | p-isbn    | 0.8793 |     58 |\n",
      "| sv         | p-issn    | 0.9655 |     58 |\n",
      "| sv         | publisher | 0.7701 |     58 |\n",
      "| sv         | title     | 0.6897 |     58 |\n",
      "| sv         | type_coar | 0.8103 |     58 |\n",
      "| sv         | year      | 0.8966 |     58 |\n",
      "CPU times: user 932 ms, sys: 185 ms, total: 1.12 s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultfile = f\"../../eval/results-{MODEL_SHORT_NAME.replace('.','_')}.md\"\n",
    "\n",
    "# evaluate using the evaluate-model script, which needs venv with vLLM installed\n",
    "!../dspy/venv/bin/python evaluate-model.py out-{MODEL_SHORT_NAME}/merged axolotl-test.jsonl {resultfile}\n",
    "!cat {resultfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greylitlm-axolotl",
   "language": "python",
   "name": "greylitlm-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
