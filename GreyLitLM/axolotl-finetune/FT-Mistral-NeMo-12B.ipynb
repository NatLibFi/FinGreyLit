{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Mistral-Nemo-Instruct-2407 model using Axolotl framework\n",
    "\n",
    "How to install dependencies (in HPC environment):\n",
    "\n",
    "- load Python and cuDNN modules\n",
    "- create a Python venv and activate it\n",
    "- install dependencies from requirements.txt (e.g. torch)\n",
    "- install Axolotl from git clone (pip won't work, see [this issue](https://github.com/OpenAccess-AI-Collective/axolotl/issues/945)):\n",
    "\n",
    "```\n",
    "git clone git@github.com:OpenAccess-AI-Collective/axolotl.git\n",
    "cd axolotl\n",
    "pip install -e '.[flash-attn,deepspeed]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/appl/easybuild/opt/CUDA/12.6.0\n"
     ]
    }
   ],
   "source": [
    "!printenv CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit\"\n",
    "#SLICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 640 train records\n",
      "Wrote 182 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'assistant', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"axolotl-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"axolotl-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"axolotl-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: axolotl-train.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "test_datasets:\n",
    "  - path: axolotl-eval.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "#chat_template: chatml\n",
    "\n",
    "peft_use_dora: true\n",
    "adapter: lora\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 5\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "  pad_token: \"<pad>\"\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-09-30 11:57:50,142] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:1940554] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-09-30 11:57:50,143] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:1940554] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-09-30 11:57:50,606] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:1940554] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-09-30 11:57:52,373] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:1940554] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-09-30 11:57:52,378] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:1940554] [RANK:0] Unable to find prepared dataset in last_run_prepared/334beafe455cfaac40ffe8726c92f74c\u001b[39m\n",
      "[2025-09-30 11:57:52,378] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:1940554] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-09-30 11:57:52,378] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:1940554] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 640 examples [00:00, 12434.01 examples/s]\n",
      "[2025-09-30 11:57:53,121] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:1940554] [RANK:0] Loading dataset: axolotl-train.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-09-30 11:57:53,158] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:1940554] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if messages[0][\"role\"] == \"system\" %}\n",
      "    {%- set system_message = messages[0][\"content\"] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n",
      "\n",
      "{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n",
      "{%- set ns = namespace() %}\n",
      "{%- set ns.index = 0 %}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n",
      "        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n",
      "            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "        {%- endif %}\n",
      "        {%- set ns.index = ns.index + 1 %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if message[\"role\"] == \"user\" %}\n",
      "        {%- if tools is not none and (message == user_messages[-1]) %}\n",
      "            {{- \"[AVAILABLE_TOOLS][\" }}\n",
      "            {%- for tool in tools %}\n",
      "                {%- set tool = tool.function %}\n",
      "                {{- '{\"type\": \"function\", \"function\": {' }}\n",
      "                {%- for key, val in tool.items() if key != \"return\" %}\n",
      "                    {%- if val is string %}\n",
      "                        {{- '\"' + key + '\": \"' + val + '\"' }}\n",
      "                    {%- else %}\n",
      "                        {{- '\"' + key + '\": ' + val|tojson }}\n",
      "                    {%- endif %}\n",
      "                    {%- if not loop.last %}\n",
      "                        {{- \", \" }}\n",
      "                    {%- endif %}\n",
      "                {%- endfor %}\n",
      "                {{- \"}}\" }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- else %}\n",
      "                    {{- \"]\" }}\n",
      "                {%- endif %}\n",
      "            {%- endfor %}\n",
      "            {{- \"[/AVAILABLE_TOOLS]\" }}\n",
      "            {%- endif %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- else %}\n",
      "            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n",
      "        {{- \"[TOOL_CALLS][\" }}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- set out = tool_call.function|tojson %}\n",
      "            {{- out[:-1] }}\n",
      "            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n",
      "                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "            {%- endif %}\n",
      "            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n",
      "            {%- if not loop.last %}\n",
      "                {{- \", \" }}\n",
      "            {%- else %}\n",
      "                {{- \"]\" + eos_token }}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "    {%- elif message[\"role\"] == \"assistant\" %}\n",
      "        {{- message[\"content\"] + eos_token}}\n",
      "    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n",
      "        {%- if message.content is defined and message.content.content is defined %}\n",
      "            {%- set content = message.content.content %}\n",
      "        {%- else %}\n",
      "            {%- set content = message.content %}\n",
      "        {%- endif %}\n",
      "        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n",
      "        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n",
      "            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "        {%- endif %}\n",
      "        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|█| 640/640 [00:06<00:00, 101.53 examples/\n",
      "[2025-09-30 11:57:59,976] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:1940554] [RANK:0] min_input_len: 344\u001b[39m\n",
      "[2025-09-30 11:57:59,976] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:1940554] [RANK:0] max_input_len: 8890\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 640/640 [00:00<00:00, 2183.30 exa\n",
      "\u001b[33m[2025-09-30 11:58:00,960] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:201] [PID:1940554] [RANK:0] Dropped 2 long samples from dataset\u001b[39m\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 638/638 [00:00<00\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 638/638 [00:00<00\n",
      "Saving the dataset (1/1 shards): 100%|█| 638/638 [00:00<00:00, 15908.96 examples\n",
      "[2025-09-30 11:58:03,201] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:1940554] [RANK:0] Unable to find prepared dataset in last_run_prepared/304a8d67b27d880d365905206b64f97b\u001b[39m\n",
      "[2025-09-30 11:58:03,201] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:1940554] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-09-30 11:58:03,201] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:1940554] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 4976.19 examples/s]\n",
      "[2025-09-30 11:58:03,624] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:1940554] [RANK:0] Loading dataset: axolotl-eval.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-09-30 11:58:03,655] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:1940554] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if messages[0][\"role\"] == \"system\" %}\n",
      "    {%- set system_message = messages[0][\"content\"] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n",
      "\n",
      "{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n",
      "{%- set ns = namespace() %}\n",
      "{%- set ns.index = 0 %}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n",
      "        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n",
      "            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "        {%- endif %}\n",
      "        {%- set ns.index = ns.index + 1 %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if message[\"role\"] == \"user\" %}\n",
      "        {%- if tools is not none and (message == user_messages[-1]) %}\n",
      "            {{- \"[AVAILABLE_TOOLS][\" }}\n",
      "            {%- for tool in tools %}\n",
      "                {%- set tool = tool.function %}\n",
      "                {{- '{\"type\": \"function\", \"function\": {' }}\n",
      "                {%- for key, val in tool.items() if key != \"return\" %}\n",
      "                    {%- if val is string %}\n",
      "                        {{- '\"' + key + '\": \"' + val + '\"' }}\n",
      "                    {%- else %}\n",
      "                        {{- '\"' + key + '\": ' + val|tojson }}\n",
      "                    {%- endif %}\n",
      "                    {%- if not loop.last %}\n",
      "                        {{- \", \" }}\n",
      "                    {%- endif %}\n",
      "                {%- endfor %}\n",
      "                {{- \"}}\" }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- else %}\n",
      "                    {{- \"]\" }}\n",
      "                {%- endif %}\n",
      "            {%- endfor %}\n",
      "            {{- \"[/AVAILABLE_TOOLS]\" }}\n",
      "            {%- endif %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- else %}\n",
      "            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n",
      "        {{- \"[TOOL_CALLS][\" }}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- set out = tool_call.function|tojson %}\n",
      "            {{- out[:-1] }}\n",
      "            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n",
      "                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "            {%- endif %}\n",
      "            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n",
      "            {%- if not loop.last %}\n",
      "                {{- \", \" }}\n",
      "            {%- else %}\n",
      "                {{- \"]\" + eos_token }}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "    {%- elif message[\"role\"] == \"assistant\" %}\n",
      "        {{- message[\"content\"] + eos_token}}\n",
      "    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n",
      "        {%- if message.content is defined and message.content.content is defined %}\n",
      "            {%- set content = message.content.content %}\n",
      "        {%- else %}\n",
      "            {%- set content = message.content %}\n",
      "        {%- endif %}\n",
      "        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n",
      "        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n",
      "            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "        {%- endif %}\n",
      "        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:05<00:00,  6.04 examples/s]\n",
      "[2025-09-30 11:58:09,478] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:1940554] [RANK:0] min_input_len: 473\u001b[39m\n",
      "[2025-09-30 11:58:09,478] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:1940554] [RANK:0] max_input_len: 3092\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:00<00:00, 152.09 exampl\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Saving the dataset (1/1 shards): 100%|█| 32/32 [00:00<00:00, 2623.28 examples/s]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-09-30 11:59:28,992] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:1940554] [RANK:0] gather_len_batches: [192]\u001b[39m\n",
      "[2025-09-30 11:59:28,992] [INFO] [axolotl.utils.data.sft._prepare_standard_dataset:123] [PID:1940554] [RANK:0] Maximum number of steps set at 240\u001b[39m\n",
      "[2025-09-30 11:59:29,689] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:1940554] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:10<00:00,  2.19s/it]\n",
      "[2025-09-30 11:59:46,264] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:1940554] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-09-30 11:59:47,613] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:1940554] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-09-30 11:59:47,613] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:1940554] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 58,818,560 || all params: 12,306,600,960 || trainable%: 0.4779\n",
      "[2025-09-30 12:04:26,650] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:1940554] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n",
      "[2025-09-30 12:04:34,329] [WARNING] [accelerate.utils.other.check_os_kernel:441] [PID:1940554] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[2025-09-30 12:05:23,886] [INFO] [axolotl.train.save_initial_configs:403] [PID:1940554] [RANK:0] Pre-saving adapter config to ./out-Mistral-Nemo-Instruct-2407...\u001b[39m\n",
      "[2025-09-30 12:05:23,888] [INFO] [axolotl.train.save_initial_configs:407] [PID:1940554] [RANK:0] Pre-saving tokenizer to ./out-Mistral-Nemo-Instruct-2407...\u001b[39m\n",
      "[2025-09-30 12:05:24,020] [INFO] [axolotl.train.save_initial_configs:410] [PID:1940554] [RANK:0] Pre-saving model config to ./out-Mistral-Nemo-Instruct-2407...\u001b[39m\n",
      "[2025-09-30 12:05:24,027] [INFO] [axolotl.train.execute_training:225] [PID:1940554] [RANK:0] Starting trainer...\u001b[39m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-09-30 12:06:27,024] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:1940554] [RANK:0] gather_len_batches: [192]\u001b[39m\n",
      "  0%|                                                   | 0/240 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.06it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:17,  1.37s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.60s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.74s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.82s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:16,  1.87s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:13<00:15,  1.91s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:15<00:13,  1.94s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:17<00:11,  1.95s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:19<00:09,  1.96s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:21<00:07,  1.96s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:23<00:05,  1.98s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:25<00:03,  1.98s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:27<00:01,  1.98s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3518261909484863, 'eval_runtime': 32.6834, 'eval_samples_per_second': 0.979, 'eval_steps_per_second': 0.49, 'epoch': 0}\n",
      "  0%|                                                   | 0/240 [00:32<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:29<00:00,  1.97s/it]\u001b[A\n",
      "{'loss': 1.1543, 'grad_norm': 3.138935089111328, 'learning_rate': 0.0, 'epoch': 0.02}0m\u001b[0m\n",
      "  0%|▏                                        | 1/240 [01:21<5:26:05, 81.86s/it][2025-09-30 12:08:23,541] [INFO] [axolotl.utils.callbacks.log_gpu_memory_usage:107] [PID:1940554] [RANK:0] cuda memory usage while training: 23.161GB (+22.050GB cache, +1.326GB misc)\u001b[39m\n",
      "{'loss': 1.0891, 'grad_norm': 2.872884511947632, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.1366, 'grad_norm': 3.009122848510742, 'learning_rate': 4e-05, 'epoch': 0.06}\n",
      "{'loss': 0.9979, 'grad_norm': 2.343946933746338, 'learning_rate': 6e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6878, 'grad_norm': 1.8937991857528687, 'learning_rate': 8e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5701, 'grad_norm': 1.2007083892822266, 'learning_rate': 0.0001, 'epoch': 0.12}\n",
      "{'loss': 0.3987, 'grad_norm': 1.1920487880706787, 'learning_rate': 0.00012, 'epoch': 0.15}\n",
      "{'loss': 0.3525, 'grad_norm': 1.1239598989486694, 'learning_rate': 0.00014, 'epoch': 0.17}\n",
      "{'loss': 0.149, 'grad_norm': 0.7807321548461914, 'learning_rate': 0.00016, 'epoch': 0.19}\n",
      "{'loss': 0.1328, 'grad_norm': 0.9232004880905151, 'learning_rate': 0.00018, 'epoch': 0.21}\n",
      "{'loss': 0.0955, 'grad_norm': 0.8041769862174988, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 0.1143, 'grad_norm': 0.5535531640052795, 'learning_rate': 0.0001999906715964522, 'epoch': 0.25}\n",
      "{'loss': 0.1172, 'grad_norm': 0.7450206279754639, 'learning_rate': 0.00019996268812619107, 'epoch': 0.27}\n",
      "{'loss': 0.1312, 'grad_norm': 0.605296790599823, 'learning_rate': 0.00019991605481003866, 'epoch': 0.29}\n",
      "{'loss': 0.1456, 'grad_norm': 0.6570977568626404, 'learning_rate': 0.0001998507803482828, 'epoch': 0.31}\n",
      "{'loss': 0.0758, 'grad_norm': 0.5268975496292114, 'learning_rate': 0.00019976687691905393, 'epoch': 0.33}\n",
      "{'loss': 0.0906, 'grad_norm': 0.5550841689109802, 'learning_rate': 0.00019966436017605297, 'epoch': 0.35}\n",
      "{'loss': 0.1419, 'grad_norm': 0.9588792324066162, 'learning_rate': 0.00019954324924563089, 'epoch': 0.38}\n",
      "{'loss': 0.1078, 'grad_norm': 0.890236496925354, 'learning_rate': 0.00019940356672322037, 'epoch': 0.4}\n",
      "{'loss': 0.0618, 'grad_norm': 0.8890350461006165, 'learning_rate': 0.00019924533866912017, 'epoch': 0.42}\n",
      "{'loss': 0.0488, 'grad_norm': 0.48778918385505676, 'learning_rate': 0.00019906859460363307, 'epoch': 0.44}\n",
      "{'loss': 0.0797, 'grad_norm': 0.4571397006511688, 'learning_rate': 0.0001988733675015585, 'epoch': 0.46}\n",
      "{'loss': 0.1597, 'grad_norm': 1.571108341217041, 'learning_rate': 0.0001986596937860402, 'epoch': 0.48}\n",
      "{'loss': 0.1013, 'grad_norm': 0.48269546031951904, 'learning_rate': 0.00019842761332177115, 'epoch': 0.5}\n",
      " 10%|████                                    | 24/240 [14:34<2:04:04, 34.46s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.01it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:18,  1.40s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.76s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.83s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:16,  1.89s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:13<00:15,  1.92s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:15<00:13,  1.95s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:17<00:11,  1.96s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:19<00:09,  1.96s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:21<00:07,  1.96s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:23<00:05,  1.98s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:25<00:03,  1.98s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:27<00:01,  1.98s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.09406933188438416, 'eval_runtime': 31.805, 'eval_samples_per_second': 1.006, 'eval_steps_per_second': 0.503, 'epoch': 0.5}\n",
      " 10%|████                                    | 24/240 [15:06<2:04:04, 34.46s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:29<00:00,  1.97s/it]\u001b[A\n",
      "{'loss': 0.0375, 'grad_norm': 0.24387195706367493, 'learning_rate': 0.00019817716940755586, 'epoch': 0.52}\n",
      "{'loss': 0.0853, 'grad_norm': 0.4828245937824249, 'learning_rate': 0.00019790840876823232, 'epoch': 0.54}\n",
      "{'loss': 0.1085, 'grad_norm': 0.4280467629432678, 'learning_rate': 0.00019762138154595446, 'epoch': 0.56}\n",
      "{'loss': 0.0612, 'grad_norm': 0.4588402807712555, 'learning_rate': 0.00019731614129083754, 'epoch': 0.58}\n",
      "{'loss': 0.1129, 'grad_norm': 0.5944822430610657, 'learning_rate': 0.00019699274495096712, 'epoch': 0.6}\n",
      "{'loss': 0.0595, 'grad_norm': 0.5267353057861328, 'learning_rate': 0.00019665125286177449, 'epoch': 0.62}\n",
      "{'loss': 0.0948, 'grad_norm': 0.34320661425590515, 'learning_rate': 0.00019629172873477995, 'epoch': 0.65}\n",
      "{'loss': 0.0937, 'grad_norm': 0.5448082685470581, 'learning_rate': 0.00019591423964570632, 'epoch': 0.67}\n",
      "{'loss': 0.0685, 'grad_norm': 0.3739439845085144, 'learning_rate': 0.0001955188560219648, 'epoch': 0.69}\n",
      "{'loss': 0.1109, 'grad_norm': 0.615368127822876, 'learning_rate': 0.00019510565162951537, 'epoch': 0.71}\n",
      "{'loss': 0.0709, 'grad_norm': 0.3954768478870392, 'learning_rate': 0.00019467470355910438, 'epoch': 0.73}\n",
      "{'loss': 0.0642, 'grad_norm': 0.2477705180644989, 'learning_rate': 0.00019422609221188207, 'epoch': 0.75}\n",
      "{'loss': 0.0772, 'grad_norm': 0.4466894865036011, 'learning_rate': 0.00019375990128440204, 'epoch': 0.77}\n",
      "{'loss': 0.0968, 'grad_norm': 0.4377408027648926, 'learning_rate': 0.00019327621775300637, 'epoch': 0.79}\n",
      "{'loss': 0.0517, 'grad_norm': 0.3177742063999176, 'learning_rate': 0.00019277513185759844, 'epoch': 0.81}\n",
      "{'loss': 0.0925, 'grad_norm': 0.4555376172065735, 'learning_rate': 0.00019225673708480717, 'epoch': 0.83}\n",
      "{'loss': 0.0382, 'grad_norm': 0.2622533440589905, 'learning_rate': 0.00019172113015054532, 'epoch': 0.85}\n",
      "{'loss': 0.0743, 'grad_norm': 0.4143001437187195, 'learning_rate': 0.00019116841098196536, 'epoch': 0.88}\n",
      "{'loss': 0.1158, 'grad_norm': 0.5394862294197083, 'learning_rate': 0.0001905986826988164, 'epoch': 0.9}\n",
      "{'loss': 0.0599, 'grad_norm': 0.5022888779640198, 'learning_rate': 0.00019001205159420513, 'epoch': 0.92}\n",
      "{'loss': 0.0323, 'grad_norm': 0.23656252026557922, 'learning_rate': 0.00018940862711476513, 'epoch': 0.94}\n",
      "{'loss': 0.0785, 'grad_norm': 0.4848686456680298, 'learning_rate': 0.0001887885218402375, 'epoch': 0.96}\n",
      "{'loss': 0.0749, 'grad_norm': 0.31952670216560364, 'learning_rate': 0.00018815185146246716, 'epoch': 0.98}\n",
      "{'loss': 0.0691, 'grad_norm': 0.41744619607925415, 'learning_rate': 0.00018749873476381828, 'epoch': 1.0}\n",
      " 20%|████████                                | 48/240 [28:53<1:49:57, 34.36s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.01it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:18,  1.40s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.77s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.84s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:17,  1.89s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:13<00:15,  1.92s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:15<00:13,  1.95s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:17<00:11,  1.96s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:19<00:09,  1.96s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:21<00:07,  1.97s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:23<00:05,  1.98s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:25<00:03,  1.98s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:27<00:01,  1.98s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.07448146492242813, 'eval_runtime': 31.8801, 'eval_samples_per_second': 1.004, 'eval_steps_per_second': 0.502, 'epoch': 1.0}\n",
      " 20%|████████                                | 48/240 [29:25<1:49:57, 34.36s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:29<00:00,  1.98s/it]\u001b[A\n",
      "{'loss': 0.0268, 'grad_norm': 0.28576645255088806, 'learning_rate': 0.00018682929359501338, 'epoch': 1.02}\n",
      "{'loss': 0.0477, 'grad_norm': 0.3026293218135834, 'learning_rate': 0.0001861436528524, 'epoch': 1.04}\n",
      "{'loss': 0.0387, 'grad_norm': 0.4705827832221985, 'learning_rate': 0.00018544194045464886, 'epoch': 1.06}\n",
      "{'loss': 0.0897, 'grad_norm': 24.789958953857422, 'learning_rate': 0.00018472428731888837, 'epoch': 1.08}\n",
      "{'loss': 0.0683, 'grad_norm': 0.6028742790222168, 'learning_rate': 0.00018399082733627965, 'epoch': 1.1}\n",
      "{'loss': 0.0431, 'grad_norm': 0.26192983984947205, 'learning_rate': 0.00018324169734703683, 'epoch': 1.12}\n",
      "{'loss': 0.0298, 'grad_norm': 0.2104385942220688, 'learning_rate': 0.00018247703711489686, 'epoch': 1.15}\n",
      "{'loss': 0.0428, 'grad_norm': 0.305035263299942, 'learning_rate': 0.0001816969893010442, 'epoch': 1.17}\n",
      "{'loss': 0.0339, 'grad_norm': 0.17350514233112335, 'learning_rate': 0.00018090169943749476, 'epoch': 1.19}\n",
      "{'loss': 0.0251, 'grad_norm': 0.17906785011291504, 'learning_rate': 0.00018009131589994418, 'epoch': 1.21}\n",
      "{'loss': 0.0293, 'grad_norm': 0.2927982211112976, 'learning_rate': 0.00017926598988008582, 'epoch': 1.23}\n",
      "{'loss': 0.0448, 'grad_norm': 0.22305220365524292, 'learning_rate': 0.00017842587535740314, 'epoch': 1.25}\n",
      "{'loss': 0.0402, 'grad_norm': 0.3019835650920868, 'learning_rate': 0.000177571129070442, 'epoch': 1.27}\n",
      "{'loss': 0.0176, 'grad_norm': 0.1720283329486847, 'learning_rate': 0.0001767019104875683, 'epoch': 1.29}\n",
      "{'loss': 0.0506, 'grad_norm': 0.23332637548446655, 'learning_rate': 0.0001758183817772163, 'epoch': 1.31}\n",
      "{'loss': 0.0173, 'grad_norm': 0.1483488827943802, 'learning_rate': 0.0001749207077776331, 'epoch': 1.33}\n",
      "{'loss': 0.0597, 'grad_norm': 0.2971848249435425, 'learning_rate': 0.0001740090559661252, 'epoch': 1.35}\n",
      "{'loss': 0.0557, 'grad_norm': 0.3098328113555908, 'learning_rate': 0.00017308359642781242, 'epoch': 1.38}\n",
      "{'loss': 0.0425, 'grad_norm': 0.2565477788448334, 'learning_rate': 0.00017214450182389559, 'epoch': 1.4}\n",
      "{'loss': 0.0319, 'grad_norm': 0.31532105803489685, 'learning_rate': 0.00017119194735944337, 'epoch': 1.42}\n",
      " 28%|███████████▎                            | 68/240 [41:31<1:38:48, 34.47s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the LoRA/DoRA into the base model (for inference & quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-06 09:49:38,925] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:2898179] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-06 09:49:38,926] [WARNING] [axolotl.utils.schemas.config.check_sample_packing_wo_flash:482] [PID:2898179] [RANK:0] sample_packing without flash, sdp, xformers or flex attention does not handle cross sample decontamination.\u001b[39m\n",
      "\u001b[33m[2025-10-06 09:49:38,926] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:2898179] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-06 09:49:39,220] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:2898179] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-06 09:49:39,232] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:318] [PID:2898179] [RANK:0] loading tokenizer... mistralai/Mistral-Nemo-Instruct-2407\u001b[39m\n",
      "[2025-10-06 09:49:40,199] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:2898179] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-10-06 09:49:40,199] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:321] [PID:2898179] [RANK:0] loading model...\u001b[39m\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:40<00:00,  8.04s/it]\n",
      "[2025-10-06 09:50:21,729] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:2898179] [RANK:0] cuda memory usage after model load: 22.813GB (+0.001GB cache, +1.223GB misc)\u001b[39m\n",
      "[2025-10-06 09:50:21,752] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:2898179] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-06 09:50:21,755] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:2898179] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-10-06 09:50:21,755] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:2898179] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 58,818,560 || all params: 12,306,600,960 || trainable%: 0.4779\n",
      "[2025-10-06 09:50:23,899] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:2898179] [RANK:0] cuda memory usage after adapters: 23.040GB (+5.212GB cache, +1.317GB misc)\u001b[39m\n",
      "[2025-10-06 09:50:24,351] [INFO] [axolotl.cli.merge_lora.do_merge_lora:31] [PID:2898179] [RANK:0] Running merge of LoRA with base model...\u001b[39m\n",
      "Unloading and merging model: 100%|██████████| 807/807 [00:00<00:00, 2558.82it/s]\n",
      "[2025-10-06 09:50:24,718] [INFO] [axolotl.cli.merge_lora.do_merge_lora:44] [PID:2898179] [RANK:0] Saving merged model to: out-Mistral-Nemo-Instruct-2407/merged...\u001b[39m\n",
      "\u001b[0mCPU times: user 624 ms, sys: 97.4 ms, total: 721 ms\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/axolotl merge-lora {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 09:55:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-06 09:55:25 [utils.py:328] non-default args: {'max_model_len': 8192, 'disable_log_stats': True, 'model': 'out-Mistral-Nemo-Instruct-2407/merged'}\n",
      "INFO 10-06 09:55:35 [__init__.py:742] Resolved architecture: MistralForCausalLM\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO 10-06 09:55:35 [__init__.py:1815] Using max model len 8192\n",
      "INFO 10-06 09:55:35 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:36 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:36 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='out-Mistral-Nemo-Instruct-2407/merged', speculative_config=None, tokenizer='out-Mistral-Nemo-Instruct-2407/merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=out-Mistral-Nemo-Instruct-2407/merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[W1006 09:55:38.758110921 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:38 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m WARNING 10-06 09:55:38 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:38 [gpu_model_runner.py:2338] Starting to load model out-Mistral-Nemo-Instruct-2407/merged...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:38 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:38 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.13s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.22s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.24s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:04<00:01,  1.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:06<00:00,  1.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:06<00:00,  1.24s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:45 [default_loader.py:268] Loading weights took 6.33 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:45 [gpu_model_runner.py:2392] Model loading took 22.8447 GiB and 6.545687 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:51 [backends.py:539] Using cache directory: /home/oisuomin/.cache/vllm/torch_compile_cache/1f2abe0fee/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:51 [backends.py:550] Dynamo bytecode transform time: 5.67 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:52 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:55:57 [backends.py:215] Compiling a graph for dynamic shape takes 5.10 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:56:05 [monitor.py:34] torch.compile takes 10.77 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:56:06 [gpu_worker.py:298] Available KV cache memory: 47.12 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:56:06 [kv_cache_utils.py:864] GPU KV cache size: 308,816 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:56:06 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 37.70x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:04<00\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:56:11 [gpu_model_runner.py:3118] Graph capturing finished in 5 secs, took 2.11 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:56:11 [gpu_worker.py:391] Free memory on device (78.7/79.18 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 22.84 GiB for weight, 1.28 GiB for peak activation, 0.02 GiB for non-torch memory, and 2.11 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=48176529920` to fit into requested memory, or `--kv-cache-memory=56157340160` to fully utilize gpu memory. Current kv cache memory in use is 50598740480 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2898854)\u001b[0;0m INFO 10-06 09:56:11 [core.py:218] init engine (profile, create kv cache, warmup model) took 25.75 seconds\n",
      "INFO 10-06 09:56:12 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-06 09:56:12 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Adding requests: 100%|███████████████████████| 182/182 [00:00<00:00, 349.48it/s]\n",
      "Processed prompts: 100%|█| 182/182 [00:58<00:00,  3.11it/s, est. speed input: 72\n",
      "ERROR 10-06 09:57:11 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n",
      "| language   | field     |   mean |   size |\n",
      "|------------|-----------|--------|--------|\n",
      "| en         | alt_title | 0.8033 |     61 |\n",
      "| en         | creator   | 0.8892 |     61 |\n",
      "| en         | doi       | 1.0000 |     61 |\n",
      "| en         | e-isbn    | 0.8743 |     61 |\n",
      "| en         | e-issn    | 0.9508 |     61 |\n",
      "| en         | language  | 1.0000 |     61 |\n",
      "| en         | p-isbn    | 0.8689 |     61 |\n",
      "| en         | p-issn    | 0.9672 |     61 |\n",
      "| en         | publisher | 0.7377 |     61 |\n",
      "| en         | title     | 0.9180 |     61 |\n",
      "| en         | type_coar | 0.8689 |     61 |\n",
      "| en         | year      | 0.9344 |     61 |\n",
      "| fi         | alt_title | 0.7945 |     73 |\n",
      "| fi         | creator   | 0.9152 |     73 |\n",
      "| fi         | doi       | 1.0000 |     73 |\n",
      "| fi         | e-isbn    | 1.0000 |     73 |\n",
      "| fi         | e-issn    | 0.8904 |     73 |\n",
      "| fi         | language  | 1.0000 |     73 |\n",
      "| fi         | p-isbn    | 0.9863 |     73 |\n",
      "| fi         | p-issn    | 0.9315 |     73 |\n",
      "| fi         | publisher | 0.9315 |     73 |\n",
      "| fi         | title     | 0.8904 |     73 |\n",
      "| fi         | type_coar | 0.9315 |     73 |\n",
      "| fi         | year      | 0.9315 |     73 |\n",
      "| se         | alt_title | 1.0000 |      3 |\n",
      "| se         | creator   | 1.0000 |      3 |\n",
      "| se         | doi       | 1.0000 |      3 |\n",
      "| se         | e-isbn    | 1.0000 |      3 |\n",
      "| se         | e-issn    | 1.0000 |      3 |\n",
      "| se         | language  | 1.0000 |      3 |\n",
      "| se         | p-isbn    | 1.0000 |      3 |\n",
      "| se         | p-issn    | 1.0000 |      3 |\n",
      "| se         | publisher | 0.6667 |      3 |\n",
      "| se         | title     | 0.6667 |      3 |\n",
      "| se         | type_coar | 0.3333 |      3 |\n",
      "| se         | year      | 0.6667 |      3 |\n",
      "| sv         | alt_title | 0.9111 |     45 |\n",
      "| sv         | creator   | 0.9778 |     45 |\n",
      "| sv         | doi       | 1.0000 |     45 |\n",
      "| sv         | e-isbn    | 0.9259 |     45 |\n",
      "| sv         | e-issn    | 0.9556 |     45 |\n",
      "| sv         | language  | 1.0000 |     45 |\n",
      "| sv         | p-isbn    | 0.8889 |     45 |\n",
      "| sv         | p-issn    | 0.9556 |     45 |\n",
      "| sv         | publisher | 0.9037 |     45 |\n",
      "| sv         | title     | 0.9778 |     45 |\n",
      "| sv         | type_coar | 0.9556 |     45 |\n",
      "| sv         | year      | 0.9111 |     45 |\n",
      "CPU times: user 646 ms, sys: 90.3 ms, total: 737 ms\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate using the evaluate-model script, which needs venv with vLLM installed\n",
    "!../dspy/venv/bin/python evaluate-model.py out-{MODEL_SHORT_NAME}/merged axolotl-test.jsonl ../../eval/results-{MODEL_SHORT_NAME}.md\n",
    "!cat ../../eval/results-{MODEL_SHORT_NAME}.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greylitlm-axolotl",
   "language": "python",
   "name": "greylitlm-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
