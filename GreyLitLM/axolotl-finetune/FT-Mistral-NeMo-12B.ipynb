{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Mistral-Nemo-Instruct-2407 model using Axolotl framework\n",
    "\n",
    "How to install dependencies (in HPC environment):\n",
    "\n",
    "- load Python and cuDNN modules\n",
    "- create a Python venv and activate it\n",
    "- install dependencies from requirements.txt (e.g. torch)\n",
    "- install Axolotl from git clone (pip won't work, see [this issue](https://github.com/OpenAccess-AI-Collective/axolotl/issues/945)):\n",
    "\n",
    "```\n",
    "git clone git@github.com:OpenAccess-AI-Collective/axolotl.git\n",
    "cd axolotl\n",
    "pip install -e '.[flash-attn,deepspeed]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? True\n",
      "BF16 is supported? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/appl/easybuild/opt/CUDA/12.6.0\n"
     ]
    }
   ],
   "source": [
    "!printenv CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit\"\n",
    "#SLICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1224 train records\n",
      "Wrote 377 test records\n",
      "Wrote 32 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 32  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'assistant', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"axolotl-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"axolotl-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"axolotl-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Axolotl configuration file\n",
    "\n",
    "CONFIG_FILE = f\"config-{MODEL_SHORT_NAME}.yml\"\n",
    "\n",
    "\n",
    "CONFIG = f\"\"\"\n",
    "base_model: {MODEL_NAME}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: axolotl-train.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "test_datasets:\n",
    "  - path: axolotl-eval.jsonl\n",
    "    type: chat_template\n",
    "    ds_type: json\n",
    "    split: train\n",
    "    field_messages: conversations\n",
    "    message_property_mappings:\n",
    "      role: from\n",
    "      content: value\n",
    "\n",
    "output_dir: ./out-{MODEL_SHORT_NAME}\n",
    "\n",
    "#chat_template: chatml\n",
    "\n",
    "peft_use_dora: true\n",
    "adapter: lora\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "eval_batch_size: 2\n",
    "num_epochs: 4\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true  # true: saves VRAM but is slower to train\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "eval_table_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "  pad_token: \"<pad>\"\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as outfile:\n",
    "    print(CONFIG, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-10-10 14:01:01,079] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3650196] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 14:01:01,080] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3650196] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 14:01:01,294] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3650196] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 14:01:02,138] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:3650196] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-10-10 14:01:02,143] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3650196] [RANK:0] Unable to find prepared dataset in last_run_prepared/334beafe455cfaac40ffe8726c92f74c\u001b[39m\n",
      "[2025-10-10 14:01:02,143] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3650196] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 14:01:02,143] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3650196] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 1224 examples [00:00, 26650.55 examples/s]\n",
      "[2025-10-10 14:01:02,721] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3650196] [RANK:0] Loading dataset: axolotl-train.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 14:01:02,752] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3650196] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if messages[0][\"role\"] == \"system\" %}\n",
      "    {%- set system_message = messages[0][\"content\"] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n",
      "\n",
      "{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n",
      "{%- set ns = namespace() %}\n",
      "{%- set ns.index = 0 %}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n",
      "        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n",
      "            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "        {%- endif %}\n",
      "        {%- set ns.index = ns.index + 1 %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if message[\"role\"] == \"user\" %}\n",
      "        {%- if tools is not none and (message == user_messages[-1]) %}\n",
      "            {{- \"[AVAILABLE_TOOLS][\" }}\n",
      "            {%- for tool in tools %}\n",
      "                {%- set tool = tool.function %}\n",
      "                {{- '{\"type\": \"function\", \"function\": {' }}\n",
      "                {%- for key, val in tool.items() if key != \"return\" %}\n",
      "                    {%- if val is string %}\n",
      "                        {{- '\"' + key + '\": \"' + val + '\"' }}\n",
      "                    {%- else %}\n",
      "                        {{- '\"' + key + '\": ' + val|tojson }}\n",
      "                    {%- endif %}\n",
      "                    {%- if not loop.last %}\n",
      "                        {{- \", \" }}\n",
      "                    {%- endif %}\n",
      "                {%- endfor %}\n",
      "                {{- \"}}\" }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- else %}\n",
      "                    {{- \"]\" }}\n",
      "                {%- endif %}\n",
      "            {%- endfor %}\n",
      "            {{- \"[/AVAILABLE_TOOLS]\" }}\n",
      "            {%- endif %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- else %}\n",
      "            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n",
      "        {{- \"[TOOL_CALLS][\" }}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- set out = tool_call.function|tojson %}\n",
      "            {{- out[:-1] }}\n",
      "            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n",
      "                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "            {%- endif %}\n",
      "            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n",
      "            {%- if not loop.last %}\n",
      "                {{- \", \" }}\n",
      "            {%- else %}\n",
      "                {{- \"]\" + eos_token }}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "    {%- elif message[\"role\"] == \"assistant\" %}\n",
      "        {{- message[\"content\"] + eos_token}}\n",
      "    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n",
      "        {%- if message.content is defined and message.content.content is defined %}\n",
      "            {%- set content = message.content.content %}\n",
      "        {%- else %}\n",
      "            {%- set content = message.content %}\n",
      "        {%- endif %}\n",
      "        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n",
      "        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n",
      "            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "        {%- endif %}\n",
      "        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|█| 1224/1224 [00:07<00:00, 172.85 example\n",
      "[2025-10-10 14:01:10,385] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3650196] [RANK:0] min_input_len: 139\u001b[39m\n",
      "[2025-10-10 14:01:10,385] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3650196] [RANK:0] max_input_len: 12030\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 1224/1224 [00:00<00:00, 2813.70 e\n",
      "\u001b[33m[2025-10-10 14:01:11,675] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:201] [PID:3650196] [RANK:0] Dropped 6 long samples from dataset\u001b[39m\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 1218/1218 [00:00<\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 1218/1218 [00:00<\n",
      "Saving the dataset (1/1 shards): 100%|█| 1218/1218 [00:00<00:00, 17580.79 exampl\n",
      "[2025-10-10 14:01:14,445] [INFO] [axolotl.utils.data.shared.load_preprocessed_dataset:467] [PID:3650196] [RANK:0] Unable to find prepared dataset in last_run_prepared/304a8d67b27d880d365905206b64f97b\u001b[39m\n",
      "[2025-10-10 14:01:14,445] [INFO] [axolotl.utils.data.sft._load_raw_datasets:310] [PID:3650196] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-10-10 14:01:14,445] [WARNING] [axolotl.utils.data.sft._load_raw_datasets:312] [PID:3650196] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset using `axolotl preprocess path/to/config.yml`.\u001b[39m\n",
      "Generating train split: 32 examples [00:00, 8701.31 examples/s]\n",
      "[2025-10-10 14:01:15,010] [INFO] [axolotl.utils.data.wrappers.get_dataset_wrapper:88] [PID:3650196] [RANK:0] Loading dataset: axolotl-eval.jsonl with base_type: chat_template and prompt_style: None\u001b[39m\n",
      "[2025-10-10 14:01:15,038] [INFO] [axolotl.prompt_strategies.chat_template.__call__:941] [PID:3650196] [RANK:0] Using chat template:\n",
      "---\n",
      "{%- if messages[0][\"role\"] == \"system\" %}\n",
      "    {%- set system_message = messages[0][\"content\"] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n",
      "\n",
      "{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n",
      "{%- set ns = namespace() %}\n",
      "{%- set ns.index = 0 %}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n",
      "        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n",
      "            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "        {%- endif %}\n",
      "        {%- set ns.index = ns.index + 1 %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if message[\"role\"] == \"user\" %}\n",
      "        {%- if tools is not none and (message == user_messages[-1]) %}\n",
      "            {{- \"[AVAILABLE_TOOLS][\" }}\n",
      "            {%- for tool in tools %}\n",
      "                {%- set tool = tool.function %}\n",
      "                {{- '{\"type\": \"function\", \"function\": {' }}\n",
      "                {%- for key, val in tool.items() if key != \"return\" %}\n",
      "                    {%- if val is string %}\n",
      "                        {{- '\"' + key + '\": \"' + val + '\"' }}\n",
      "                    {%- else %}\n",
      "                        {{- '\"' + key + '\": ' + val|tojson }}\n",
      "                    {%- endif %}\n",
      "                    {%- if not loop.last %}\n",
      "                        {{- \", \" }}\n",
      "                    {%- endif %}\n",
      "                {%- endfor %}\n",
      "                {{- \"}}\" }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- else %}\n",
      "                    {{- \"]\" }}\n",
      "                {%- endif %}\n",
      "            {%- endfor %}\n",
      "            {{- \"[/AVAILABLE_TOOLS]\" }}\n",
      "            {%- endif %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- else %}\n",
      "            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n",
      "        {{- \"[TOOL_CALLS][\" }}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- set out = tool_call.function|tojson %}\n",
      "            {{- out[:-1] }}\n",
      "            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n",
      "                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "            {%- endif %}\n",
      "            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n",
      "            {%- if not loop.last %}\n",
      "                {{- \", \" }}\n",
      "            {%- else %}\n",
      "                {{- \"]\" + eos_token }}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "    {%- elif message[\"role\"] == \"assistant\" %}\n",
      "        {{- message[\"content\"] + eos_token}}\n",
      "    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n",
      "        {%- if message.content is defined and message.content.content is defined %}\n",
      "            {%- set content = message.content.content %}\n",
      "        {%- else %}\n",
      "            {%- set content = message.content %}\n",
      "        {%- endif %}\n",
      "        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n",
      "        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n",
      "            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
      "        {%- endif %}\n",
      "        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "---\u001b[39m\n",
      "Tokenizing Prompts (num_proc=32): 100%|██| 32/32 [00:04<00:00,  6.63 examples/s]\n",
      "[2025-10-10 14:01:20,590] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3650196] [RANK:0] min_input_len: 350\u001b[39m\n",
      "[2025-10-10 14:01:20,590] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3650196] [RANK:0] max_input_len: 3071\u001b[39m\n",
      "Dropping Long Sequences (num_proc=32): 100%|█| 32/32 [00:00<00:00, 144.47 exampl\n",
      "Drop Samples with Zero Trainable Tokens (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Add position_id column (Sample Packing) (num_proc=32): 100%|█| 32/32 [00:00<00:0\n",
      "Saving the dataset (1/1 shards): 100%|█| 32/32 [00:00<00:00, 2644.48 examples/s]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 14:02:33,542] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3650196] [RANK:0] gather_len_batches: [372]\u001b[39m\n",
      "[2025-10-10 14:02:33,543] [INFO] [axolotl.utils.data.sft._prepare_standard_dataset:123] [PID:3650196] [RANK:0] Maximum number of steps set at 372\u001b[39m\n",
      "[2025-10-10 14:02:34,255] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:3650196] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:00<00:00, 16.53it/s]\n",
      "[2025-10-10 14:02:41,807] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3650196] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 14:02:43,368] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:3650196] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-10-10 14:02:43,368] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:3650196] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 58,818,560 || all params: 12,306,600,960 || trainable%: 0.4779\n",
      "[2025-10-10 14:07:04,338] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3650196] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n",
      "[2025-10-10 14:07:08,501] [WARNING] [accelerate.utils.other.check_os_kernel:441] [PID:3650196] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[2025-10-10 14:07:28,221] [INFO] [axolotl.train.save_initial_configs:403] [PID:3650196] [RANK:0] Pre-saving adapter config to ./out-Mistral-Nemo-Instruct-2407...\u001b[39m\n",
      "[2025-10-10 14:07:28,223] [INFO] [axolotl.train.save_initial_configs:407] [PID:3650196] [RANK:0] Pre-saving tokenizer to ./out-Mistral-Nemo-Instruct-2407...\u001b[39m\n",
      "[2025-10-10 14:07:28,372] [INFO] [axolotl.train.save_initial_configs:410] [PID:3650196] [RANK:0] Pre-saving model config to ./out-Mistral-Nemo-Instruct-2407...\u001b[39m\n",
      "[2025-10-10 14:07:28,379] [INFO] [axolotl.train.execute_training:225] [PID:3650196] [RANK:0] Starting trainer...\u001b[39m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m[2025-10-10 14:08:37,688] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:431] [PID:3650196] [RANK:0] gather_len_batches: [372]\u001b[39m\n",
      "  0%|                                                   | 0/372 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.05it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:17,  1.38s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.60s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.75s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.85s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:17,  1.90s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:13<00:15,  1.93s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:15<00:13,  1.96s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:17<00:11,  1.97s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:19<00:09,  1.97s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:21<00:07,  1.97s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:23<00:05,  1.99s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:25<00:03,  1.99s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:27<00:01,  1.99s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2301990985870361, 'eval_runtime': 32.7645, 'eval_samples_per_second': 0.977, 'eval_steps_per_second': 0.488, 'epoch': 0}\n",
      "  0%|                                                   | 0/372 [00:32<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:29<00:00,  1.98s/it]\u001b[A\n",
      "{'loss': 1.5347, 'grad_norm': 3.9496662616729736, 'learning_rate': 0.0, 'epoch': 0.01}m\u001b[0m\n",
      "  0%|                                         | 1/372 [01:23<8:35:09, 83.31s/it][2025-10-10 14:10:36,187] [INFO] [axolotl.utils.callbacks.log_gpu_memory_usage:107] [PID:3650196] [RANK:0] cuda memory usage while training: 23.161GB (+22.050GB cache, +1.326GB misc)\u001b[39m\n",
      "{'loss': 1.2978, 'grad_norm': 3.250135660171509, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.478, 'grad_norm': 2.789288282394409, 'learning_rate': 4e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2783, 'grad_norm': 3.2073311805725098, 'learning_rate': 6e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9943, 'grad_norm': 1.81669020652771, 'learning_rate': 8e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6522, 'grad_norm': 1.5722992420196533, 'learning_rate': 0.0001, 'epoch': 0.06}\n",
      "{'loss': 0.5135, 'grad_norm': 1.3833379745483398, 'learning_rate': 0.00012, 'epoch': 0.08}\n",
      "{'loss': 0.3591, 'grad_norm': 1.3855087757110596, 'learning_rate': 0.00014, 'epoch': 0.09}\n",
      "{'loss': 0.1661, 'grad_norm': 0.8439947962760925, 'learning_rate': 0.00016, 'epoch': 0.1}\n",
      "{'loss': 0.2036, 'grad_norm': 0.8902679085731506, 'learning_rate': 0.00018, 'epoch': 0.11}\n",
      "{'loss': 0.1144, 'grad_norm': 0.7340015769004822, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.1272, 'grad_norm': 0.6513540744781494, 'learning_rate': 0.00019999623426388962, 'epoch': 0.13}\n",
      "{'loss': 0.1619, 'grad_norm': 0.9090708494186401, 'learning_rate': 0.00019998493733917384, 'epoch': 0.14}\n",
      "{'loss': 0.1569, 'grad_norm': 0.7386364936828613, 'learning_rate': 0.00019996611007667742, 'epoch': 0.15}\n",
      "{'loss': 0.1363, 'grad_norm': 0.6758573055267334, 'learning_rate': 0.00019993975389437038, 'epoch': 0.16}\n",
      "{'loss': 0.1099, 'grad_norm': 0.5170311331748962, 'learning_rate': 0.00019990587077726128, 'epoch': 0.17}\n",
      "{'loss': 0.077, 'grad_norm': 0.4210495352745056, 'learning_rate': 0.0001998644632772477, 'epoch': 0.18}\n",
      "{'loss': 0.0781, 'grad_norm': 0.5322313904762268, 'learning_rate': 0.00019981553451292396, 'epoch': 0.19}\n",
      "{'loss': 0.0848, 'grad_norm': 0.5091044306755066, 'learning_rate': 0.0001997590881693464, 'epoch': 0.2}\n",
      "{'loss': 0.173, 'grad_norm': 0.4911971390247345, 'learning_rate': 0.00019969512849775565, 'epoch': 0.22}\n",
      "{'loss': 0.0593, 'grad_norm': 0.31272467970848083, 'learning_rate': 0.00019962366031525664, 'epoch': 0.23}\n",
      "{'loss': 0.1178, 'grad_norm': 0.686004638671875, 'learning_rate': 0.00019954468900445566, 'epoch': 0.24}\n",
      "{'loss': 0.0657, 'grad_norm': 0.4714222848415375, 'learning_rate': 0.00019945822051305507, 'epoch': 0.25}\n",
      "{'loss': 0.0714, 'grad_norm': 0.36985886096954346, 'learning_rate': 0.00019936426135340528, 'epoch': 0.26}\n",
      "{'loss': 0.0991, 'grad_norm': 0.5983330607414246, 'learning_rate': 0.0001992628186020143, 'epoch': 0.27}\n",
      "{'loss': 0.0916, 'grad_norm': 0.5906301140785217, 'learning_rate': 0.00019915389989901474, 'epoch': 0.28}\n",
      "{'loss': 0.0409, 'grad_norm': 0.37990710139274597, 'learning_rate': 0.00019903751344758848, 'epoch': 0.29}\n",
      "{'loss': 0.0831, 'grad_norm': 0.3662678301334381, 'learning_rate': 0.0001989136680133488, 'epoch': 0.3}\n",
      "{'loss': 0.0647, 'grad_norm': 0.49270278215408325, 'learning_rate': 0.00019878237292368013, 'epoch': 0.31}\n",
      "{'loss': 0.1074, 'grad_norm': 0.47748926281929016, 'learning_rate': 0.0001986436380670357, 'epoch': 0.32}\n",
      "{'loss': 0.0439, 'grad_norm': 0.3411274254322052, 'learning_rate': 0.00019849747389219272, 'epoch': 0.33}\n",
      "{'loss': 0.0562, 'grad_norm': 0.4352797865867615, 'learning_rate': 0.0001983438914074654, 'epoch': 0.34}\n",
      "{'loss': 0.067, 'grad_norm': 0.6251165270805359, 'learning_rate': 0.00019818290217987587, 'epoch': 0.35}\n",
      "{'loss': 0.0961, 'grad_norm': 0.5527893900871277, 'learning_rate': 0.00019801451833428312, 'epoch': 0.37}\n",
      "{'loss': 0.0808, 'grad_norm': 0.4559420049190521, 'learning_rate': 0.00019783875255246973, 'epoch': 0.38}\n",
      "{'loss': 0.0862, 'grad_norm': 0.34997451305389404, 'learning_rate': 0.0001976556180721867, 'epoch': 0.39}\n",
      "{'loss': 0.0726, 'grad_norm': 0.4618159234523773, 'learning_rate': 0.00019746512868615656, 'epoch': 0.4}\n",
      "{'loss': 0.1742, 'grad_norm': 0.6897645592689514, 'learning_rate': 0.00019726729874103448, 'epoch': 0.41}\n",
      "{'loss': 0.2257, 'grad_norm': 1.1354221105575562, 'learning_rate': 0.00019706214313632784, 'epoch': 0.42}\n",
      "{'loss': 0.1447, 'grad_norm': 0.6022347211837769, 'learning_rate': 0.00019684967732327396, 'epoch': 0.43}\n",
      "{'loss': 0.0615, 'grad_norm': 0.4615597724914551, 'learning_rate': 0.00019662991730367663, 'epoch': 0.44}\n",
      "{'loss': 0.0983, 'grad_norm': 0.41201990842819214, 'learning_rate': 0.00019640287962870062, 'epoch': 0.45}\n",
      "{'loss': 0.0587, 'grad_norm': 0.30203884840011597, 'learning_rate': 0.00019616858139762534, 'epoch': 0.46}\n",
      "{'loss': 0.122, 'grad_norm': 0.4936884641647339, 'learning_rate': 0.000195927040256557, 'epoch': 0.47}\n",
      "{'loss': 0.0543, 'grad_norm': 0.2502936124801636, 'learning_rate': 0.00019567827439709954, 'epoch': 0.48}\n",
      "{'loss': 0.053, 'grad_norm': 0.23598457872867584, 'learning_rate': 0.00019542230255498454, 'epoch': 0.49}\n",
      "{'loss': 0.1666, 'grad_norm': 0.4910357892513275, 'learning_rate': 0.0001951591440086602, 'epoch': 0.51}\n",
      " 13%|█████                                   | 47/372 [28:29<3:25:57, 38.02s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.00it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:18,  1.41s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.77s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.84s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:17,  1.89s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:13<00:15,  1.93s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:13,  1.96s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:18<00:11,  1.97s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:19<00:09,  1.97s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:21<00:07,  1.98s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:24<00:05,  1.99s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:26<00:03,  1.99s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:28<00:01,  1.99s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.08009321987628937, 'eval_runtime': 32.0589, 'eval_samples_per_second': 0.998, 'eval_steps_per_second': 0.499, 'epoch': 0.51}\n",
      " 13%|█████                                   | 47/372 [29:02<3:25:57, 38.02s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  1.99s/it]\u001b[A\n",
      "{'loss': 0.0957, 'grad_norm': 0.4058610498905182, 'learning_rate': 0.00019488881857783935, 'epoch': 0.52}\n",
      "{'loss': 0.0632, 'grad_norm': 0.38455069065093994, 'learning_rate': 0.00019461134662200668, 'epoch': 0.53}\n",
      "{'loss': 0.0432, 'grad_norm': 0.24464300274848938, 'learning_rate': 0.00019432674903888548, 'epoch': 0.54}\n",
      "{'loss': 0.1174, 'grad_norm': 0.6870338916778564, 'learning_rate': 0.0001940350472628637, 'epoch': 0.55}\n",
      "{'loss': 0.115, 'grad_norm': 0.677678108215332, 'learning_rate': 0.00019373626326337946, 'epoch': 0.56}\n",
      "{'loss': 0.0589, 'grad_norm': 0.3808708190917969, 'learning_rate': 0.0001934304195432668, 'epoch': 0.57}\n",
      "{'loss': 0.0855, 'grad_norm': 0.38976308703422546, 'learning_rate': 0.0001931175391370605, 'epoch': 0.58}\n",
      "{'loss': 0.0416, 'grad_norm': 0.48199477791786194, 'learning_rate': 0.00019279764560926142, 'epoch': 0.59}\n",
      "{'loss': 0.1162, 'grad_norm': 0.49185046553611755, 'learning_rate': 0.00019247076305256176, 'epoch': 0.6}\n",
      "{'loss': 0.0423, 'grad_norm': 0.33511999249458313, 'learning_rate': 0.00019213691608603047, 'epoch': 0.61}\n",
      "{'loss': 0.1511, 'grad_norm': 0.5065772533416748, 'learning_rate': 0.00019179612985325908, 'epoch': 0.62}\n",
      "{'loss': 0.2089, 'grad_norm': 0.7095733880996704, 'learning_rate': 0.00019144843002046806, 'epoch': 0.63}\n",
      "{'loss': 0.052, 'grad_norm': 0.382382869720459, 'learning_rate': 0.0001910938427745737, 'epoch': 0.65}\n",
      "{'loss': 0.0682, 'grad_norm': 0.40568098425865173, 'learning_rate': 0.000190732394821216, 'epoch': 0.66}\n",
      "{'loss': 0.0836, 'grad_norm': 0.4628673493862152, 'learning_rate': 0.00019036411338274703, 'epoch': 0.67}\n",
      "{'loss': 0.0596, 'grad_norm': 0.31166476011276245, 'learning_rate': 0.00018998902619618116, 'epoch': 0.68}\n",
      "{'loss': 0.0871, 'grad_norm': 0.3492445647716522, 'learning_rate': 0.00018960716151110554, 'epoch': 0.69}\n",
      "{'loss': 0.0466, 'grad_norm': 0.31457966566085815, 'learning_rate': 0.00018921854808755294, 'epoch': 0.7}\n",
      "{'loss': 0.0915, 'grad_norm': 0.4415035545825958, 'learning_rate': 0.00018882321519383534, 'epoch': 0.71}\n",
      "{'loss': 0.0756, 'grad_norm': 0.37418559193611145, 'learning_rate': 0.00018842119260433982, 'epoch': 0.72}\n",
      "{'loss': 0.0585, 'grad_norm': 0.36821022629737854, 'learning_rate': 0.00018801251059728604, 'epoch': 0.73}\n",
      "{'loss': 0.0664, 'grad_norm': 0.3771395981311798, 'learning_rate': 0.0001875971999524458, 'epoch': 0.74}\n",
      "{'loss': 0.0295, 'grad_norm': 0.2051183432340622, 'learning_rate': 0.000187175291948825, 'epoch': 0.75}\n",
      "{'loss': 0.1261, 'grad_norm': 0.46669548749923706, 'learning_rate': 0.0001867468183623077, 'epoch': 0.76}\n",
      "{'loss': 0.0536, 'grad_norm': 0.2897455096244812, 'learning_rate': 0.00018631181146326305, 'epoch': 0.77}\n",
      "{'loss': 0.0914, 'grad_norm': 0.5007356405258179, 'learning_rate': 0.0001858703040141148, 'epoch': 0.78}\n",
      "{'loss': 0.0546, 'grad_norm': 0.31025436520576477, 'learning_rate': 0.00018542232926687383, 'epoch': 0.8}\n",
      "{'loss': 0.0592, 'grad_norm': 0.4201521873474121, 'learning_rate': 0.0001849679209606338, 'epoch': 0.81}\n",
      "{'loss': 0.0603, 'grad_norm': 0.3406270444393158, 'learning_rate': 0.00018450711331903006, 'epoch': 0.82}\n",
      "{'loss': 0.1017, 'grad_norm': 0.6977733373641968, 'learning_rate': 0.00018403994104766212, 'epoch': 0.83}\n",
      "{'loss': 0.088, 'grad_norm': 0.49443933367729187, 'learning_rate': 0.00018356643933147986, 'epoch': 0.84}\n",
      "{'loss': 0.0319, 'grad_norm': 0.28025323152542114, 'learning_rate': 0.00018308664383213344, 'epoch': 0.85}\n",
      "{'loss': 0.0846, 'grad_norm': 0.3279196321964264, 'learning_rate': 0.00018260059068528762, 'epoch': 0.86}\n",
      "{'loss': 0.1251, 'grad_norm': 0.7058982849121094, 'learning_rate': 0.00018210831649790018, 'epoch': 0.87}\n",
      "{'loss': 0.06, 'grad_norm': 0.397583931684494, 'learning_rate': 0.00018160985834546475, 'epoch': 0.88}\n",
      "{'loss': 0.0779, 'grad_norm': 0.5781264901161194, 'learning_rate': 0.00018110525376921862, 'epoch': 0.89}\n",
      "{'loss': 0.1188, 'grad_norm': 0.7837209105491638, 'learning_rate': 0.00018059454077331527, 'epoch': 0.9}\n",
      "{'loss': 0.0591, 'grad_norm': 0.43586328625679016, 'learning_rate': 0.00018007775782196214, 'epoch': 0.91}\n",
      "{'loss': 0.0435, 'grad_norm': 0.35524046421051025, 'learning_rate': 0.00017955494383652365, 'epoch': 0.92}\n",
      "{'loss': 0.0431, 'grad_norm': 0.27028360962867737, 'learning_rate': 0.00017902613819258985, 'epoch': 0.94}\n",
      "{'loss': 0.0388, 'grad_norm': 0.22474977374076843, 'learning_rate': 0.00017849138071701092, 'epoch': 0.95}\n",
      "{'loss': 0.2693, 'grad_norm': 1.183829665184021, 'learning_rate': 0.0001779507116848976, 'epoch': 0.96}\n",
      "{'loss': 0.0561, 'grad_norm': 0.3357965052127838, 'learning_rate': 0.00017740417181658788, 'epoch': 0.97}\n",
      "{'loss': 0.0975, 'grad_norm': 0.5092337131500244, 'learning_rate': 0.00017685180227458003, 'epoch': 0.98}\n",
      "{'loss': 0.0638, 'grad_norm': 0.3803369700908661, 'learning_rate': 0.00017629364466043273, 'epoch': 0.99}\n",
      "{'loss': 0.0705, 'grad_norm': 0.3243740499019623, 'learning_rate': 0.00017572974101163165, 'epoch': 1.0}\n",
      "{'loss': 0.2982, 'grad_norm': 0.4702061712741852, 'learning_rate': 0.00017516013379842337, 'epoch': 1.01}\n",
      " 25%|██████████                              | 94/372 [57:46<3:44:45, 48.51s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.00it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:18,  1.41s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.63s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:08<00:19,  1.77s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:10<00:18,  1.85s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:12<00:17,  1.91s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:14<00:15,  1.95s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:13,  1.97s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:18<00:12,  2.00s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:20<00:09,  2.00s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:22<00:07,  2.00s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:24<00:06,  2.04s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:26<00:04,  2.03s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:28<00:02,  2.02s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0766775980591774, 'eval_runtime': 32.4121, 'eval_samples_per_second': 0.987, 'eval_steps_per_second': 0.494, 'epoch': 1.01}\n",
      " 25%|██████████                              | 94/372 [58:18<3:44:45, 48.51s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  2.03s/it]\u001b[A\n",
      "{'loss': 0.3446, 'grad_norm': 0.5444556474685669, 'learning_rate': 0.00017458486592061704, 'epoch': 1.02}\n",
      "{'loss': 0.332, 'grad_norm': 0.4487421214580536, 'learning_rate': 0.00017400398070435293, 'epoch': 1.03}\n",
      "{'loss': 0.1716, 'grad_norm': 0.44078534841537476, 'learning_rate': 0.00017341752189883983, 'epoch': 1.04}\n",
      "{'loss': 0.1237, 'grad_norm': 0.30264952778816223, 'learning_rate': 0.00017282553367305975, 'epoch': 1.05}\n",
      "{'loss': 0.0994, 'grad_norm': 0.3807378113269806, 'learning_rate': 0.0001722280606124415, 'epoch': 1.06}\n",
      "{'loss': 0.0582, 'grad_norm': 0.278300940990448, 'learning_rate': 0.00017162514771550255, 'epoch': 1.08}\n",
      "{'loss': 0.0432, 'grad_norm': 0.28837329149246216, 'learning_rate': 0.00017101684039046036, 'epoch': 1.09}\n",
      "{'loss': 0.0434, 'grad_norm': 0.21788464486598969, 'learning_rate': 0.0001704031844518121, 'epoch': 1.1}\n",
      "{'loss': 0.0375, 'grad_norm': 0.23933640122413635, 'learning_rate': 0.0001697842261168843, 'epoch': 1.11}\n",
      "{'loss': 0.0637, 'grad_norm': 0.38481032848358154, 'learning_rate': 0.0001691600120023521, 'epoch': 1.12}\n",
      "{'loss': 0.046, 'grad_norm': 0.23870278894901276, 'learning_rate': 0.00016853058912072802, 'epoch': 1.13}\n",
      "{'loss': 0.0208, 'grad_norm': 0.1827789694070816, 'learning_rate': 0.00016789600487682156, 'epoch': 1.14}\n",
      "{'loss': 0.0421, 'grad_norm': 0.2154378592967987, 'learning_rate': 0.0001672563070641688, 'epoch': 1.15}\n",
      "{'loss': 0.0536, 'grad_norm': 0.3079039752483368, 'learning_rate': 0.0001666115438614328, 'epoch': 1.16}\n",
      "{'loss': 0.0542, 'grad_norm': 0.30433762073516846, 'learning_rate': 0.00016596176382877506, 'epoch': 1.17}\n",
      "{'loss': 0.0254, 'grad_norm': 0.28176429867744446, 'learning_rate': 0.00016530701590419824, 'epoch': 1.18}\n",
      "{'loss': 0.0654, 'grad_norm': 0.3205340802669525, 'learning_rate': 0.00016464734939986036, 'epoch': 1.19}\n",
      "{'loss': 0.0644, 'grad_norm': 0.23149612545967102, 'learning_rate': 0.00016398281399836097, 'epoch': 1.2}\n",
      "{'loss': 0.0241, 'grad_norm': 0.3060845136642456, 'learning_rate': 0.00016331345974899923, 'epoch': 1.22}\n",
      "{'loss': 0.0343, 'grad_norm': 0.31524473428726196, 'learning_rate': 0.00016263933706400451, 'epoch': 1.23}\n",
      "{'loss': 0.0348, 'grad_norm': 0.2723752558231354, 'learning_rate': 0.00016196049671473954, 'epoch': 1.24}\n",
      "{'loss': 0.0484, 'grad_norm': 0.48924511671066284, 'learning_rate': 0.0001612769898278766, 'epoch': 1.25}\n",
      "{'loss': 0.0344, 'grad_norm': 0.14973942935466766, 'learning_rate': 0.00016058886788154712, 'epoch': 1.26}\n",
      "{'loss': 0.0465, 'grad_norm': 0.38418829441070557, 'learning_rate': 0.00015989618270146423, 'epoch': 1.27}\n",
      "{'loss': 0.086, 'grad_norm': 0.6187708377838135, 'learning_rate': 0.0001591989864570199, 'epoch': 1.28}\n",
      "{'loss': 0.0405, 'grad_norm': 0.3637832701206207, 'learning_rate': 0.00015849733165735556, 'epoch': 1.29}\n",
      "{'loss': 0.0263, 'grad_norm': 0.32966455817222595, 'learning_rate': 0.00015779127114740757, 'epoch': 1.3}\n",
      "{'loss': 0.0439, 'grad_norm': 0.36987006664276123, 'learning_rate': 0.0001570808581039271, 'epoch': 1.31}\n",
      "{'loss': 0.0771, 'grad_norm': 0.6171319484710693, 'learning_rate': 0.00015636614603147512, 'epoch': 1.32}\n",
      "{'loss': 0.0514, 'grad_norm': 0.9434117674827576, 'learning_rate': 0.0001556471887583929, 'epoch': 1.33}\n",
      "{'loss': 0.0717, 'grad_norm': 0.3344567120075226, 'learning_rate': 0.0001549240404327477, 'epoch': 1.34}\n",
      "{'loss': 0.0445, 'grad_norm': 0.2935965359210968, 'learning_rate': 0.00015419675551825475, 'epoch': 1.35}\n",
      "{'loss': 0.0258, 'grad_norm': 0.19835837185382843, 'learning_rate': 0.0001534653887901754, 'epoch': 1.37}\n",
      "{'loss': 0.0381, 'grad_norm': 0.5512697100639343, 'learning_rate': 0.00015272999533119162, 'epoch': 1.38}\n",
      "{'loss': 0.031, 'grad_norm': 0.32832375168800354, 'learning_rate': 0.00015199063052725745, 'epoch': 1.39}\n",
      "{'loss': 0.0428, 'grad_norm': 0.24461981654167175, 'learning_rate': 0.0001512473500634277, 'epoch': 1.4}\n",
      "{'loss': 0.0756, 'grad_norm': 0.4987837076187134, 'learning_rate': 0.00015050020991966406, 'epoch': 1.41}\n",
      "{'loss': 0.0232, 'grad_norm': 0.1802099347114563, 'learning_rate': 0.0001497492663666189, 'epoch': 1.42}\n",
      "{'loss': 0.0423, 'grad_norm': 0.19089791178703308, 'learning_rate': 0.00014899457596139729, 'epoch': 1.43}\n",
      "{'loss': 0.0508, 'grad_norm': 0.25996407866477966, 'learning_rate': 0.00014823619554329745, 'epoch': 1.44}\n",
      "{'loss': 0.0544, 'grad_norm': 0.49484190344810486, 'learning_rate': 0.00014747418222952995, 'epoch': 1.45}\n",
      "{'loss': 0.0291, 'grad_norm': 0.43265044689178467, 'learning_rate': 0.0001467085934109158, 'epoch': 1.46}\n",
      "{'loss': 0.0806, 'grad_norm': 0.30771538615226746, 'learning_rate': 0.00014593948674756417, 'epoch': 1.47}\n",
      "{'loss': 0.0395, 'grad_norm': 0.2776540517807007, 'learning_rate': 0.0001451669201645298, 'epoch': 1.48}\n",
      "{'loss': 0.058, 'grad_norm': 0.2780553102493286, 'learning_rate': 0.00014439095184745024, 'epoch': 1.49}\n",
      "{'loss': 0.0445, 'grad_norm': 0.23851898312568665, 'learning_rate': 0.00014361164023816376, 'epoch': 1.51}\n",
      "{'loss': 0.0415, 'grad_norm': 0.7156733274459839, 'learning_rate': 0.00014282904403030772, 'epoch': 1.52}\n",
      " 38%|██████████████                       | 141/372 [1:25:55<2:15:09, 35.11s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:14,  1.01s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:18,  1.42s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:06<00:19,  1.64s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:08<00:20,  1.83s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:10<00:18,  1.89s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:12<00:17,  1.93s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:14<00:15,  1.96s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:13,  1.99s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:18<00:11,  2.00s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:20<00:09,  2.00s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:22<00:07,  2.00s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:24<00:06,  2.01s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:26<00:04,  2.01s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:28<00:02,  2.01s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06375591456890106, 'eval_runtime': 32.4764, 'eval_samples_per_second': 0.985, 'eval_steps_per_second': 0.493, 'epoch': 1.52}\n",
      " 38%|██████████████                       | 141/372 [1:26:28<2:15:09, 35.11s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  2.01s/it]\u001b[A\n",
      "{'loss': 0.0276, 'grad_norm': 0.27615442872047424, 'learning_rate': 0.00014204322216489814, 'epoch': 1.53}\n",
      "{'loss': 0.0478, 'grad_norm': 0.26091694831848145, 'learning_rate': 0.00014125423382589048, 'epoch': 1.54}\n",
      "{'loss': 0.0734, 'grad_norm': 0.4148167371749878, 'learning_rate': 0.00014046213843572236, 'epoch': 1.55}\n",
      "{'loss': 0.0339, 'grad_norm': 0.23636341094970703, 'learning_rate': 0.00013966699565083802, 'epoch': 1.56}\n",
      "{'loss': 0.0177, 'grad_norm': 0.21329058706760406, 'learning_rate': 0.0001388688653571954, 'epoch': 1.57}\n",
      "{'loss': 0.0336, 'grad_norm': 0.18855297565460205, 'learning_rate': 0.00013806780766575588, 'epoch': 1.58}\n",
      "{'loss': 0.0432, 'grad_norm': 0.3332251012325287, 'learning_rate': 0.00013726388290795697, 'epoch': 1.59}\n",
      "{'loss': 0.041, 'grad_norm': 0.2695523202419281, 'learning_rate': 0.00013645715163116846, 'epoch': 1.6}\n",
      "{'loss': 0.0776, 'grad_norm': 0.46468493342399597, 'learning_rate': 0.00013564767459413237, 'epoch': 1.61}\n",
      "{'loss': 0.0501, 'grad_norm': 0.2735249102115631, 'learning_rate': 0.0001348355127623869, 'epoch': 1.62}\n",
      "{'loss': 0.0541, 'grad_norm': 0.3595227897167206, 'learning_rate': 0.00013402072730367475, 'epoch': 1.63}\n",
      "{'loss': 0.0506, 'grad_norm': 0.30168506503105164, 'learning_rate': 0.0001332033795833364, 'epoch': 1.65}\n",
      "{'loss': 0.018, 'grad_norm': 0.21911664307117462, 'learning_rate': 0.0001323835311596884, 'epoch': 1.66}\n",
      "{'loss': 0.0203, 'grad_norm': 0.2874102294445038, 'learning_rate': 0.00013156124377938699, 'epoch': 1.67}\n",
      "{'loss': 0.0194, 'grad_norm': 0.24809320271015167, 'learning_rate': 0.0001307365793727778, 'epoch': 1.68}\n",
      "{'loss': 0.065, 'grad_norm': 0.46861496567726135, 'learning_rate': 0.00012990960004923154, 'epoch': 1.69}\n",
      "{'loss': 0.0208, 'grad_norm': 0.2662180960178375, 'learning_rate': 0.00012908036809246623, 'epoch': 1.7}\n",
      "{'loss': 0.036, 'grad_norm': 0.34440311789512634, 'learning_rate': 0.00012824894595585637, 'epoch': 1.71}\n",
      "{'loss': 0.0642, 'grad_norm': 0.42907553911209106, 'learning_rate': 0.00012741539625772918, 'epoch': 1.72}\n",
      "{'loss': 0.0353, 'grad_norm': 0.29387640953063965, 'learning_rate': 0.0001265797817766486, 'epoch': 1.73}\n",
      "{'loss': 0.0436, 'grad_norm': 0.20783832669258118, 'learning_rate': 0.0001257421654466872, 'epoch': 1.74}\n",
      "{'loss': 0.0325, 'grad_norm': 0.2649771571159363, 'learning_rate': 0.00012490261035268612, 'epoch': 1.75}\n",
      "{'loss': 0.0778, 'grad_norm': 0.4965193569660187, 'learning_rate': 0.00012406117972550414, 'epoch': 1.76}\n",
      "{'loss': 0.055, 'grad_norm': 0.30810660123825073, 'learning_rate': 0.00012321793693725509, 'epoch': 1.77}\n",
      "{'loss': 0.0462, 'grad_norm': 1.6135423183441162, 'learning_rate': 0.0001223729454965354, 'epoch': 1.78}\n",
      "{'loss': 0.0487, 'grad_norm': 0.8420636653900146, 'learning_rate': 0.00012152626904364067, 'epoch': 1.8}\n",
      "{'loss': 0.0287, 'grad_norm': 0.2794727683067322, 'learning_rate': 0.00012067797134577275, 'epoch': 1.81}\n",
      "{'loss': 0.0335, 'grad_norm': 0.22803349792957306, 'learning_rate': 0.00011982811629223709, 'epoch': 1.82}\n",
      "{'loss': 0.0374, 'grad_norm': 0.2352273166179657, 'learning_rate': 0.00011897676788963101, 'epoch': 1.83}\n",
      "{'loss': 0.0573, 'grad_norm': 0.2671058475971222, 'learning_rate': 0.0001181239902570229, 'epoch': 1.84}\n",
      "{'loss': 0.0298, 'grad_norm': 0.27054303884506226, 'learning_rate': 0.00011726984762112328, 'epoch': 1.85}\n",
      "{'loss': 0.0411, 'grad_norm': 0.31422844529151917, 'learning_rate': 0.0001164144043114475, 'epoch': 1.86}\n",
      "{'loss': 0.0538, 'grad_norm': 0.2522332966327667, 'learning_rate': 0.00011555772475547084, 'epoch': 1.87}\n",
      "{'loss': 0.0203, 'grad_norm': 0.5858333706855774, 'learning_rate': 0.00011469987347377602, 'epoch': 1.88}\n",
      "{'loss': 0.1305, 'grad_norm': 0.676235556602478, 'learning_rate': 0.00011384091507519403, 'epoch': 1.89}\n",
      "{'loss': 0.0874, 'grad_norm': 0.3808560371398926, 'learning_rate': 0.00011298091425193806, 'epoch': 1.9}\n",
      "{'loss': 0.0349, 'grad_norm': 0.23972265422344208, 'learning_rate': 0.00011211993577473121, 'epoch': 1.91}\n",
      "{'loss': 0.0206, 'grad_norm': 0.19843727350234985, 'learning_rate': 0.00011125804448792831, 'epoch': 1.92}\n",
      "{'loss': 0.0272, 'grad_norm': 0.22365407645702362, 'learning_rate': 0.00011039530530463218, 'epoch': 1.94}\n",
      "{'loss': 0.0555, 'grad_norm': 0.6573826670646667, 'learning_rate': 0.00010953178320180475, 'epoch': 1.95}\n",
      "{'loss': 0.0493, 'grad_norm': 0.3356449604034424, 'learning_rate': 0.00010866754321537338, 'epoch': 1.96}\n",
      "{'loss': 0.0591, 'grad_norm': 0.6149529218673706, 'learning_rate': 0.0001078026504353325, 'epoch': 1.97}\n",
      "{'loss': 0.0426, 'grad_norm': 0.24567891657352448, 'learning_rate': 0.0001069371700008416, 'epoch': 1.98}\n",
      "{'loss': 0.0224, 'grad_norm': 0.19882994890213013, 'learning_rate': 0.00010607116709531918, 'epoch': 1.99}\n",
      "{'loss': 0.0437, 'grad_norm': 0.33720386028289795, 'learning_rate': 0.00010520470694153353, 'epoch': 2.0}\n",
      "{'loss': 0.1398, 'grad_norm': 0.32420244812965393, 'learning_rate': 0.00010433785479669038, 'epoch': 2.01}\n",
      "{'loss': 0.1799, 'grad_norm': 0.3827034831047058, 'learning_rate': 0.0001034706759475182, 'epoch': 2.02}\n",
      " 51%|██████████████████▋                  | 188/372 [1:54:44<2:23:02, 46.65s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:02<00:14,  1.00s/it]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:04<00:18,  1.42s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:06<00:19,  1.65s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:08<00:19,  1.79s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:10<00:18,  1.86s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:12<00:17,  1.90s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:14<00:15,  1.94s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:13,  1.97s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:18<00:11,  1.98s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:20<00:09,  1.98s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:22<00:07,  2.00s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:24<00:06,  2.02s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:26<00:04,  2.02s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:28<00:02,  2.02s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.05915478616952896, 'eval_runtime': 32.3148, 'eval_samples_per_second': 0.99, 'eval_steps_per_second': 0.495, 'epoch': 2.02}\n",
      " 51%|██████████████████▋                  | 188/372 [1:55:17<2:23:02, 46.65s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  2.01s/it]\u001b[A\n",
      "{'loss': 0.0631, 'grad_norm': 0.23641328513622284, 'learning_rate': 0.0001026032357053512, 'epoch': 2.03}\n",
      "{'loss': 0.1062, 'grad_norm': 0.48230594396591187, 'learning_rate': 0.0001017355994012102, 'epoch': 2.04}\n",
      "{'loss': 0.0138, 'grad_norm': 0.17603375017642975, 'learning_rate': 0.00010086783238088244, 'epoch': 2.05}\n",
      "{'loss': 0.1201, 'grad_norm': 0.36986687779426575, 'learning_rate': 0.0001, 'epoch': 2.06}\n",
      "{'loss': 0.0288, 'grad_norm': 0.15992037951946259, 'learning_rate': 9.913216761911755e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0749, 'grad_norm': 0.6428144574165344, 'learning_rate': 9.826440059878982e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0235, 'grad_norm': 0.21523334085941315, 'learning_rate': 9.739676429464881e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0479, 'grad_norm': 0.44169092178344727, 'learning_rate': 9.652932405248181e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0265, 'grad_norm': 0.20763520896434784, 'learning_rate': 9.566214520330966e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0322, 'grad_norm': 0.3030739724636078, 'learning_rate': 9.479529305846652e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0336, 'grad_norm': 0.19809068739414215, 'learning_rate': 9.392883290468083e-05, 'epoch': 2.14}\n",
      "{'loss': 0.023, 'grad_norm': 0.2080211043357849, 'learning_rate': 9.306282999915839e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0178, 'grad_norm': 0.3112691640853882, 'learning_rate': 9.219734956466752e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0428, 'grad_norm': 0.29636478424072266, 'learning_rate': 9.133245678462663e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0298, 'grad_norm': 0.3395690619945526, 'learning_rate': 9.046821679819527e-05, 'epoch': 2.18}\n",
      "{'loss': 0.015, 'grad_norm': 0.1444852650165558, 'learning_rate': 8.960469469536786e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0195, 'grad_norm': 0.1851954311132431, 'learning_rate': 8.874195551207174e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0517, 'grad_norm': 0.28238147497177124, 'learning_rate': 8.788006422526881e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0257, 'grad_norm': 0.1974003165960312, 'learning_rate': 8.701908574806197e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0305, 'grad_norm': 0.3390862047672272, 'learning_rate': 8.615908492480598e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0337, 'grad_norm': 0.3224775195121765, 'learning_rate': 8.530012652622397e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0161, 'grad_norm': 0.19191531836986542, 'learning_rate': 8.444227524452918e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0749, 'grad_norm': 0.5565598607063293, 'learning_rate': 8.358559568855249e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0289, 'grad_norm': 0.30707603693008423, 'learning_rate': 8.273015237887673e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0288, 'grad_norm': 0.27113619446754456, 'learning_rate': 8.187600974297714e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0086, 'grad_norm': 0.1298176795244217, 'learning_rate': 8.102323211036904e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0102, 'grad_norm': 0.1603226214647293, 'learning_rate': 8.017188370776292e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0441, 'grad_norm': 0.36990395188331604, 'learning_rate': 7.932202865422726e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0296, 'grad_norm': 0.16266047954559326, 'learning_rate': 7.847373095635937e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0184, 'grad_norm': 0.15536445379257202, 'learning_rate': 7.762705450346462e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0273, 'grad_norm': 0.2655876576900482, 'learning_rate': 7.678206306274495e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0477, 'grad_norm': 0.4485727846622467, 'learning_rate': 7.59388202744959e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0311, 'grad_norm': 0.22186724841594696, 'learning_rate': 7.509738964731389e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0187, 'grad_norm': 0.2564341723918915, 'learning_rate': 7.425783455331281e-05, 'epoch': 2.39}\n",
      "{'loss': 0.015, 'grad_norm': 0.18110540509223938, 'learning_rate': 7.342021822335143e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0206, 'grad_norm': 0.26258888840675354, 'learning_rate': 7.258460374227085e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0097, 'grad_norm': 0.15986043214797974, 'learning_rate': 7.175105404414362e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0172, 'grad_norm': 0.14534473419189453, 'learning_rate': 7.091963190753376e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0292, 'grad_norm': 0.21120905876159668, 'learning_rate': 7.009039995076844e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0243, 'grad_norm': 0.23550769686698914, 'learning_rate': 6.926342062722223e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0334, 'grad_norm': 0.4175477921962738, 'learning_rate': 6.843875622061304e-05, 'epoch': 2.46}\n",
      "{'loss': 0.008, 'grad_norm': 0.13454768061637878, 'learning_rate': 6.761646884031164e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0264, 'grad_norm': 0.2574772834777832, 'learning_rate': 6.679662041666362e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0212, 'grad_norm': 0.21343904733657837, 'learning_rate': 6.597927269632526e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0114, 'grad_norm': 0.17481224238872528, 'learning_rate': 6.516448723761315e-05, 'epoch': 2.51}\n",
      "{'loss': 0.0288, 'grad_norm': 0.22233134508132935, 'learning_rate': 6.435232540586763e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0204, 'grad_norm': 0.1622263342142105, 'learning_rate': 6.354284836883156e-05, 'epoch': 2.53}\n",
      " 63%|███████████████████████▎             | 235/372 [2:22:36<1:19:27, 34.80s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.00it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:18,  1.41s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.76s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.84s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:17,  1.89s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:13<00:15,  1.92s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:13,  1.96s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:17<00:11,  1.97s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:19<00:09,  1.97s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:21<00:07,  1.97s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:23<00:05,  1.99s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:25<00:03,  1.99s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:27<00:01,  1.99s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.054256949573755264, 'eval_runtime': 32.0036, 'eval_samples_per_second': 1.0, 'eval_steps_per_second': 0.5, 'epoch': 2.53}\n",
      " 63%|███████████████████████▎             | 235/372 [2:23:08<1:19:27, 34.80s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  1.99s/it]\u001b[A\n",
      "{'loss': 0.0197, 'grad_norm': 0.19025446474552155, 'learning_rate': 6.273611709204304e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0076, 'grad_norm': 0.25098738074302673, 'learning_rate': 6.193219233424414e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0852, 'grad_norm': 0.30388766527175903, 'learning_rate': 6.11311346428046e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0672, 'grad_norm': 0.4049187898635864, 'learning_rate': 6.033300434916203e-05, 'epoch': 2.57}\n",
      "{'loss': 0.0446, 'grad_norm': 0.2555975914001465, 'learning_rate': 5.9537861564277654e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0236, 'grad_norm': 0.2672576904296875, 'learning_rate': 5.8745766174109495e-05, 'epoch': 2.59}\n",
      "{'loss': 0.069, 'grad_norm': 0.32279857993125916, 'learning_rate': 5.795677783510187e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0664, 'grad_norm': 0.3855745494365692, 'learning_rate': 5.7170955969692265e-05, 'epoch': 2.61}\n",
      "{'loss': 0.0438, 'grad_norm': 0.38311636447906494, 'learning_rate': 5.638835976183627e-05, 'epoch': 2.62}\n",
      "{'loss': 0.0156, 'grad_norm': 0.28014934062957764, 'learning_rate': 5.5609048152549794e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0216, 'grad_norm': 0.2449733316898346, 'learning_rate': 5.483307983547026e-05, 'epoch': 2.65}\n",
      "{'loss': 0.031, 'grad_norm': 0.2269480675458908, 'learning_rate': 5.406051325243586e-05, 'epoch': 2.66}\n",
      "{'loss': 0.0245, 'grad_norm': 0.286924809217453, 'learning_rate': 5.329140658908423e-05, 'epoch': 2.67}\n",
      "{'loss': 0.0247, 'grad_norm': 0.24573540687561035, 'learning_rate': 5.2525817770470084e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0092, 'grad_norm': 0.1521611362695694, 'learning_rate': 5.1763804456702545e-05, 'epoch': 2.69}\n",
      "{'loss': 0.0205, 'grad_norm': 0.24970051646232605, 'learning_rate': 5.1005424038602724e-05, 'epoch': 2.7}\n",
      "{'loss': 0.0274, 'grad_norm': 0.2576825022697449, 'learning_rate': 5.025073363338111e-05, 'epoch': 2.71}\n",
      "{'loss': 0.0152, 'grad_norm': 0.1620182991027832, 'learning_rate': 4.949979008033596e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0236, 'grad_norm': 0.27894577383995056, 'learning_rate': 4.8752649936572304e-05, 'epoch': 2.73}\n",
      "{'loss': 0.023, 'grad_norm': 0.2184315323829651, 'learning_rate': 4.800936947274255e-05, 'epoch': 2.74}\n",
      "{'loss': 0.0344, 'grad_norm': 0.32047146558761597, 'learning_rate': 4.7270004668808397e-05, 'epoch': 2.75}\n",
      "{'loss': 0.03, 'grad_norm': 0.3285655677318573, 'learning_rate': 4.65346112098246e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0165, 'grad_norm': 0.19335810840129852, 'learning_rate': 4.5803244481745275e-05, 'epoch': 2.77}\n",
      "{'loss': 0.0132, 'grad_norm': 0.17600330710411072, 'learning_rate': 4.5075959567252335e-05, 'epoch': 2.78}\n",
      "{'loss': 0.0141, 'grad_norm': 0.1790998876094818, 'learning_rate': 4.435281124160715e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0166, 'grad_norm': 0.35996443033218384, 'learning_rate': 4.363385396852491e-05, 'epoch': 2.81}\n",
      "{'loss': 0.0222, 'grad_norm': 0.23878422379493713, 'learning_rate': 4.291914189607297e-05, 'epoch': 2.82}\n",
      "{'loss': 0.0262, 'grad_norm': 0.20065359771251678, 'learning_rate': 4.220872885259247e-05, 'epoch': 2.83}\n",
      "{'loss': 0.0192, 'grad_norm': 0.2147212028503418, 'learning_rate': 4.1502668342644455e-05, 'epoch': 2.84}\n",
      "{'loss': 0.0094, 'grad_norm': 0.11684802919626236, 'learning_rate': 4.080101354298016e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0259, 'grad_norm': 0.3618733882904053, 'learning_rate': 4.0103817298535794e-05, 'epoch': 2.86}\n",
      "{'loss': 0.0074, 'grad_norm': 0.17163199186325073, 'learning_rate': 3.9411132118452896e-05, 'epoch': 2.87}\n",
      "{'loss': 0.0183, 'grad_norm': 0.17235566675662994, 'learning_rate': 3.872301017212337e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0191, 'grad_norm': 0.2432747483253479, 'learning_rate': 3.8039503285260506e-05, 'epoch': 2.89}\n",
      "{'loss': 0.0079, 'grad_norm': 0.14418064057826996, 'learning_rate': 3.73606629359955e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0164, 'grad_norm': 0.29749980568885803, 'learning_rate': 3.6686540251000756e-05, 'epoch': 2.91}\n",
      "{'loss': 0.0243, 'grad_norm': 0.4533531963825226, 'learning_rate': 3.6017186001639036e-05, 'epoch': 2.92}\n",
      "{'loss': 0.0055, 'grad_norm': 0.08939612656831741, 'learning_rate': 3.535265060013965e-05, 'epoch': 2.94}\n",
      "{'loss': 0.0134, 'grad_norm': 0.16140596568584442, 'learning_rate': 3.4692984095801796e-05, 'epoch': 2.95}\n",
      "{'loss': 0.0223, 'grad_norm': 0.1649613380432129, 'learning_rate': 3.4038236171224946e-05, 'epoch': 2.96}\n",
      "{'loss': 0.0071, 'grad_norm': 0.1219814270734787, 'learning_rate': 3.3388456138567225e-05, 'epoch': 2.97}\n",
      "{'loss': 0.0485, 'grad_norm': 0.21307244896888733, 'learning_rate': 3.274369293583121e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0021, 'grad_norm': 0.03772525116801262, 'learning_rate': 3.210399512317849e-05, 'epoch': 2.99}\n",
      "{'loss': 0.019, 'grad_norm': 0.2652471959590912, 'learning_rate': 3.146941087927203e-05, 'epoch': 3.0}\n",
      "{'loss': 0.1419, 'grad_norm': 0.39709773659706116, 'learning_rate': 3.0839987997647935e-05, 'epoch': 3.01}\n",
      "{'loss': 0.0248, 'grad_norm': 0.14551517367362976, 'learning_rate': 3.0215773883115706e-05, 'epoch': 3.02}\n",
      "{'loss': 0.0347, 'grad_norm': 0.1720062643289566, 'learning_rate': 2.9596815548187908e-05, 'epoch': 3.03}\n",
      " 76%|████████████████████████████         | 282/372 [2:50:59<1:00:03, 40.03s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.00it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:18,  1.41s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.76s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.84s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:17,  1.89s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:13<00:15,  1.93s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:13,  1.96s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:18<00:11,  1.97s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:19<00:09,  1.97s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:21<00:07,  1.97s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:23<00:05,  1.99s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:25<00:03,  1.99s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:27<00:01,  1.99s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.04974579066038132, 'eval_runtime': 31.9915, 'eval_samples_per_second': 1.0, 'eval_steps_per_second': 0.5, 'epoch': 3.03}\n",
      " 76%|████████████████████████████         | 282/372 [2:51:31<1:00:03, 40.03s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  1.99s/it]\u001b[A\n",
      "{'loss': 0.0815, 'grad_norm': 0.28883612155914307, 'learning_rate': 2.8983159609539635e-05, 'epoch': 3.04}\n",
      "{'loss': 0.0903, 'grad_norm': 0.3219872713088989, 'learning_rate': 2.8374852284497446e-05, 'epoch': 3.05}\n",
      "{'loss': 0.0179, 'grad_norm': 0.18515221774578094, 'learning_rate': 2.7771939387558554e-05, 'epoch': 3.06}\n",
      "{'loss': 0.0097, 'grad_norm': 0.17617496848106384, 'learning_rate': 2.717446632694025e-05, 'epoch': 3.08}\n",
      "{'loss': 0.0054, 'grad_norm': 0.10384272038936615, 'learning_rate': 2.6582478101160167e-05, 'epoch': 3.09}\n",
      "{'loss': 0.0056, 'grad_norm': 0.09129200875759125, 'learning_rate': 2.599601929564709e-05, 'epoch': 3.1}\n",
      "{'loss': 0.0056, 'grad_norm': 0.11665219068527222, 'learning_rate': 2.5415134079383006e-05, 'epoch': 3.11}\n",
      "{'loss': 0.006, 'grad_norm': 0.08910037577152252, 'learning_rate': 2.4839866201576646e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0069, 'grad_norm': 0.14573372900485992, 'learning_rate': 2.4270258988368376e-05, 'epoch': 3.13}\n",
      "{'loss': 0.0061, 'grad_norm': 0.17267335951328278, 'learning_rate': 2.3706355339567286e-05, 'epoch': 3.14}\n",
      "{'loss': 0.0146, 'grad_norm': 0.33061131834983826, 'learning_rate': 2.3148197725419983e-05, 'epoch': 3.15}\n",
      "{'loss': 0.0033, 'grad_norm': 0.09653349965810776, 'learning_rate': 2.2595828183412172e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0141, 'grad_norm': 0.14918652176856995, 'learning_rate': 2.2049288315102412e-05, 'epoch': 3.17}\n",
      "{'loss': 0.0038, 'grad_norm': 0.11296386271715164, 'learning_rate': 2.1508619282989084e-05, 'epoch': 3.18}\n",
      "{'loss': 0.0066, 'grad_norm': 0.1117623820900917, 'learning_rate': 2.097386180741019e-05, 'epoch': 3.19}\n",
      "{'loss': 0.0612, 'grad_norm': 0.32504287362098694, 'learning_rate': 2.0445056163476374e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0099, 'grad_norm': 0.23831981420516968, 'learning_rate': 1.9922242178037864e-05, 'epoch': 3.22}\n",
      "{'loss': 0.0095, 'grad_norm': 0.11970655620098114, 'learning_rate': 1.940545922668472e-05, 'epoch': 3.23}\n",
      "{'loss': 0.0268, 'grad_norm': 0.39399927854537964, 'learning_rate': 1.88947462307814e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0037, 'grad_norm': 0.09880375862121582, 'learning_rate': 1.8390141654535265e-05, 'epoch': 3.25}\n",
      "{'loss': 0.0032, 'grad_norm': 0.10080272704362869, 'learning_rate': 1.789168350209983e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0048, 'grad_norm': 0.1267927885055542, 'learning_rate': 1.739940931471239e-05, 'epoch': 3.27}\n",
      "{'loss': 0.0243, 'grad_norm': 0.19731935858726501, 'learning_rate': 1.6913356167866578e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0269, 'grad_norm': 0.2298610359430313, 'learning_rate': 1.6433560668520176e-05, 'epoch': 3.29}\n",
      "{'loss': 0.0129, 'grad_norm': 0.26912882924079895, 'learning_rate': 1.5960058952337887e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0109, 'grad_norm': 0.12437641620635986, 'learning_rate': 1.5492886680969963e-05, 'epoch': 3.31}\n",
      "{'loss': 0.0056, 'grad_norm': 0.19315506517887115, 'learning_rate': 1.5032079039366209e-05, 'epoch': 3.32}\n",
      "{'loss': 0.0127, 'grad_norm': 0.4371780753135681, 'learning_rate': 1.4577670733126203e-05, 'epoch': 3.33}\n",
      "{'loss': 0.0274, 'grad_norm': 0.3432261645793915, 'learning_rate': 1.4129695985885228e-05, 'epoch': 3.34}\n",
      "{'loss': 0.011, 'grad_norm': 0.2012912929058075, 'learning_rate': 1.3688188536736968e-05, 'epoch': 3.35}\n",
      "{'loss': 0.015, 'grad_norm': 0.3566889464855194, 'learning_rate': 1.3253181637692324e-05, 'epoch': 3.37}\n",
      "{'loss': 0.0079, 'grad_norm': 0.1733108013868332, 'learning_rate': 1.2824708051175016e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0163, 'grad_norm': 0.43047547340393066, 'learning_rate': 1.2402800047554208e-05, 'epoch': 3.39}\n",
      "{'loss': 0.0231, 'grad_norm': 0.263813853263855, 'learning_rate': 1.1987489402713981e-05, 'epoch': 3.4}\n",
      "{'loss': 0.0068, 'grad_norm': 0.25598037242889404, 'learning_rate': 1.1578807395660207e-05, 'epoch': 3.41}\n",
      "{'loss': 0.0059, 'grad_norm': 0.1474243551492691, 'learning_rate': 1.1176784806164676e-05, 'epoch': 3.42}\n",
      "{'loss': 0.002, 'grad_norm': 0.048815805464982986, 'learning_rate': 1.078145191244706e-05, 'epoch': 3.43}\n",
      "{'loss': 0.012, 'grad_norm': 0.1787852644920349, 'learning_rate': 1.0392838488894463e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0036, 'grad_norm': 0.08219683170318604, 'learning_rate': 1.0010973803818857e-05, 'epoch': 3.45}\n",
      "{'loss': 0.0181, 'grad_norm': 0.5295798778533936, 'learning_rate': 9.635886617252975e-06, 'epoch': 3.46}\n",
      "{'loss': 0.0056, 'grad_norm': 0.13663344085216522, 'learning_rate': 9.267605178784033e-06, 'epoch': 3.47}\n",
      "{'loss': 0.0158, 'grad_norm': 0.16998042166233063, 'learning_rate': 8.906157225426315e-06, 'epoch': 3.48}\n",
      "{'loss': 0.012, 'grad_norm': 0.307504802942276, 'learning_rate': 8.55156997953197e-06, 'epoch': 3.49}\n",
      "{'loss': 0.002, 'grad_norm': 0.05788201093673706, 'learning_rate': 8.203870146740932e-06, 'epoch': 3.51}\n",
      "{'loss': 0.0136, 'grad_norm': 0.20124855637550354, 'learning_rate': 7.86308391396956e-06, 'epoch': 3.52}\n",
      "{'loss': 0.0082, 'grad_norm': 0.0674871951341629, 'learning_rate': 7.529236947438256e-06, 'epoch': 3.53}\n",
      "{'loss': 0.0136, 'grad_norm': 0.2375301569700241, 'learning_rate': 7.202354390738608e-06, 'epoch': 3.54}\n",
      " 88%|██████████████████████████████████▍    | 329/372 [3:18:50<24:55, 34.77s/it]\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 2/16 [00:01<00:13,  1.00it/s]\u001b[A\n",
      " 19%|████████▎                                   | 3/16 [00:03<00:18,  1.41s/it]\u001b[A\n",
      " 25%|███████████                                 | 4/16 [00:05<00:19,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▊                              | 5/16 [00:07<00:19,  1.76s/it]\u001b[A\n",
      " 38%|████████████████▌                           | 6/16 [00:09<00:18,  1.84s/it]\u001b[A\n",
      " 44%|███████████████████▎                        | 7/16 [00:11<00:17,  1.89s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 8/16 [00:14<00:15,  1.94s/it]\u001b[A\n",
      " 56%|████████████████████████▊                   | 9/16 [00:16<00:13,  1.97s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 10/16 [00:18<00:11,  1.98s/it]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 11/16 [00:20<00:09,  1.98s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 12/16 [00:22<00:07,  1.98s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:24<00:05,  1.99s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 14/16 [00:26<00:03,  1.99s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 15/16 [00:28<00:01,  1.99s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0531182661652565, 'eval_runtime': 32.1811, 'eval_samples_per_second': 0.994, 'eval_steps_per_second': 0.497, 'epoch': 3.54}\n",
      " 88%|██████████████████████████████████▍    | 329/372 [3:19:22<24:55, 34.77s/it]\n",
      "100%|███████████████████████████████████████████| 16/16 [00:30<00:00,  1.99s/it]\u001b[A\n",
      "{'loss': 0.0063, 'grad_norm': 0.13881070911884308, 'learning_rate': 6.882460862939522e-06, 'epoch': 3.55}\n",
      "{'loss': 0.0129, 'grad_norm': 0.3089697062969208, 'learning_rate': 6.5695804567332044e-06, 'epoch': 3.56}\n",
      "{'loss': 0.0061, 'grad_norm': 0.16021370887756348, 'learning_rate': 6.263736736620551e-06, 'epoch': 3.57}\n",
      "{'loss': 0.0096, 'grad_norm': 0.28185606002807617, 'learning_rate': 5.964952737136353e-06, 'epoch': 3.58}\n",
      "{'loss': 0.0037, 'grad_norm': 0.08367226272821426, 'learning_rate': 5.673250961114529e-06, 'epoch': 3.59}\n",
      "{'loss': 0.0102, 'grad_norm': 0.20459313690662384, 'learning_rate': 5.388653377993324e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0123, 'grad_norm': 0.12466669827699661, 'learning_rate': 5.111181422160671e-06, 'epoch': 3.61}\n",
      "{'loss': 0.0099, 'grad_norm': 0.19718307256698608, 'learning_rate': 4.840855991339799e-06, 'epoch': 3.62}\n",
      "{'loss': 0.0142, 'grad_norm': 0.16291266679763794, 'learning_rate': 4.577697445015472e-06, 'epoch': 3.63}\n",
      "{'loss': 0.0024, 'grad_norm': 0.08423283696174622, 'learning_rate': 4.321725602900473e-06, 'epoch': 3.65}\n",
      "{'loss': 0.0164, 'grad_norm': 0.5876819491386414, 'learning_rate': 4.072959743443017e-06, 'epoch': 3.66}\n",
      "{'loss': 0.0109, 'grad_norm': 0.18947003781795502, 'learning_rate': 3.83141860237467e-06, 'epoch': 3.67}\n",
      "{'loss': 0.0149, 'grad_norm': 0.17917203903198242, 'learning_rate': 3.5971203712993894e-06, 'epoch': 3.68}\n",
      "{'loss': 0.0021, 'grad_norm': 0.07302084565162659, 'learning_rate': 3.3700826963233735e-06, 'epoch': 3.69}\n",
      "{'loss': 0.0039, 'grad_norm': 0.12555430829524994, 'learning_rate': 3.1503226767260252e-06, 'epoch': 3.7}\n",
      "{'loss': 0.009, 'grad_norm': 0.11674973368644714, 'learning_rate': 2.9378568636721835e-06, 'epoch': 3.71}\n",
      "{'loss': 0.0139, 'grad_norm': 0.22601361572742462, 'learning_rate': 2.732701258965531e-06, 'epoch': 3.72}\n",
      "{'loss': 0.0102, 'grad_norm': 0.20110484957695007, 'learning_rate': 2.5348713138434564e-06, 'epoch': 3.73}\n",
      "{'loss': 0.0163, 'grad_norm': 0.21737264096736908, 'learning_rate': 2.3443819278132996e-06, 'epoch': 3.74}\n",
      "{'loss': 0.0027, 'grad_norm': 0.09407555311918259, 'learning_rate': 2.161247447530268e-06, 'epoch': 3.75}\n",
      "{'loss': 0.012, 'grad_norm': 0.6999492049217224, 'learning_rate': 1.985481665716882e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0187, 'grad_norm': 0.22109228372573853, 'learning_rate': 1.8170978201241474e-06, 'epoch': 3.77}\n",
      "{'loss': 0.0066, 'grad_norm': 0.10907348245382309, 'learning_rate': 1.6561085925346332e-06, 'epoch': 3.78}\n",
      "{'loss': 0.018, 'grad_norm': 0.15519075095653534, 'learning_rate': 1.5025261078073005e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0207, 'grad_norm': 0.25373876094818115, 'learning_rate': 1.3563619329643119e-06, 'epoch': 3.81}\n",
      "{'loss': 0.0282, 'grad_norm': 0.42145836353302, 'learning_rate': 1.2176270763198828e-06, 'epoch': 3.82}\n",
      "{'loss': 0.0097, 'grad_norm': 0.5413452982902527, 'learning_rate': 1.0863319866512346e-06, 'epoch': 3.83}\n",
      "{'loss': 0.0052, 'grad_norm': 0.15821176767349243, 'learning_rate': 9.624865524115346e-07, 'epoch': 3.84}\n",
      "{'loss': 0.0054, 'grad_norm': 0.17486798763275146, 'learning_rate': 8.461001009852809e-07, 'epoch': 3.85}\n",
      "{'loss': 0.0116, 'grad_norm': 0.22846847772598267, 'learning_rate': 7.371813979857312e-07, 'epoch': 3.86}\n",
      "{'loss': 0.0049, 'grad_norm': 0.12151121348142624, 'learning_rate': 6.357386465947301e-07, 'epoch': 3.87}\n",
      "{'loss': 0.0264, 'grad_norm': 0.23715782165527344, 'learning_rate': 5.417794869449377e-07, 'epoch': 3.88}\n",
      "{'loss': 0.0144, 'grad_norm': 0.23402294516563416, 'learning_rate': 4.5531099554435576e-07, 'epoch': 3.89}\n",
      "{'loss': 0.0118, 'grad_norm': 0.17316992580890656, 'learning_rate': 3.763396847433875e-07, 'epoch': 3.9}\n",
      "{'loss': 0.0059, 'grad_norm': 0.24102778732776642, 'learning_rate': 3.048715022443749e-07, 'epoch': 3.91}\n",
      "{'loss': 0.0087, 'grad_norm': 0.10586239397525787, 'learning_rate': 2.409118306536229e-07, 'epoch': 3.92}\n",
      "{'loss': 0.0398, 'grad_norm': 0.37450388073921204, 'learning_rate': 1.8446548707604648e-07, 'epoch': 3.94}\n",
      "{'loss': 0.0032, 'grad_norm': 0.11286906898021698, 'learning_rate': 1.3553672275230523e-07, 'epoch': 3.95}\n",
      "{'loss': 0.0085, 'grad_norm': 0.07843545079231262, 'learning_rate': 9.412922273871471e-08, 'epoch': 3.96}\n",
      "{'loss': 0.0065, 'grad_norm': 0.3185107111930847, 'learning_rate': 6.024610562962441e-08, 'epoch': 3.97}\n",
      "{'loss': 0.0055, 'grad_norm': 0.12142994999885559, 'learning_rate': 3.388992332259422e-08, 'epoch': 3.98}\n",
      "{'loss': 0.0396, 'grad_norm': 0.4840910732746124, 'learning_rate': 1.506266082615948e-08, 'epoch': 3.99}\n",
      "{'loss': 0.0091, 'grad_norm': 0.23619522154331207, 'learning_rate': 3.7657361103837776e-09, 'epoch': 4.0}\n",
      "{'train_runtime': 13461.7136, 'train_samples_per_second': 0.221, 'train_steps_per_second': 0.028, 'train_loss': 0.06876532674982383, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████████| 372/372 [3:44:21<00:00, 36.19s/it]\n",
      "[2025-10-10 17:52:59,539] [INFO] [axolotl.train.save_trained_model:244] [PID:3650196] [RANK:0] Training completed! Saving trained model to ./out-Mistral-Nemo-Instruct-2407.\u001b[39m\n",
      "[2025-10-10 17:53:00,193] [INFO] [axolotl.train.save_trained_model:341] [PID:3650196] [RANK:0] Model successfully saved to ./out-Mistral-Nemo-Instruct-2407\u001b[39m\n",
      "\u001b[0mCPU times: user 1min 17s, sys: 9.64 s, total: 1min 26s\n",
      "Wall time: 3h 52min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the LoRA/DoRA into the base model (for inference & quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-10 17:53:28,093] [INFO] [axolotl.utils.schemas.config.check_eval_packing:756] [PID:3687518] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "\u001b[33m[2025-10-10 17:53:28,093] [WARNING] [axolotl.utils.schemas.config.check_sample_packing_wo_flash:482] [PID:3687518] [RANK:0] sample_packing without flash, sdp, xformers or flex attention does not handle cross sample decontamination.\u001b[39m\n",
      "\u001b[33m[2025-10-10 17:53:28,093] [WARNING] [axolotl.utils.schemas.config.hint_lora_8bit:871] [PID:3687518] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\u001b[39m\n",
      "[2025-10-10 17:53:28,292] [INFO] [axolotl.utils.config.log_gpu_memory_usage:107] [PID:3687518] [RANK:0] cuda memory usage baseline: 0.000GB (+0.818GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-10-10 17:53:28,305] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:318] [PID:3687518] [RANK:0] loading tokenizer... mistralai/Mistral-Nemo-Instruct-2407\u001b[39m\n",
      "[2025-10-10 17:53:29,168] [INFO] [axolotl.loaders.tokenizer.load_tokenizer:294] [PID:3687518] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-10-10 17:53:29,168] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:321] [PID:3687518] [RANK:0] loading model...\u001b[39m\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:25<00:00,  5.18s/it]\n",
      "[2025-10-10 17:53:56,693] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3687518] [RANK:0] cuda memory usage after model load: 22.813GB (+0.001GB cache, +1.223GB misc)\u001b[39m\n",
      "[2025-10-10 17:53:56,729] [INFO] [axolotl.loaders.model._configure_embedding_dtypes:308] [PID:3687518] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-10-10 17:53:56,732] [INFO] [axolotl.loaders.adapter.load_lora:82] [PID:3687518] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-10-10 17:53:56,732] [INFO] [axolotl.loaders.adapter.load_lora:99] [PID:3687518] [RANK:0] Initializing LoRA weights using dora. This might take longer.\u001b[39m\n",
      "trainable params: 58,818,560 || all params: 12,306,600,960 || trainable%: 0.4779\n",
      "[2025-10-10 17:54:01,024] [INFO] [axolotl.loaders.model.log_gpu_memory_usage:107] [PID:3687518] [RANK:0] cuda memory usage after adapters: 23.040GB (+5.212GB cache, +1.317GB misc)\u001b[39m\n",
      "[2025-10-10 17:54:01,476] [INFO] [axolotl.cli.merge_lora.do_merge_lora:31] [PID:3687518] [RANK:0] Running merge of LoRA with base model...\u001b[39m\n",
      "Unloading and merging model: 100%|██████████| 807/807 [00:00<00:00, 2348.35it/s]\n",
      "[2025-10-10 17:54:01,871] [INFO] [axolotl.cli.merge_lora.do_merge_lora:44] [PID:3687518] [RANK:0] Saving merged model to: out-Mistral-Nemo-Instruct-2407/merged...\u001b[39m\n",
      "\u001b[0mCPU times: user 618 ms, sys: 112 ms, total: 730 ms\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!venv/bin/axolotl merge-lora {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 17:55:23 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-10 17:55:30 [utils.py:328] non-default args: {'max_model_len': 8192, 'disable_log_stats': True, 'model': 'out-Mistral-Nemo-Instruct-2407/merged'}\n",
      "INFO 10-10 17:55:46 [__init__.py:742] Resolved architecture: MistralForCausalLM\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO 10-10 17:55:46 [__init__.py:1815] Using max model len 8192\n",
      "INFO 10-10 17:55:47 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:55:48 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:55:48 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='out-Mistral-Nemo-Instruct-2407/merged', speculative_config=None, tokenizer='out-Mistral-Nemo-Instruct-2407/merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=out-Mistral-Nemo-Instruct-2407/merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[W1010 17:55:53.378409403 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:55:53 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m WARNING 10-10 17:55:53 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:55:53 [gpu_model_runner.py:2338] Starting to load model out-Mistral-Nemo-Instruct-2407/merged...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:55:53 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:55:54 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.20s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.22s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.24s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:04<00:01,  1.24s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:06<00:00,  1.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:06<00:00,  1.30s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:01 [default_loader.py:268] Loading weights took 6.54 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:01 [gpu_model_runner.py:2392] Model loading took 22.8447 GiB and 7.384845 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:11 [backends.py:539] Using cache directory: /home/oisuomin/.cache/vllm/torch_compile_cache/1f2abe0fee/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:11 [backends.py:550] Dynamo bytecode transform time: 9.94 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.782 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:31 [monitor.py:34] torch.compile takes 9.94 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:32 [gpu_worker.py:298] Available KV cache memory: 47.12 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:33 [kv_cache_utils.py:864] GPU KV cache size: 308,816 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:33 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 37.70x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:05<00\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:38 [gpu_model_runner.py:3118] Graph capturing finished in 6 secs, took 2.11 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:38 [gpu_worker.py:391] Free memory on device (78.7/79.18 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 22.84 GiB for weight, 1.28 GiB for peak activation, 0.02 GiB for non-torch memory, and 2.11 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=48176529920` to fit into requested memory, or `--kv-cache-memory=56157340160` to fully utilize gpu memory. Current kv cache memory in use is 50598740480 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3687712)\u001b[0;0m INFO 10-10 17:56:38 [core.py:218] init engine (profile, create kv cache, warmup model) took 37.20 seconds\n",
      "INFO 10-10 17:56:39 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-10 17:56:39 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Adding requests: 100%|███████████████████████| 377/377 [00:01<00:00, 207.72it/s]\n",
      "Processed prompts: 100%|█| 377/377 [01:59<00:00,  3.17it/s, est. speed input: 75\n",
      "Errors: 1 out of 377 records (0.27%)\n",
      "ERROR 10-10 17:58:41 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n",
      "| language   | field     |   mean |   size |\n",
      "|------------|-----------|--------|--------|\n",
      "| en         | alt_title | 0.9037 |    135 |\n",
      "| en         | creator   | 0.8772 |    135 |\n",
      "| en         | doi       | 1.0000 |    135 |\n",
      "| en         | e-isbn    | 0.8728 |    135 |\n",
      "| en         | e-issn    | 0.9556 |    135 |\n",
      "| en         | language  | 0.9926 |    135 |\n",
      "| en         | p-isbn    | 0.9037 |    135 |\n",
      "| en         | p-issn    | 0.9630 |    135 |\n",
      "| en         | publisher | 0.8198 |    135 |\n",
      "| en         | title     | 0.9407 |    135 |\n",
      "| en         | type_coar | 0.9185 |    135 |\n",
      "| en         | year      | 0.9481 |    135 |\n",
      "| fi         | alt_title | 0.8840 |    181 |\n",
      "| fi         | creator   | 0.8699 |    181 |\n",
      "| fi         | doi       | 1.0000 |    181 |\n",
      "| fi         | e-isbn    | 0.9180 |    181 |\n",
      "| fi         | e-issn    | 0.9116 |    181 |\n",
      "| fi         | language  | 0.9945 |    181 |\n",
      "| fi         | p-isbn    | 0.9724 |    181 |\n",
      "| fi         | p-issn    | 0.9779 |    181 |\n",
      "| fi         | publisher | 0.8564 |    181 |\n",
      "| fi         | title     | 0.8398 |    181 |\n",
      "| fi         | type_coar | 0.8122 |    181 |\n",
      "| fi         | year      | 0.9006 |    181 |\n",
      "| se         | alt_title | 0.6667 |      3 |\n",
      "| se         | creator   | 1.0000 |      3 |\n",
      "| se         | doi       | 1.0000 |      3 |\n",
      "| se         | e-isbn    | 1.0000 |      3 |\n",
      "| se         | e-issn    | 1.0000 |      3 |\n",
      "| se         | language  | 1.0000 |      3 |\n",
      "| se         | p-isbn    | 1.0000 |      3 |\n",
      "| se         | p-issn    | 1.0000 |      3 |\n",
      "| se         | publisher | 0.6667 |      3 |\n",
      "| se         | title     | 0.6667 |      3 |\n",
      "| se         | type_coar | 1.0000 |      3 |\n",
      "| se         | year      | 0.6667 |      3 |\n",
      "| sv         | alt_title | 0.8103 |     58 |\n",
      "| sv         | creator   | 0.9080 |     58 |\n",
      "| sv         | doi       | 1.0000 |     58 |\n",
      "| sv         | e-isbn    | 0.9080 |     58 |\n",
      "| sv         | e-issn    | 0.9483 |     58 |\n",
      "| sv         | language  | 1.0000 |     58 |\n",
      "| sv         | p-isbn    | 0.8966 |     58 |\n",
      "| sv         | p-issn    | 0.9310 |     58 |\n",
      "| sv         | publisher | 0.9080 |     58 |\n",
      "| sv         | title     | 0.9655 |     58 |\n",
      "| sv         | type_coar | 0.9483 |     58 |\n",
      "| sv         | year      | 0.9310 |     58 |\n",
      "CPU times: user 1.58 s, sys: 197 ms, total: 1.77 s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate using the evaluate-model script, which needs venv with vLLM installed\n",
    "!../dspy/venv/bin/python evaluate-model.py out-{MODEL_SHORT_NAME}/merged axolotl-test.jsonl ../../eval/results-{MODEL_SHORT_NAME}.md\n",
    "!cat ../../eval/results-{MODEL_SHORT_NAME}.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greylitlm-axolotl",
   "language": "python",
   "name": "greylitlm-axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
