{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01bae98",
   "metadata": {},
   "source": [
    "# Convert metadata and PDFs to LLM dataset\n",
    "\n",
    "This notebook will process the already downloaded PDF files and convert them to a data set suitable for fine-tuning and evaluating LLMs.\n",
    "\n",
    "A new field \"content\" will be added to each record. The field contains an object that in turn contains the fields \"pdfinfo\" and \"pages\", that contain the metadata and text extracted from the PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7189eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting ../metadata/article-eng-test.jsonl to ../llm-dataset/article-eng-test.jsonl\n",
      "converting ../metadata/article-eng-train.jsonl to ../llm-dataset/article-eng-train.jsonl\n",
      "converting ../metadata/article-fin-test.jsonl to ../llm-dataset/article-fin-test.jsonl\n",
      "converting ../metadata/article-fin-train.jsonl to ../llm-dataset/article-fin-train.jsonl\n",
      "converting ../metadata/article-swe-test.jsonl to ../llm-dataset/article-swe-test.jsonl\n",
      "converting ../metadata/article-swe-train.jsonl to ../llm-dataset/article-swe-train.jsonl\n",
      "converting ../metadata/book-eng-test.jsonl to ../llm-dataset/book-eng-test.jsonl\n",
      "converting ../metadata/book-eng-train.jsonl to ../llm-dataset/book-eng-train.jsonl\n",
      "converting ../metadata/book-fin-test.jsonl to ../llm-dataset/book-fin-test.jsonl\n",
      "converting ../metadata/book-fin-train.jsonl to ../llm-dataset/book-fin-train.jsonl\n",
      "converting ../metadata/book-swe-test.jsonl to ../llm-dataset/book-swe-test.jsonl\n",
      "converting ../metadata/book-swe-train.jsonl to ../llm-dataset/book-swe-train.jsonl\n",
      "converting ../metadata/docthes-eng-test.jsonl to ../llm-dataset/docthes-eng-test.jsonl\n",
      "converting ../metadata/docthes-eng-train.jsonl to ../llm-dataset/docthes-eng-train.jsonl\n",
      "converting ../metadata/docthes-fin-test.jsonl to ../llm-dataset/docthes-fin-test.jsonl\n",
      "converting ../metadata/docthes-fin-train.jsonl to ../llm-dataset/docthes-fin-train.jsonl\n",
      "converting ../metadata/docthes-swe-test.jsonl to ../llm-dataset/docthes-swe-test.jsonl\n",
      "converting ../metadata/docthes-swe-train.jsonl to ../llm-dataset/docthes-swe-train.jsonl\n",
      "converting ../metadata/report-eng-test.jsonl to ../llm-dataset/report-eng-test.jsonl\n",
      "converting ../metadata/report-eng-train.jsonl to ../llm-dataset/report-eng-train.jsonl\n",
      "converting ../metadata/report-fin-test.jsonl to ../llm-dataset/report-fin-test.jsonl\n",
      "converting ../metadata/report-fin-train.jsonl to ../llm-dataset/report-fin-train.jsonl\n",
      "converting ../metadata/report-swe-test.jsonl to ../llm-dataset/report-swe-test.jsonl\n",
      "converting ../metadata/report-swe-train.jsonl to ../llm-dataset/report-swe-train.jsonl\n",
      "converting ../metadata/thes-eng-test.jsonl to ../llm-dataset/thes-eng-test.jsonl\n",
      "converting ../metadata/thes-eng-train.jsonl to ../llm-dataset/thes-eng-train.jsonl\n",
      "converting ../metadata/thes-fin-test.jsonl to ../llm-dataset/thes-fin-test.jsonl\n",
      "converting ../metadata/thes-fin-train.jsonl to ../llm-dataset/thes-fin-train.jsonl\n",
      "converting ../metadata/thes-swe-test.jsonl to ../llm-dataset/thes-swe-test.jsonl\n",
      "converting ../metadata/thes-swe-train.jsonl to ../llm-dataset/thes-swe-train.jsonl\n",
      "CPU times: user 20.9 s, sys: 460 ms, total: 21.3 s\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os.path\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "import fitz\n",
    "import regex  # has better Unicode support than standard library re module\n",
    "\n",
    "PAGES = [0, 1, 2, 3, 4, 5, 6, 7, -1]  # pages to analyze: first 8 pages + last page\n",
    "THRESHOLD = 100                       # paragraphs shorter than this will always be kept\n",
    "LONG_PARAGRAPH_PAGES = [0, 1]         # on first two pages, some long paragraphs are accepted\n",
    "LONG_PARAGRAPH_MAX = 2                # how many long paragraphs to keep on the first two pages\n",
    "\n",
    "PDF_METADATA_SKIP = {'format', 'creator', 'producer'}  # PDF metadata fields not to include in extracted text\n",
    "\n",
    "metadata_files = glob.glob(\"../metadata/*.jsonl\")\n",
    "\n",
    "def id_to_fn(identifier):\n",
    "    \"\"\"convert a URI identifier to a simpler string we can use as a filename for the PDF\"\"\"\n",
    "    return '../pdfs/' + identifier.replace('https://', '').replace('/','_') + \".pdf\"\n",
    "\n",
    "def extract_content(fn):\n",
    "    \"\"\"extract and return the pdfinfo metadata and the first few pages of text (and last page) from the given PDF file\"\"\"\n",
    "\n",
    "    pdfinfo = {}\n",
    "    pages = []\n",
    "    \n",
    "    with fitz.open(fn) as pdf:\n",
    "\n",
    "        for key in pdf.metadata.keys():\n",
    "            if key not in PDF_METADATA_SKIP and pdf.metadata.get(key):\n",
    "                pdfinfo[key] = pdf.metadata.get(key)\n",
    "\n",
    "        for page in PAGES:\n",
    "            if page > len(pdf) - 2:\n",
    "                continue\n",
    "\n",
    "            texts = []\n",
    "            text = pdf[page].get_text(sort=True)\n",
    "            # Use regular expression to split text into paragraphs\n",
    "            # Delimiter: newline(s) followed by an upper case character\n",
    "            paragraphs = regex.split(r'\\n+(?=\\p{Lu})', text, flags=re.UNICODE)\n",
    "            long_paragraph_count = 0\n",
    "\n",
    "            for paragraph in paragraphs:\n",
    "                paragraph = \" \".join(paragraph.strip().split())\n",
    "\n",
    "                if '.....' in paragraph or '. . . . .' in paragraph: # looks like a ToC entry, skip it\n",
    "                    continue\n",
    "                elif len(paragraph) < THRESHOLD:  # short paragraph, keep it\n",
    "                    texts.append(paragraph)\n",
    "                elif page in LONG_PARAGRAPH_PAGES and long_paragraph_count < LONG_PARAGRAPH_MAX:\n",
    "                    # allow some long paragraphs on the first two pages\n",
    "                    long_paragraph_count += 1\n",
    "                    texts.append(paragraph)\n",
    "                else:  # must be a long paragraph, skip it\n",
    "                    pass\n",
    "            text = '\\n'.join(texts)\n",
    "            if text:\n",
    "                pages.append({\"page\": page + 1, \"text\": text})\n",
    "    return {\"pdfinfo\": pdfinfo, \"pages\": pages}\n",
    "\n",
    "\n",
    "for mdfile in sorted(metadata_files):\n",
    "    out_path = mdfile.replace('metadata', 'llm-dataset')\n",
    "    print(f\"converting {mdfile} to {out_path}\")\n",
    "    with open(mdfile) as infile, open(out_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            rec = json.loads(line)\n",
    "            pdf_path = id_to_fn(rec[\"id\"])\n",
    "            content = extract_content(pdf_path)\n",
    "            outrec = {\"id\": rec[\"id\"], \"url\": rec[\"url\"], \"content\": content, \"ground_truth\": rec[\"ground_truth\"]}\n",
    "            json.dump(outrec, outfile)\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2e393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
