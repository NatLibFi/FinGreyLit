{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035f928d-7aaa-4199-87d1-469462d3b560",
   "metadata": {},
   "source": [
    "# Convert metadata and PDFs to LLM dataset\n",
    "\n",
    "This notebook will process the already downloaded PDF files and convert them to a data set suitable for fine-tuning and evaluating LLMs. Each record will be converted to two fields, \"text\" and \"metadata\", where \"text\" contains the text extracted from the first few pages and last page of the PDF file and \"metadata\" is a string that represents the metadata of the document in a very simple textual key/value format. In addition, the \"id\" and \"url\" fields (containing the document ID/URL and the PDF URL, respectively) will be retained in the new record and the \"ground_truth\" field will contain the original JSON structured metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a1f619-a192-4060-926f-c7b307b18e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting ../metadata/docthes-eng-test.jsonl to ../llm-dataset/docthes-eng-test.jsonl\n",
      "converting ../metadata/docthes-eng-train.jsonl to ../llm-dataset/docthes-eng-train.jsonl\n",
      "converting ../metadata/docthes-fin-test.jsonl to ../llm-dataset/docthes-fin-test.jsonl\n",
      "converting ../metadata/docthes-fin-train.jsonl to ../llm-dataset/docthes-fin-train.jsonl\n",
      "converting ../metadata/docthes-swe-test.jsonl to ../llm-dataset/docthes-swe-test.jsonl\n",
      "converting ../metadata/docthes-swe-train.jsonl to ../llm-dataset/docthes-swe-train.jsonl\n",
      "converting ../metadata/mono-eng-test.jsonl to ../llm-dataset/mono-eng-test.jsonl\n",
      "converting ../metadata/mono-eng-train.jsonl to ../llm-dataset/mono-eng-train.jsonl\n",
      "converting ../metadata/mono-fin-test.jsonl to ../llm-dataset/mono-fin-test.jsonl\n",
      "converting ../metadata/mono-fin-train.jsonl to ../llm-dataset/mono-fin-train.jsonl\n",
      "converting ../metadata/mono-swe-test.jsonl to ../llm-dataset/mono-swe-test.jsonl\n",
      "converting ../metadata/mono-swe-train.jsonl to ../llm-dataset/mono-swe-train.jsonl\n",
      "converting ../metadata/serial-eng-test.jsonl to ../llm-dataset/serial-eng-test.jsonl\n",
      "converting ../metadata/serial-eng-train.jsonl to ../llm-dataset/serial-eng-train.jsonl\n",
      "converting ../metadata/serial-fin-test.jsonl to ../llm-dataset/serial-fin-test.jsonl\n",
      "converting ../metadata/serial-fin-train.jsonl to ../llm-dataset/serial-fin-train.jsonl\n",
      "converting ../metadata/serial-swe-test.jsonl to ../llm-dataset/serial-swe-test.jsonl\n",
      "converting ../metadata/serial-swe-train.jsonl to ../llm-dataset/serial-swe-train.jsonl\n",
      "converting ../metadata/thes-eng-test.jsonl to ../llm-dataset/thes-eng-test.jsonl\n",
      "converting ../metadata/thes-eng-train.jsonl to ../llm-dataset/thes-eng-train.jsonl\n",
      "converting ../metadata/thes-fin-test.jsonl to ../llm-dataset/thes-fin-test.jsonl\n",
      "converting ../metadata/thes-fin-train.jsonl to ../llm-dataset/thes-fin-train.jsonl\n",
      "converting ../metadata/thes-swe-test.jsonl to ../llm-dataset/thes-swe-test.jsonl\n",
      "converting ../metadata/thes-swe-train.jsonl to ../llm-dataset/thes-swe-train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "import fitz\n",
    "import regex  # has better Unicode support than standard library re module\n",
    "\n",
    "PAGES = [0, 1, 2, 3, 4, 5, 6, 7, -1]  # pages to analyze: first 8 pages + last page\n",
    "THRESHOLD = 100                       # paragraphs shorter than this will always be kept\n",
    "LONG_PARAGRAPH_PAGES = [0, 1]         # on first two pages, some long paragraphs are accepted\n",
    "LONG_PARAGRAPH_MAX = 2                # how many long paragraphs to keep on the first two pages\n",
    "\n",
    "KV_SKIP_FIELDS = {'id', 'url', 'rowid', 'repository', 'doctype', 'subset'}  # fields to not include in key-value metadata\n",
    "\n",
    "PDF_METADATA_SKIP = {'format', 'creator', 'producer'}  # PDF metadata fields not to include in extracted text\n",
    "\n",
    "METADATA_FIELD_NAMES = {\n",
    "    \"dc.contributor\": \"Contributor\",\n",
    "    \"dc.contributor.author\": \"Author\",\n",
    "    \"dc.contributor.degreeSupervisor\": \"Supervisor\",\n",
    "    \"dc.contributor.department\": \"Department\",\n",
    "    \"dc.contributor.editor\": \"Editor\",\n",
    "    \"dc.contributor.faculty\": \"Faculty\",\n",
    "    \"dc.contributor.opponent\": \"Opponent\",\n",
    "    \"dc.contributor.organization\": \"Organization\",\n",
    "    \"dc.contributor.orgunit\": \"Org. unit\",\n",
    "    \"dc.contributor.reviewer\": \"Reviewer\",\n",
    "    \"dc.contributor.studysubject\": \"Study subject\",\n",
    "    \"dc.contributor.supervisor\": \"Supervisor\",\n",
    "    \"dc.date.issued\": \"Issued\",\n",
    "    \"dc.format.extent\": None,  # number of pages - hard to extract\n",
    "    \"dc.format.pagerange\": \"Page range\",\n",
    "    \"dc.identifier.isbn\": \"ISBN (online)\",\n",
    "    \"dc.identifier.urn\": \"URN\",\n",
    "    \"dc.language.iso\": \"Language\",\n",
    "    \"dc.publisher\": \"Publisher\",\n",
    "    \"dc.relation.contractor\": \"Contractor\",\n",
    "    \"dc.relation.doi\": \"DOI\",\n",
    "    \"dc.relation.eissn\": \"ISSN (online)\",\n",
    "    \"dc.relation.isbn\": \"ISBN (printed)\",\n",
    "    \"dc.relation.ispartofjournal\": \"Journal name\",\n",
    "    \"dc.relation.ispartofseries\": \"Series name\",\n",
    "    \"dc.relation.issue\": \"Issue\",\n",
    "    \"dc.relation.numberinseries\": \"Number in series\",\n",
    "    \"dc.relation.pissn\": \"ISSN (printed)\",\n",
    "    \"dc.relation.volume\": \"Volume\",\n",
    "    \"dc.series.year\": \"Series year\",\n",
    "    \"dc.source.identifier\": None,  # rare + hard to extract\n",
    "    \"dc.subject.degreeprogram\": \"Degree program\",\n",
    "    \"dc.subject.discipline\": \"Discipline\",\n",
    "    \"dc.title\": \"Title\",\n",
    "    \"dc.title.alternative\": \"Alternative title\",\n",
    "    \"dc.type.coar\": \"COAR type\",\n",
    "    \"dc.type.okm\": \"OKM type\",\n",
    "    \"dc.type.ontasot\": \"Thesis level\",\n",
    "}\n",
    "\n",
    "LANG_MAP = {\n",
    "    'fin': 'fi',\n",
    "    'swe': 'sv',\n",
    "    'eng': 'en'\n",
    "}\n",
    "\n",
    "LANG_AWARE_FIELDS = (\n",
    "    'dc.contributor.department',\n",
    "    'dc.contributor.faculty',\n",
    "    'dc.contributor.organization',\n",
    "    'dc.subject.degreeprogram',\n",
    "    'dc.subject.discipline',\n",
    "    'dc.title.alternative',\n",
    ")\n",
    "\n",
    "metadata_files = glob.glob(\"../metadata/*.jsonl\")\n",
    "\n",
    "def id_to_fn(identifier):\n",
    "    \"\"\"convert a URI identifier to a simpler string we can use as a filename for the PDF\"\"\"\n",
    "    return '../pdfs/' + identifier.replace('https://', '').replace('/','_') + \".pdf\"\n",
    "\n",
    "def extract_text(fn):\n",
    "    \"\"\"extract and return the first few pages of text from the given PDF file\"\"\"\n",
    "\n",
    "    with fitz.open(fn) as pdf:\n",
    "        texts = []\n",
    "\n",
    "        for key in pdf.metadata.keys():\n",
    "            if key not in PDF_METADATA_SKIP and pdf.metadata.get(key):\n",
    "                texts.append(f\"{key}: {pdf.metadata.get(key)}\")\n",
    "\n",
    "        for page in PAGES:\n",
    "            if page > len(pdf) - 2:\n",
    "                continue\n",
    "\n",
    "            text = pdf[page].get_text(sort=True)\n",
    "            # Use regular expression to split text into paragraphs\n",
    "            # Delimiter: newline(s) followed by an upper case character\n",
    "            paragraphs = regex.split(r'\\n+(?=\\p{Lu})', text, flags=re.UNICODE)\n",
    "            long_paragraph_count = 0\n",
    "\n",
    "            for paragraph in paragraphs:\n",
    "                paragraph = \" \".join(paragraph.strip().split())\n",
    "\n",
    "                if '.....' in paragraph or '. . . . .' in paragraph: # looks like a ToC entry, skip it\n",
    "                    continue\n",
    "                elif len(paragraph) < THRESHOLD:  # short paragraph, keep it\n",
    "                    texts.append(paragraph)\n",
    "                elif page in LONG_PARAGRAPH_PAGES and long_paragraph_count < LONG_PARAGRAPH_MAX:\n",
    "                    # allow some long paragraphs on the first two pages\n",
    "                    long_paragraph_count += 1\n",
    "                    texts.append(paragraph)\n",
    "                else:  # must be a long paragraph, skip it\n",
    "                    pass\n",
    "\n",
    "    return '\\n'.join(texts)\n",
    "\n",
    "def choose_value_by_lang(lang, vals):\n",
    "    lang_vals = {}\n",
    "    fallback_val = vals[0].split(' {')[0]\n",
    "    for val in vals:\n",
    "        m = re.match(r\"(.*) {(\\w\\w)}\", val)\n",
    "        if m:\n",
    "            lang_vals[m.group(2)] = m.group(1)\n",
    "        else:\n",
    "            fallback_val = val\n",
    "    if lang in lang_vals:\n",
    "        return lang_vals[lang]\n",
    "    return fallback_val\n",
    "\n",
    "def metadata_to_kvtext(rec):\n",
    "    langcode = LANG_MAP[rec[\"dc.language.iso\"]]\n",
    "    lines = []\n",
    "    for fld in sorted(rec.keys()):\n",
    "        if fld in KV_SKIP_FIELDS:\n",
    "            continue\n",
    "        fldname = METADATA_FIELD_NAMES[fld]\n",
    "        if not fldname:\n",
    "            continue\n",
    "        vals = rec[fld]\n",
    "        if not isinstance(vals, list):\n",
    "            vals = [vals]\n",
    "        if fld in LANG_AWARE_FIELDS:\n",
    "            vals = [choose_value_by_lang(langcode, vals)]\n",
    "        for val in sorted(vals):\n",
    "            lines.append(f\"{fldname}: {val}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "for mdfile in sorted(metadata_files):\n",
    "    out_path = mdfile.replace('metadata', 'llm-dataset')\n",
    "    print(f\"converting {mdfile} to {out_path}\")\n",
    "    with open(mdfile) as infile, open(out_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            rec = json.loads(line)\n",
    "            pdf_path = id_to_fn(rec[\"id\"])\n",
    "            pdf_text = extract_text(pdf_path)\n",
    "            metadata = metadata_to_kvtext(rec)\n",
    "            ground_truth = {fld: val for fld, val in rec.items() if fld.startswith('dc.')}\n",
    "            outrec = {\"id\": rec[\"id\"], \"url\": rec[\"url\"], \"text\": pdf_text, \"metadata\": metadata, \"ground_truth\": ground_truth}\n",
    "            json.dump(outrec, outfile)\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55010719-fbe5-46b1-ba5d-5b1348cae122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
