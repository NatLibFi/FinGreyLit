{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b2d84b",
   "metadata": {},
   "source": [
    "# Convert metadata and PDFs to LLM dataset\n",
    "\n",
    "This notebook will process the already downloaded PDF files and convert them to a data set suitable for fine-tuning and evaluating LLMs.\n",
    "\n",
    "A new field \"content\" will be added to each record. The field contains an object that in turn contains the fields \"pdfinfo\" and \"pages\", that contain the metadata and text extracted from the PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff7144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import collections\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import regex  # has better Unicode support than standard library re module\n",
    "\n",
    "PAGES = [0, 1, 2, 3, 4, 5, -2, -1]  # pages to analyze: first six pages + last two pages\n",
    "CHARACTER_BUDGET = 5000  # Limit on how many characters per document to include\n",
    "\n",
    "\n",
    "PDF_METADATA_SKIP = {'format', 'creator', 'producer'}  # PDF metadata fields not to include in extracted text\n",
    "\n",
    "metadata_files = glob.glob(\"../metadata/*.jsonl\")\n",
    "\n",
    "def id_to_fn(identifier):\n",
    "    \"\"\"convert a URI identifier to a simpler string we can use as a filename for the PDF\"\"\"\n",
    "    return '../pdfs/' + identifier.replace('https://', '').replace('/','_') + \".pdf\"\n",
    "\n",
    "def comma_proportion(chunk):\n",
    "    if not chunk:\n",
    "        return 0\n",
    "    return (chunk.count(',') + chunk.count(';')) / len(chunk)\n",
    "\n",
    "def emph_proportion(chunk):\n",
    "    if not chunk:\n",
    "        return 0\n",
    "    return (chunk.count('_') + chunk.count('*')) / len(chunk)\n",
    "\n",
    "def chunk_score(chunk, page_num):\n",
    "    if not chunk.strip() or chunk == '-----':\n",
    "        return None, None\n",
    "    if '.....' in chunk or '. . . . .' in chunk or '_ _ _ _ _' in chunk:\n",
    "        return None, None\n",
    "    if re.match(r'^\\W+$', chunk):\n",
    "        return None, None\n",
    "\n",
    "    score = -len(chunk) - 1000 * int(page_num/2)\n",
    "    feats = set()\n",
    "    if re.search(r'(?<!\\d)20\\d\\d(?!\\d)', chunk):\n",
    "        score += 500\n",
    "        feats.add(\"year\")\n",
    "    if re.search(r'\\bdoi\\b', chunk, re.IGNORECASE):\n",
    "        score += 1000\n",
    "        feats.add(\"doi\")\n",
    "    if re.search(r'\\bisbn\\b', chunk, re.IGNORECASE):\n",
    "        score += 1000\n",
    "        feats.add(\"isbn\")\n",
    "    if re.search(r'\\bissn\\b', chunk, re.IGNORECASE):\n",
    "        score += 1000\n",
    "        feats.add(\"issn\")\n",
    "    if re.search(r'\\bhttps?\\b', chunk, re.IGNORECASE):\n",
    "        score += 1000\n",
    "        feats.add(\"http\")\n",
    "    if chunk.startswith('#'):\n",
    "        score += 1000\n",
    "        feats.add(\"headline\")\n",
    "    if comma_proportion(chunk) > 0.01:\n",
    "        score += 10000 * comma_proportion(chunk)\n",
    "        feats.add(\"commas\")\n",
    "    if emph_proportion(chunk) > 0.01:\n",
    "        score += 10000 * emph_proportion(chunk)\n",
    "        feats.add(\"emph\")\n",
    "    return score, feats\n",
    "\n",
    "def split_text(text):\n",
    "    # Use regular expression to split text into paragraphs\n",
    "    # Delimiter: newline(s) followed by an upper case character (possibly with preceding Markdown markup)\n",
    "    return regex.split(r'\\n+(?=[#_*]*\\p{Lu})', text, flags=re.UNICODE)\n",
    "\n",
    "def extract_content(fn):\n",
    "    \"\"\"extract and return the pdfinfo metadata and selected chunks of text from the given PDF file\"\"\"\n",
    "\n",
    "    pdfinfo = {}\n",
    "    page_content = collections.defaultdict(list)\n",
    "\n",
    "    with pymupdf.open(fn) as doc:\n",
    "        for key in doc.metadata.keys():\n",
    "            if key not in PDF_METADATA_SKIP and doc.metadata.get(key):\n",
    "                pdfinfo[key] = doc.metadata.get(key)\n",
    "        \n",
    "        # Extract valid pages, remove duplicates, and sort numerically\n",
    "        all_pages = list(range(len(doc)))\n",
    "        pages_to_extract = list(sorted({\n",
    "            all_pages[idx] for idx in PAGES\n",
    "            if -len(doc) <= idx < len(doc)\n",
    "        }))\n",
    "\n",
    "        page_texts = pymupdf4llm.to_markdown(doc, pages=pages_to_extract, page_chunks=True, show_progress=False,\n",
    "                                             ignore_images=True, ignore_graphics=True)\n",
    "\n",
    "    all_chunks = []\n",
    "    for page in page_texts:\n",
    "        for chunk in split_text(page['text']):\n",
    "            score, feats = chunk_score(chunk, page['metadata']['page'])\n",
    "            if score is not None:\n",
    "                all_chunks.append({\n",
    "                    'text': chunk,\n",
    "                    'page': page['metadata']['page'],\n",
    "                    'score': score,\n",
    "                    'feats': feats,\n",
    "                    'index': len(all_chunks),\n",
    "                    'length': len(chunk)\n",
    "                })\n",
    "    \n",
    "    # Select chunks within character budget\n",
    "    selected_indices = set()\n",
    "    total_chars = 0\n",
    "    # Sort by score descending\n",
    "    for chunk in sorted(all_chunks, key=lambda x: x['score'], reverse=True):\n",
    "        if total_chars + 1 + chunk['length'] <= CHARACTER_BUDGET:\n",
    "            selected_indices.add(chunk['index'])\n",
    "            total_chars += chunk['length']\n",
    "\n",
    "    for chunk in all_chunks:\n",
    "        if chunk['index'] in selected_indices:\n",
    "            page_content[chunk['page']].append(chunk['text'])\n",
    "\n",
    "    pages = []\n",
    "    for pageno in sorted(page_content.keys()):\n",
    "        text = \"\\n\".join(page_content[pageno])\n",
    "        pages.append({\"page\": pageno, \"text\": text})\n",
    "    return {\"pdfinfo\": pdfinfo, \"pages\": pages}\n",
    "\n",
    "def convert_metadata(metadata_files):\n",
    "    for mdfile in sorted(metadata_files):\n",
    "        out_path = mdfile.replace('metadata', 'llm-dataset')\n",
    "        print(f\"converting {mdfile} to {out_path}\")\n",
    "        with open(mdfile) as infile, open(out_path, \"w\") as outfile:\n",
    "            for line in infile:\n",
    "                rec = json.loads(line)\n",
    "                pdf_path = id_to_fn(rec[\"id\"])\n",
    "                content = extract_content(pdf_path)\n",
    "                outrec = {\"id\": rec[\"id\"], \"url\": rec[\"url\"], \"content\": content, \"ground_truth\": rec[\"ground_truth\"]}\n",
    "                json.dump(outrec, outfile)\n",
    "                outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c89ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting ../metadata/article-en-test.jsonl to ../llm-dataset/article-en-test.jsonl\n",
      "converting ../metadata/article-en-train.jsonl to ../llm-dataset/article-en-train.jsonl\n",
      "converting ../metadata/article-fi-test.jsonl to ../llm-dataset/article-fi-test.jsonl\n",
      "converting ../metadata/article-fi-train.jsonl to ../llm-dataset/article-fi-train.jsonl\n",
      "converting ../metadata/article-se-test.jsonl to ../llm-dataset/article-se-test.jsonl\n",
      "converting ../metadata/article-se-train.jsonl to ../llm-dataset/article-se-train.jsonl\n",
      "converting ../metadata/article-sv-test.jsonl to ../llm-dataset/article-sv-test.jsonl\n",
      "converting ../metadata/article-sv-train.jsonl to ../llm-dataset/article-sv-train.jsonl\n",
      "converting ../metadata/book-en-test.jsonl to ../llm-dataset/book-en-test.jsonl\n",
      "converting ../metadata/book-en-train.jsonl to ../llm-dataset/book-en-train.jsonl\n",
      "converting ../metadata/book-fi-test.jsonl to ../llm-dataset/book-fi-test.jsonl\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "converting ../metadata/book-fi-train.jsonl to ../llm-dataset/book-fi-train.jsonl\n",
      "converting ../metadata/book-se-test.jsonl to ../llm-dataset/book-se-test.jsonl\n",
      "converting ../metadata/book-se-train.jsonl to ../llm-dataset/book-se-train.jsonl\n",
      "converting ../metadata/book-sv-test.jsonl to ../llm-dataset/book-sv-test.jsonl\n",
      "converting ../metadata/book-sv-train.jsonl to ../llm-dataset/book-sv-train.jsonl\n",
      "converting ../metadata/docthes-en-test.jsonl to ../llm-dataset/docthes-en-test.jsonl\n",
      "converting ../metadata/docthes-en-train.jsonl to ../llm-dataset/docthes-en-train.jsonl\n",
      "converting ../metadata/docthes-fi-test.jsonl to ../llm-dataset/docthes-fi-test.jsonl\n",
      "converting ../metadata/docthes-fi-train.jsonl to ../llm-dataset/docthes-fi-train.jsonl\n",
      "converting ../metadata/docthes-se-test.jsonl to ../llm-dataset/docthes-se-test.jsonl\n",
      "converting ../metadata/docthes-se-train.jsonl to ../llm-dataset/docthes-se-train.jsonl\n",
      "converting ../metadata/docthes-sv-test.jsonl to ../llm-dataset/docthes-sv-test.jsonl\n",
      "converting ../metadata/docthes-sv-train.jsonl to ../llm-dataset/docthes-sv-train.jsonl\n",
      "converting ../metadata/report-en-test.jsonl to ../llm-dataset/report-en-test.jsonl\n",
      "converting ../metadata/report-en-train.jsonl to ../llm-dataset/report-en-train.jsonl\n",
      "converting ../metadata/report-fi-test.jsonl to ../llm-dataset/report-fi-test.jsonl\n",
      "converting ../metadata/report-fi-train.jsonl to ../llm-dataset/report-fi-train.jsonl\n",
      "converting ../metadata/report-se-test.jsonl to ../llm-dataset/report-se-test.jsonl\n",
      "converting ../metadata/report-se-train.jsonl to ../llm-dataset/report-se-train.jsonl\n",
      "converting ../metadata/report-sv-test.jsonl to ../llm-dataset/report-sv-test.jsonl\n",
      "converting ../metadata/report-sv-train.jsonl to ../llm-dataset/report-sv-train.jsonl\n",
      "converting ../metadata/thes-en-test.jsonl to ../llm-dataset/thes-en-test.jsonl\n",
      "converting ../metadata/thes-en-train.jsonl to ../llm-dataset/thes-en-train.jsonl\n",
      "MuPDF error: format error: object out of range (297 0 R); xref size 297\n",
      "\n",
      "MuPDF error: format error: object out of range (300 0 R); xref size 297\n",
      "\n",
      "MuPDF error: format error: object out of range (298 0 R); xref size 297\n",
      "\n",
      "MuPDF error: format error: object out of range (299 0 R); xref size 297\n",
      "\n",
      "converting ../metadata/thes-fi-test.jsonl to ../llm-dataset/thes-fi-test.jsonl\n",
      "converting ../metadata/thes-fi-train.jsonl to ../llm-dataset/thes-fi-train.jsonl\n",
      "converting ../metadata/thes-se-test.jsonl to ../llm-dataset/thes-se-test.jsonl\n",
      "converting ../metadata/thes-se-train.jsonl to ../llm-dataset/thes-se-train.jsonl\n",
      "converting ../metadata/thes-sv-test.jsonl to ../llm-dataset/thes-sv-test.jsonl\n",
      "converting ../metadata/thes-sv-train.jsonl to ../llm-dataset/thes-sv-train.jsonl\n",
      "CPU times: user 6min 41s, sys: 2.94 s, total: 6min 44s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "convert_metadata(metadata_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90efaa-4108-4c9f-9724-ff977857c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
